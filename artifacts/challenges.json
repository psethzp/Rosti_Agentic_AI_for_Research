[
  {
    "id": "r0001",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0001",
    "claim_text": "Models trained on biased data may produce unfair or discriminatory results, negatively impacting teaching and learning processes. This bias can disproportionately affect minority groups or specific cultures, potentially leading to the fading of local knowledge. Addressing bias requires continuous model updates with diverse, unbiased data and expert human supervision to review outputs. Educators need training to recognize and address potential biases and other failures in model outputs.",
    "summary": "LLMs can exacerbate educational inequalities due to biased data and unequal access.",
    "detail": "The claim states that biased data in LLMs can lead to unfair or discriminatory results, disproportionately affecting minority groups and potentially causing local knowledge to fade. Evidence E1 supports this by highlighting that LLMs can create unfair access for non-English speaking users and that financial barriers to accessing and maintaining these models can widen the education gap. This suggests a failure of LLMs to provide equitable educational opportunities, particularly for marginalized linguistic and socio-economic groups.",
    "evidence": [
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 10,
        "char_start": 1938,
        "char_end": 4252,
        "quote": "users, causing unfair access to such education technologies for non-English speaking users. Despite the efforts of various research communities to address multilingualism fairness for AI technologies, there is still much room for improvement. Lastly, the unfairness related to financial means for access- ing, trainin...",
        "chunk_id": "1-ChatGPT_for_Good:p0010:c0001",
        "chunk_text": "users, causing unfair access to such education technologies\nfor non-English speaking users. Despite the efforts of various\nresearch communities to address multilingualism fairness for\nAI technologies, there is still much room for improvement.\nLastly, the unfairness related to financial means for access-\ning, training and maintaining large language models may need\nto be regulated by governmental organisations with the aim\nto provide equity-oriented means to all educational entities\ninterested in using these modern technologies. Without fair\naccess, this AI technology may seriously widen the education\ngap like no other technology before it.\nWe therefore conclude with UNESCO\u2019s call to ensure that\nAI does not widen the technological and educational divides\nwithin and between countries, and recommended important\nstrategies for the use of AI in a responsible and fair way to\nreduce this existing gap instead. According to the UNESCO\neducation 2030 Agenda [67]: \u201cUNESCO\u2019s mandate calls in-\nherently for a human-centred approach to AI. It aims to shift\nthe conversation to include AI\u2019s role in addressing current\ninequalities regarding access to knowledge, research and the\ndiversity of cultural expressions and to ensure AI does not\nwiden the technological divides within and between countries.\nThe promise of \u201cAI for all\u201d must be that everyone can take ad-\nvantage of the technological revolution under way and access\nits fruits, notably in terms of innovation and knowledge.\u201d\n6. Concluding Remarks\nThe use of large language models in education is a promising\narea of research that offers many opportunities to enhance the\nlearning experience for students and support the work of teach-\ners. However, to unleash their full potential for education, it is\ncrucial to approach the use of these models with caution and\nto critically evaluate their limitations and potential biases. In-\ntegrating large language models into education must therefore\nmeet stringent privacy, security, and - for sustainable scal-\ning - environmental, regulatory and ethical requirements, and\nmust be done in conjunction with ongoing human monitoring,\nguidance, and critical thinking.\nWhile this position paper reflects the optimism of the au-\nthors about the opportunities of large language models as a\ntransformative technology in"
      }
    ],
    "severity": "High",
    "actions": [
      "Investigate specific examples of LLM bias negatively impacting minority groups or local knowledge.",
      "Explore the effectiveness of proposed solutions like governmental regulation for equitable access.",
      "Assess the current state of multilingual fairness in LLMs and identify remaining challenges."
    ]
  },
  {
    "id": "r0003",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0002",
    "claim_text": "Students may become too reliant on models, which can negatively impact their critical thinking and problem-solving abilities. The effortless acquisition of information from models can simplify learning to the point where deeper cognitive skills are not adequately developed. Models cannot replace the creativity, critical thinking, and problem-solving skills fostered through human instruction. Curricula should encourage the creative and complementary use of LLMs rather than their outright replacement of human teaching.",
    "summary": "The claim asserts that LLMs hinder critical thinking, but evidence suggests they can be used to foster it.",
    "detail": "The claim (c0002) states that LLMs can negatively impact critical thinking and problem-solving abilities by simplifying learning and preventing the development of deeper cognitive skills. However, evidence snippets E3 and E2 suggest the opposite. E3 explicitly mentions that LLMs can assist in the development of critical thinking skills by generating questions and prompts that encourage students to analyze and interpret information. E2 discusses adaptive learning technologies and customization of LLM output to align with teaching styles, implying a potential for LLMs to support, rather than hinder, cognitive development when integrated thoughtfully.",
    "evidence": [
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 2,
        "char_start": 1901,
        "char_end": 4241,
        "quote": "these mod- els, opportunities for enhancement of learning and teaching experiences may be possible for individuals at all levels of edu- cation, including primary, secondary, tertiary and professional development. For elementary school students, large language models can assist in the development of reading and writ...",
        "chunk_id": "1-ChatGPT_for_Good:p0002:c0001",
        "chunk_text": "these mod-\nels, opportunities for enhancement of learning and teaching\nexperiences may be possible for individuals at all levels of edu-\ncation, including primary, secondary, tertiary and professional\ndevelopment.\nFor elementary school students, large language models\ncan assist in the development of reading and writing skills\n(e.g., by suggesting syntactic and grammatical corrections), as\nwell as in the development of writing style and critical think-\ning skills. These models can be used to generate questions\nand prompts that encourage students to think critically about\nwhat they are reading and writing, and to analyze and inter-\npret the information presented to them. Additionally, large\nlanguage models can also assist in the development of reading\ncomprehension skills by providing students with summaries\nand explanations of complex texts, which can make reading\nand understanding the material easier.\nFor middle and high school students, large language\nmodels can assist in the learning of a language and of writ-\ning styles for various subjects and topics, e.g., mathematics,\nphysics, language and literature, and other subjects. These\nmodels can be used to generate practice problems and quizzes,\nwhich can help students to better understand, contextual-\nize and retain the material they are learning. Additionally,\nlarge language models can also assist in the development of\nproblem-solving skills by providing students with explana-\ntions, step-by-step solutions, and interesting related questions\nto problems, which can help them to understand the reasoning\nbehind the solutions and develop analytical and out-of-the-box\nthinking.\nFor university students, large language models can assist\nin the research and writing tasks, as well as in the development\nof critical thinking and problem-solving skills. These models\ncan be used to generate summaries and outlines of texts, which\ncan help students to quickly understand the main points of a\ntext and to organize their thoughts for writing. Additionally,\nlarge language models can also assist in the development of\nresearch skills by providing students with information and re-\nsources on a particular topic and hinting at unexplored aspects\nand current research topics, which can help them to better\nunderstand and analyze the material.\nFor group & remote learning, large"
      },
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 9,
        "char_start": 3895,
        "char_end": 5102,
        "quote": "ogy, but it is conceivable that with more advanced models, the adaptability will increase. More specifically, a sensible mitigation strategy would be comprised of: \u2022 Use of adaptive learning technologies to personalize the output of the model to the needs of individual students by using student data (e.g., about lea...",
        "chunk_id": "1-ChatGPT_for_Good:p0009:c0002",
        "chunk_text": "ogy, but it is conceivable that with more advanced models, the\nadaptability will increase.\nMore specifically, a sensible mitigation strategy would be\ncomprised of:\n\u2022 Use of adaptive learning technologies to personalize the\noutput of the model to the needs of individual students\nby using student data (e.g., about learning style, prior\nknowledge, and performance, etc.)\n\u2022 Customization of the language model\u2019s output to align\nwith the teaching style and curriculum (by using data\nprovided by the teacher)\n\u2022 Use of multi-modal learning and teaching approaches,\nwhich combine text, audio, video, and experimentation\nto provide a more engaging and personalized experience\nfor students and teachers\n\u2022 Use of hybrid approaches, which combine the strengths\nof both human teachers and language models to gener-\nate targeted and personalized learning materials (based\non feedback, guidance, and support provided by the\nteachers)\n\u2022 Regular review of the model and continual improve-\nment for curriculum-related uses cases to ensure ade-\nquate and accurate functioning for education purposes\n\u2022 Research and development to create more advanced\nmodels that can better adapt to the diverse needs of\nstudents and teachers"
      }
    ],
    "severity": "High",
    "actions": [
      "Investigate specific examples or research that demonstrate LLMs hindering critical thinking.",
      "Explore how LLMs are being used in educational settings to actively promote critical thinking.",
      "Clarify the conditions under which LLMs might hinder versus enhance critical thinking development."
    ]
  },
  {
    "id": "r0005",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0003",
    "claim_text": "Current LLMs often favor English speakers, creating unfair access for non-English speaking users and highlighting a need for multilingual fairness improvements. The financial burden of accessing, training, and maintaining LLMs can create inequities, necessitating governmental regulation for equitable access. Integrating LLMs requires understanding their capabilities and limitations, and developing appropriate user interfaces for diverse learners. The cost of training and maintaining LLMs can be a significant financial burden for schools with limited budgets.",
    "summary": "LLMs can perpetuate societal biases, leading to unfair outcomes, especially for minority groups.",
    "detail": "Evidence E2 explicitly states that LLMs can perpetuate and amplify existing societal biases and unfairness. This can negatively impact teaching and learning, particularly if the training data is biased. For instance, local knowledge of minority groups or cultures might be marginalized. Therefore, ensuring diverse and representative training data is crucial for fairness.",
    "evidence": [
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 6,
        "char_start": 3861,
        "char_end": 5568,
        "quote": "con- tent \u2022 Inheritance and detailed terms of use for the content generated by the model \u2022 Informing and raising awareness of the users about these policies Bias and fairness. Large language models can perpetuate and amplify existing biases and unfairness in society, which can negatively impact teaching and learning...",
        "chunk_id": "1-ChatGPT_for_Good:p0006:c0002",
        "chunk_text": "con-\ntent\n\u2022 Inheritance and detailed terms of use for the content\ngenerated by the model\n\u2022 Informing and raising awareness of the users about\nthese policies\nBias and fairness.\nLarge language models can perpetuate\nand amplify existing biases and unfairness in society, which\ncan negatively impact teaching and learning processes and\noutcomes. For example, if a model is trained on data that\nis biased towards certain groups of people, it may produce\nresults that are unfair or discriminatory towards those groups\n(e.g., local knowledge about minorities such as small ethnic\ngroups or cultures can fade into the background). Thus, it is\nimportant to ensure that the training data or the data used for\nfine-tuning on down-stream tasks for the model is diverse and\nrepresentative of different groups of people. Regular monitor-\ning and testing of the model\u2019s performance on different groups\nof people can help identify and address any biases early on.\nHence, human oversight in the process is indispensable and\ncritical for the mitigation of bias and beneficial application of\nlarge language models in education.\nMore specifically, a responsible mitigation strategy would\nfocus on the following key aspects:\n\u2022 A diverse set of data to train or fine-tune the model, to\nensure that it is not biased towards any particular group\n\u2022 Regular monitoring and evaluation of the model\u2019s per-\nformance (on diverse groups of people) to identify and\naddress any biases that may arise\n\u2022 Fairness measures and bias-correction techniques, such\nas pre-processing or post-processing methods\n\u2022 Transparency mechanisms that enable users to compre-\nhend the model\u2019s output, and the data and assumptions\nthat were used to generate it"
      }
    ],
    "severity": "High",
    "actions": [
      "Investigate specific examples of how LLMs perpetuate bias against minority groups.",
      "Explore methods for identifying and mitigating bias in LLM training data and outputs.",
      "Assess the effectiveness of current strategies for ensuring diverse and representative training data."
    ]
  },
  {
    "id": "r0007",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0004",
    "claim_text": "It is crucial to use multiple authoritative sources to verify information provided by LLMs to ensure correctness and integrity. The use of LLMs should be integrated with human expertise, such as teachers or subject matter experts, who review and validate the model's output. Developing protocols and standards for fact-checking and corroborating information is essential for maintaining accuracy. Clear and transparent communication about the model's performance, capabilities, and limitations is necessary for responsible use.",
    "summary": "LLMs can negatively impact critical thinking and problem-solving skills by providing effortless answers.",
    "detail": "The claim emphasizes the need for human expertise to validate LLM output and prevent over-reliance. Evidence E2 directly supports this by stating that learners may rely too heavily on LLMs, and the 'effortlessly generated information' can negatively impact their critical thinking and problem-solving skills. This happens because LLMs simplify the acquisition of answers, potentially leading to laziness and discouraging independent investigation and conclusion-drawing.",
    "evidence": [
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 7,
        "char_start": 0,
        "char_end": 2297,
        "quote": "ChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education \u2014 7/13 \u2022 Professional training and resources to educators on how to recognize and address potential biases and other fail- ures in the model\u2019s output \u2022 Continuous updates of the model with diverse, unbiased data, and supervision...",
        "chunk_id": "1-ChatGPT_for_Good:p0007:c0000",
        "chunk_text": "ChatGPT for Good? On Opportunities and Challenges of Large Language Models for Education \u2014 7/13\n\u2022 Professional training and resources to educators on how\nto recognize and address potential biases and other fail-\nures in the model\u2019s output\n\u2022 Continuous updates of the model with diverse, unbiased\ndata, and supervision of human experts to review the\nresults\nLearners may rely too heavily on the model.\nThe effort-\nlessly generated information could negatively impact their\ncritical thinking and problem-solving skills. This is because\nthe model simplifies the acquisition of answers or informa-\ntion, which can amplify laziness and counteract the learners\u2019\ninterest to conduct their own investigations and come to their\nown conclusions or solutions.\nTo encounter this risk, it is important to be aware of the\nlimitations of large language models and use them only as\na tool to support and enhance learning [55], rather than as\na replacement for human authorities and other authoritative\nsources. Thus a responsible mitigation strategy would focus\non the following key aspects:\n\u2022 Raising awareness of the limitations and unexpected\nbrittleness of large language models and AI systems in\ngeneral (i.e., experimenting with the model to build an\nown understanding of the workings and limitations)\n\u2022 Using language models to generate hypotheses and ex-\nplore different perspectives, rather than just to generate\nanswers\n\u2022 Strategies to use other educational resources (e.g., books,\narticles) and other authoritative sources to evaluate and\ncorroborate the factual correctness of the information\nprovided by the model (i.e., encouraging learners to\nquestion the generated content)\n\u2022 Incorporating critical thinking and problem-solving ac-\ntivities into the curriculum, to help students develop\nthese skills\n\u2022 Incorporating human expertise and teachers to review,\nvalidate and explain the information provided by the\nmodel\nIt is important to note that the use of large language models\nshould be integrated into the curriculum in a way that com-\nplements and enhances the learning experience, rather than\nreplacing it.\nTeachers may become too reliant on the models.\nUsing\nlarge language models can provide accurate and relevant infor-\nmation, but they cannot replace the creativity, critical thinking,\nand"
      }
    ],
    "severity": "High",
    "actions": [
      "Highlight the risk of LLMs hindering the development of critical thinking and problem-solving skills.",
      "Emphasize the importance of human educators guiding students to use LLMs as tools for deeper understanding rather than as replacements for their own cognitive processes."
    ]
  },
  {
    "id": "r0002",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0001",
    "claim_text": "Models trained on biased data may produce unfair or discriminatory results, negatively impacting teaching and learning processes. This bias can disproportionately affect minority groups or specific cultures, potentially leading to the fading of local knowledge. Addressing bias requires continuous model updates with diverse, unbiased data and expert human supervision to review outputs. Educators need training to recognize and address potential biases and other failures in model outputs.",
    "summary": "LLMs require significant human oversight and educator training to mitigate failures.",
    "detail": "The claim emphasizes the need for continuous model updates with diverse data, expert human supervision, and educator training to recognize and address biases and other failures in LLM outputs. Evidence E2 outlines strategies for ensuring correctness and integrity, including using multiple authoritative sources, human expertise to validate information, and developing fact-checking protocols. Evidence E3 suggests adaptive learning and customization to personalize outputs, implying that LLMs alone are not sufficient and require integration with human pedagogical approaches. These points collectively indicate a limitation in LLMs' ability to function autonomously and reliably in educational settings without substantial human intervention and training.",
    "evidence": [
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 9,
        "char_start": 2010,
        "char_end": 4288,
        "quote": "it is providing up-to-date and accurate information \u2022 Use of multiple authoritative sources to verify the in- formation provided by the model to ensure correctness and integrity \u2022 Use of the model in conjunction with human expertise, e.g., teachers or subject matter experts, who review and validate the information p...",
        "chunk_id": "1-ChatGPT_for_Good:p0009:c0001",
        "chunk_text": "it is providing up-to-date and\naccurate information\n\u2022 Use of multiple authoritative sources to verify the in-\nformation provided by the model to ensure correctness\nand integrity\n\u2022 Use of the model in conjunction with human expertise,\ne.g., teachers or subject matter experts, who review and\nvalidate the information provided by the model\n\u2022 Development of protocol and standards for fact-checking\nand corroborating information provided by the model\n\u2022 Provide clear and transparent information on the model\u2019s\nperformance, what it is or is not capable of, and the con-\nditions under which it operates.\n\u2022 Training and resources for educators and learners on\nhow to use the model, interpret its results and evaluate\nthe information provided\n\u2022 Regular review and evaluation of the model with trans-\nparent reporting on the model\u2019s performance, i.e., what\nit is or is not capable of and the identification of con-\nditions under which inaccuracies or other issues may\narise\nDifficulty to distinguish between real knowledge and con-\nvincingly written but unverified model output.\nThe abil-\nity of large language models to generate human-like text can\nmake it difficult for students to distinguish between real knowl-\nedge and unverified information. This can lead to students\naccepting false or misleading information as true, without\nquestioning its validity.\nTo mitigate this risk, in addition to the above verification-\nand integrity-related mitigation strategy, it is important to pro-\nvide education on how information can be evaluated critically\nand teach students exploration, investigation, verification, and\ncorroboration strategies.\nLack of adaptability.\nLarge language models are not able to\nadapt to the diverse needs of students and teachers, and may\nnot be able to provide the level of personalization required for\neffective learning. This is a limitation of the current technol-\nogy, but it is conceivable that with more advanced models, the\nadaptability will increase.\nMore specifically, a sensible mitigation strategy would be\ncomprised of:\n\u2022 Use of adaptive learning technologies to personalize the\noutput of the model to the needs of individual students\nby using student data (e.g., about learning style, prior\nknowledge, and performance, etc.)\n\u2022 Customization of the"
      },
      {
        "source_id": "1-ChatGPT_for_Good",
        "page": 9,
        "char_start": 3895,
        "char_end": 5102,
        "quote": "ogy, but it is conceivable that with more advanced models, the adaptability will increase. More specifically, a sensible mitigation strategy would be comprised of: \u2022 Use of adaptive learning technologies to personalize the output of the model to the needs of individual students by using student data (e.g., about lea...",
        "chunk_id": "1-ChatGPT_for_Good:p0009:c0002",
        "chunk_text": "ogy, but it is conceivable that with more advanced models, the\nadaptability will increase.\nMore specifically, a sensible mitigation strategy would be\ncomprised of:\n\u2022 Use of adaptive learning technologies to personalize the\noutput of the model to the needs of individual students\nby using student data (e.g., about learning style, prior\nknowledge, and performance, etc.)\n\u2022 Customization of the language model\u2019s output to align\nwith the teaching style and curriculum (by using data\nprovided by the teacher)\n\u2022 Use of multi-modal learning and teaching approaches,\nwhich combine text, audio, video, and experimentation\nto provide a more engaging and personalized experience\nfor students and teachers\n\u2022 Use of hybrid approaches, which combine the strengths\nof both human teachers and language models to gener-\nate targeted and personalized learning materials (based\non feedback, guidance, and support provided by the\nteachers)\n\u2022 Regular review of the model and continual improve-\nment for curriculum-related uses cases to ensure ade-\nquate and accurate functioning for education purposes\n\u2022 Research and development to create more advanced\nmodels that can better adapt to the diverse needs of\nstudents and teachers"
      }
    ],
    "severity": "Medium",
    "actions": [
      "Evaluate the practical challenges and effectiveness of implementing continuous human supervision for LLM outputs in education.",
      "Research the development and impact of training programs for educators on LLM usage and bias detection.",
      "Examine the feasibility and effectiveness of adaptive learning and customization strategies mentioned in E3 for mitigating LLM failures."
    ]
  }
]