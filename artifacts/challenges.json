[
  {
    "id": "r0001",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0001",
    "claim_text": "The probability of an LLM agent losing focus increases as cognitive load, a measure of task complexity, rises, simulating mental fatigue. A 'threshold' parameter can determine how easily an LLM agent is distracted by off-topic comments or complex thoughts, impacting its ability to stay on task. Evaluation metrics like 'Time-on-Task' measure the percentage of conversational turns where an LLM's output remains semantically relevant to the ongoing topic, highlighting potential failures in maintaining focus. The frequency of off-topic utterances serves as a direct count of instances where an LLM deviates from the established conversation, indicating a failure in sustained attention.",
    "summary": "The claim discusses LLM focus and cognitive load, but the evidence focuses on simulating cognitive functions and emotional states rather than directly measuring LLM focus degradation.",
    "detail": "The claim posits that LLM focus degrades with increased cognitive load and introduces a 'threshold' parameter for distraction. It also mentions evaluation metrics like 'Time-on-Task' and frequency of off-topic utterances. However, the provided evidence snippets (E1, E2, E3) describe the implementation of modules to simulate cognitive functions, emotional states, and introduce noise/memory limitations within an LLM agent. While these modules might indirectly influence focus, they do not directly measure or validate the claim's assertion about LLM focus degradation due to cognitive load or the effectiveness of the proposed 'threshold' parameter. The evidence does not provide metrics for 'Time-on-Task' or 'off-topic utterances' as described in the claim.",
    "evidence": [
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 4,
        "char_start": 0,
        "char_end": 2483,
        "quote": "4 Evaluation Metrics: \u2022 Emotional Valence in Language: Sentiment analysis of the agent\u2019s generated text to track emotional shifts. \u2022 Behavioral Indicators: Measurement of behaviors such as help-seeking frequency, response length, and proactive questioning. \u2022 Human Expert Ratings: Educators will evaluate the authenti...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0004:c0000",
        "chunk_text": "4\nEvaluation Metrics:\n\u2022 Emotional Valence in Language: Sentiment analysis of the agent\u2019s generated text to track emotional shifts.\n\u2022 Behavioral Indicators: Measurement of behaviors such as help-seeking frequency, response length, and\nproactive questioning.\n\u2022 Human Expert Ratings: Educators will evaluate the authenticity of the agent\u2019s emotional and motivational\ntrajectory.\nTestable Hypotheses:\n\u2022 H1 : An agent whose \u2018Control Appraisal\u2019 is systematically lowered through negative feedback will transition\nto negative emotional states (e.g., frustration, anxiety) and exhibit a corresponding increase in negative-valence\nlanguage.\n\u2022 H2 : Agents equipped with the Affective Module will be perceived by human raters as demonstrating more\nbelievable and differentiated student personalities than agents defined only by a static dispositional prompt.\n2) The Attention Module (Focus Manager):\nTheoretical Grounding: This module is grounded in hierarchical models of attention that differentiate between\nsustained attention (maintaining focus over time) and selective attention (filtering distractions) [10][50]. It also\ndraws from cognitive models of executive function, such as Baddeley\u2019s model of working memory [5], which\nincludes a central executive responsible for attentional control, and Load Theory of Attention [34] which posits that\nthe ability to ignore distractions depends on the perceptual load of the current task. These concepts are quite central\nto understanding various underlying conditions that affect Attention, such as ADHD [9]. Our model operationalizes\nthese concepts to simulate how an agent\u2019s focus can decay, be captured by competing stimuli, or be reset by direct\nengagement.\nOperational Definition: The module will be a state-based system governed by the following parameters:\n\u2022 Attention Span: A numerical value representing the number of conversational turns an agent can maintain\nfocus on a single topic before a potential \u201cattention lapse.\u201d This will be implemented as a timer that resets\nupon re-engagement.\n\u2022 Distractibility Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an\noff-topic comment, a complex thought) shifting the agent\u2019s focus.\n\u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum,\nthe probability of an attention lapse increases, simulating mental fatigue.\nEvaluation Metrics:\n\u2022 Time-on-Task: Percentage of conversational turns where the"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 3,
        "char_start": 0,
        "char_end": 2687,
        "quote": "3 c) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules that dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM that is trained to read this Control Panel and generate behavior that matches the specified cognit...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0003:c0000",
        "chunk_text": "3\nc) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules\nthat dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM\nthat is trained to read this Control Panel and generate behavior that matches the specified cognitive state to simulate\ncore cognitive functions and individual differences. The modules function as follows:\n\u2022 Affect Module (Emotional State Modulator): A state-machine implementation tracking dynamic emotional\nstates (confident, confused, anxious, engaged) based on interaction history and performance outcomes. This\nmodule will modify LLM prompt components in real-time to reflect emotional influences on communication\nstyle, response length, and cognitive processing approach. Here we update the emotion field of the Control\nPanel (e.g., \u201cemotion\u201d: \u201cprimary\u201d: \u201canxious\u201d, \u201cconfidence\u201d: 0.3), which the LLM interprets to modulate its\ncommunication style, response length, and cognitive processing approach.\n\u2022 Attention Module (Focus Manager): A dynamic attention modeling system that simulates varying atten-\ntion spans and distractibility patterns. Drawing from cognitive models of attention and executive function\n[6][56][56], this module will implement time-decay functions for attention, threshold-based coherence degra-\ndation, and attention reset mechanisms triggered by direct engagement or environmental changes. Here we\nupdate the focus field of the Control Panel in real-time (e.g., \u201cfocus\u201d: \u201clevel\u201d: 0.4), which instructs the LLM\nto adjust its response coherence and relevance.\n\u2022 Memory and Decoding Module (Memory Filter): A cognitive processing filter that models information\nprocessing variations, particularly those reflecting a spectrum of information processing speeds and accuracies\n[33][43][19]. This system will introduce controlled character-level noise (e.g., b/d reversals, transposition errors)\nwith probability distributions calibrated to diverse reading patterns, affecting information encoding and retrieval\naccuracy. Here we update the memory field of the Control Panel (e.g., \u201cmemory\u201d: \u201cdecoding fidelity\u201d: 0.85),\naffecting information encoding and retrieval accuracy.\nd) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation\nthrough multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis-\ntency, quantitative analysis of behavioral differentiation between agent types, and integration testing within educa-\ntional simulation environments. We will also explore the proposed \u201cCommittee of Agents\u201d alternative architecture as\nan ablation study,"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 5,
        "char_start": 0,
        "char_end": 2714,
        "quote": "5 \u2022 Phonological Noise Parameter: A probability distribution that introduces controlled, character-level errors into the agent\u2019s \u201cinternal reading\u201d of text (e.g., b/d reversals, transposition errors) to simulate decoding difficulties. \u2022 Working Memory Buffer: A fixed-size buffer representing the agent\u2019s capacity to ...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0005:c0000",
        "chunk_text": "5\n\u2022 Phonological Noise Parameter: A probability distribution that introduces controlled, character-level errors into\nthe agent\u2019s \u201cinternal reading\u201d of text (e.g., b/d reversals, transposition errors) to simulate decoding difficulties.\n\u2022 Working Memory Buffer: A fixed-size buffer representing the agent\u2019s capacity to hold information. Effortful\ndecoding, as dictated by the noise parameter, will consume space in this buffer, reducing the resources available\nfor higher-level comprehension.\nEvaluation Metrics:\n\u2022 Reading Comprehension Accuracy: Agent performance on simulated comprehension tasks.\n\u2022 Error Analysis: Classification of agent errors into phonological, orthographic, or semantic categories.\n\u2022 Information Recall Fidelity: The accuracy with which an agent can recall facts presented in a passage, testing\nthe integrity of its working memory.\nTestable Hypotheses:\n\u2022 H5 : Agents with a high \u2018Phonological Noise\u2019 parameter will make significantly more phonologically-based\nerrors than agents with a low parameter.\n\u2022 H6 : A reduction in the \u2018Working Memory Buffer\u2019 size will be negatively correlated with an agent\u2019s ability\nto answer questions about long passages of text.\nIV. EXPECTED MILESTONES\nThe project is envisioned as a four-month intensive research program with clearly defined deliverables and\nevaluation checkpoints:\n\u2022 Month 1: Foundation and Architecture\nComplete a systematic review of educational psychology to formalize student archetypes, followed by the\ndesign of the complete Cognitive Twin architecture and development of initial meta-prompts.\nDeliverables: Comprehensive literature analysis, formal archetype specifications, and a complete architectural\ndesign with an initial meta-prompt library.\n\u2022 Month 2: Implementation and Preliminary Validation\nDevelop and implement the three core cognitive processing modules (Affect, Attention, Memory and Decod-\ning). Integrate all system components and conduct preliminary validation studies, including agent behavior\nconsistency testing.\nDeliverables: A functional, integrated system with documented performance characteristics and preliminary\nvalidation results.\n\u2022 Month 3: Comprehensive Evaluation and Alternative Architecture Exploration\nConduct full validation study including human expert evaluation, behavioral differentiation analysis, and\nCommittee of Agents implementation as a comparative study.\nDeliverables: Complete validation results and architectural comparison analysis.\n\u2022 Month 4: Analysis, Documentation, and Dissemination\nAnalyze all collected data, optimize system parameters based on evaluation results, and prepare comprehensive\ndocumentation, including academic paper preparation for submission to relevant conferences"
      }
    ],
    "severity": "High",
    "actions": [
      "Seek evidence that directly measures LLM focus degradation under varying cognitive loads.",
      "Look for studies that validate the 'threshold' parameter's impact on LLM distraction.",
      "Find evidence that uses 'Time-on-Task' or 'frequency of off-topic utterances' as evaluation metrics for LLM focus."
    ]
  },
  {
    "id": "r0003",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0002",
    "claim_text": "A 'Phonological Noise Parameter' can introduce character-level errors into an LLM's internal text processing, mimicking decoding challenges. These simulated decoding difficulties consume space in a fixed-size 'Working Memory Buffer,' reducing the LLM's capacity for higher-level comprehension. Reading comprehension accuracy is a key metric used to evaluate an LLM's performance on simulated comprehension tasks, revealing potential weaknesses in text processing. The LLM's ability to handle phonological and orthographic processing, including character-level decoding fidelity and working memory for text, is crucial for its overall accuracy.",
    "summary": "The claim introduces a 'Phonological Noise Parameter' and its impact on LLM working memory, but the evidence does not directly support or refute this specific mechanism.",
    "detail": "The claim posits that a 'Phonological Noise Parameter' can introduce character-level errors, consume working memory, and reduce comprehension. However, none of the provided evidence snippets discuss this specific parameter or its effects. Evidence E1 discusses 'Cognitive Load' and 'Attention Span' in a general sense related to task complexity and focus, and E3 describes implementing core cognitive modules like an 'Affect Module' to simulate emotional states and their influence on LLM prompts. While these touch upon cognitive concepts, they do not validate or invalidate the mechanism of a 'Phonological Noise Parameter' as described in the claim.",
    "evidence": [
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 4,
        "char_start": 2056,
        "char_end": 3798,
        "quote": "Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an off-topic comment, a complex thought) shifting the agent\u2019s focus. \u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum, the probability of an attention lapse increases...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0004:c0001",
        "chunk_text": "Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an\noff-topic comment, a complex thought) shifting the agent\u2019s focus.\n\u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum,\nthe probability of an attention lapse increases, simulating mental fatigue.\nEvaluation Metrics:\n\u2022 Time-on-Task: Percentage of conversational turns where the agent\u2019s output is semantically relevant to the\nongoing topic.\n\u2022 Frequency of Off-Topic Utterances: A count of instances where the agent\u2019s output deviates from the\nestablished topic.\n\u2022 Human Expert Ratings: Blind evaluators will rate the believability of an agent\u2019s attentional patterns.\nTestable Hypotheses:\n\u2022 H3 : Agents with a lower \u2018Attention Span\u2019 parameter will exhibit a statistically significant higher frequency\nof off-topic utterances compared to agents with higher attention spans.\n\u2022 H4 : Agents with a lower \u2018Distractibility Threshold\u2019 will show greater performance degradation in simulated\nenvironments with high levels of external distractions.\n3) The Memory and Decoding Module (Memory Filter):\nTheoretical Grounding: The module is based on the Dual-Route Cascade Model of Reading [11][35], which\nexplains how individuals decode familiar and unfamiliar words. Deficiencies in phonological (sound-based) and\northographic (visual-based) processing, which are often implicated in dyslexia, will be modeled [40]. Furthermore,\nit incorporates principles from Verbal Efficiency Theory [49], which posits that effortful decoding consumes finite\nworking memory resources needed for comprehension.\nOperational Definition: This module acts as a filter that pre-processes text input to the LLM. It is defined by:"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 3,
        "char_start": 0,
        "char_end": 2687,
        "quote": "3 c) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules that dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM that is trained to read this Control Panel and generate behavior that matches the specified cognit...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0003:c0000",
        "chunk_text": "3\nc) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules\nthat dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM\nthat is trained to read this Control Panel and generate behavior that matches the specified cognitive state to simulate\ncore cognitive functions and individual differences. The modules function as follows:\n\u2022 Affect Module (Emotional State Modulator): A state-machine implementation tracking dynamic emotional\nstates (confident, confused, anxious, engaged) based on interaction history and performance outcomes. This\nmodule will modify LLM prompt components in real-time to reflect emotional influences on communication\nstyle, response length, and cognitive processing approach. Here we update the emotion field of the Control\nPanel (e.g., \u201cemotion\u201d: \u201cprimary\u201d: \u201canxious\u201d, \u201cconfidence\u201d: 0.3), which the LLM interprets to modulate its\ncommunication style, response length, and cognitive processing approach.\n\u2022 Attention Module (Focus Manager): A dynamic attention modeling system that simulates varying atten-\ntion spans and distractibility patterns. Drawing from cognitive models of attention and executive function\n[6][56][56], this module will implement time-decay functions for attention, threshold-based coherence degra-\ndation, and attention reset mechanisms triggered by direct engagement or environmental changes. Here we\nupdate the focus field of the Control Panel in real-time (e.g., \u201cfocus\u201d: \u201clevel\u201d: 0.4), which instructs the LLM\nto adjust its response coherence and relevance.\n\u2022 Memory and Decoding Module (Memory Filter): A cognitive processing filter that models information\nprocessing variations, particularly those reflecting a spectrum of information processing speeds and accuracies\n[33][43][19]. This system will introduce controlled character-level noise (e.g., b/d reversals, transposition errors)\nwith probability distributions calibrated to diverse reading patterns, affecting information encoding and retrieval\naccuracy. Here we update the memory field of the Control Panel (e.g., \u201cmemory\u201d: \u201cdecoding fidelity\u201d: 0.85),\naffecting information encoding and retrieval accuracy.\nd) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation\nthrough multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis-\ntency, quantitative analysis of behavioral differentiation between agent types, and integration testing within educa-\ntional simulation environments. We will also explore the proposed \u201cCommittee of Agents\u201d alternative architecture as\nan ablation study,"
      }
    ],
    "severity": "High",
    "actions": [
      "Seek evidence that directly addresses the 'Phonological Noise Parameter' and its impact on LLM internal processing and working memory.",
      "Investigate if the concepts of 'Cognitive Load' or 'Attention Span' from E1 can be mapped to the 'Phonological Noise Parameter' or its effects.",
      "Explore if the 'Affect Module' in E3 has any implications for character-level processing or working memory limitations."
    ]
  },
  {
    "id": "r0007",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0004",
    "claim_text": "An 'Affect Module' can be implemented to track dynamic emotional states like confidence, confusion, anxiety, and engagement based on interaction history. The emotional valence in an LLM's generated language, assessed through sentiment analysis, can track shifts in its simulated emotional state. Behavioral indicators such as help-seeking frequency, response length, and proactive questioning are used to measure the LLM's simulated behaviors. Human experts, such as educators, evaluate the authenticity of an LLM agent's emotional and motivational trajectory, providing a measure of its simulation fidelity.",
    "summary": "The claim describes an 'Affect Module' for LLMs, but the evidence focuses on validating the believability and consistency of agent personas, not the internal emotional tracking.",
    "detail": "The claim (c0004) posits the implementation of an 'Affect Module' to track dynamic emotional states within an LLM, using sentiment analysis of generated language and behavioral indicators. However, the provided evidence (E1) details a validation phase that includes human expert rating of agent believability and archetype consistency, and quantitative analysis of behavioral differentiation. While this validation aims to assess the *output* of the agent's persona, it does not directly confirm the existence or functionality of an internal 'Affect Module' that dynamically tracks and simulates specific emotional states like confidence, confusion, anxiety, and engagement as described in the claim. The evidence focuses on the *perception* of the agent's persona rather than the internal mechanisms for generating it.",
    "evidence": [
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 3,
        "char_start": 2206,
        "char_end": 4464,
        "quote": "accuracy. d) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation through multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis- tency, quantitative analysis of behavioral differentiation between agent types, and integ...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0003:c0001",
        "chunk_text": "accuracy.\nd) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation\nthrough multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis-\ntency, quantitative analysis of behavioral differentiation between agent types, and integration testing within educa-\ntional simulation environments. We will also explore the proposed \u201cCommittee of Agents\u201d alternative architecture as\nan ablation study, implementing student agents as coordinating processes managing multiple trait-specific sub-agents\nto compare monolithic versus composable persona approaches.\nA. Core Cognitive Module Architecture\nThis section details the design of the three core cognitive modules that form the foundation of our psychologically-\ngrounded student agents. Each module is designed to be a programmable \u201dwrapper\u201d that shapes the LLM\u2019s generative\nprocess, with parameters derived from formalized student archetypes.\n1) The Affective Module (Emotional State Modulator):\nTheoretical Grounding: This module is grounded in Pekrun\u2019s Control-Value Theory of Achievement Emotions\n[47] [48], which links emotions to an individual\u2019s perceived control over and valuation of a task. It also incorporates\nAppraisal Theory [52][44], suggesting that emotions arise from our interpretation of events. This framework allows\nus to model how a \u201cDeep Learner\u201d might react to a challenge with curiosity, while a \u201cSurface Learner\u201d might react\nwith anxiety, based on their underlying motivations [15][25].\nOperational Definition: The module will be a state-machine where states are key achievement emotions (e.g.,\nConfident, Anxious, Engaged, Bored). State transitions are governed by:\n\u2022 Control Appraisal: An internal variable representing perceived self-efficacy, which changes based on perfor-\nmance outcomes and feedback.\n\u2022 Value Appraisal: A parameter derived from the agent\u2019s archetype that defines the intrinsic or extrinsic value\nof a task.\n\u2022 Affective Influence: The current emotional state will dynamically modify the LLM\u2019s meta-prompt in real-time.\nAn \u2018Anxious\u2019 state may add instructions for shorter, more hesitant responses, while an \u2018Engaged\u2019 state may\nprompt for more proactive and detailed contributions."
      }
    ],
    "severity": "High",
    "actions": [
      "Clarify if the 'Affect Module' is a proposed component or an implemented feature.",
      "Provide evidence that directly describes the internal mechanisms for tracking and simulating dynamic emotional states within the LLM.",
      "Explain how sentiment analysis of generated language and behavioral indicators are used to *track* simulated emotional states, not just to *assess* the agent's overall believability."
    ]
  },
  {
    "id": "r0002",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0001",
    "claim_text": "The probability of an LLM agent losing focus increases as cognitive load, a measure of task complexity, rises, simulating mental fatigue. A 'threshold' parameter can determine how easily an LLM agent is distracted by off-topic comments or complex thoughts, impacting its ability to stay on task. Evaluation metrics like 'Time-on-Task' measure the percentage of conversational turns where an LLM's output remains semantically relevant to the ongoing topic, highlighting potential failures in maintaining focus. The frequency of off-topic utterances serves as a direct count of instances where an LLM deviates from the established conversation, indicating a failure in sustained attention.",
    "summary": "The claim's focus on LLM 'mental fatigue' is not directly supported by the provided evidence, which describes simulated cognitive states rather than actual fatigue.",
    "detail": "The claim uses the term 'mental fatigue' to describe the LLM's loss of focus as cognitive load increases. The evidence, however, details the implementation of an 'Affect Module' that tracks dynamic emotional states like 'confident, confused, anxious, engaged' (E2) and simulates 'decoding difficulties' through a 'Phonological Noise Parameter' (E3). These are simulations of cognitive states and processing challenges, not direct evidence of LLM 'fatigue' in the human sense. The claim's analogy to human mental fatigue is not substantiated by the provided snippets.",
    "evidence": [
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 4,
        "char_start": 0,
        "char_end": 2483,
        "quote": "4 Evaluation Metrics: \u2022 Emotional Valence in Language: Sentiment analysis of the agent\u2019s generated text to track emotional shifts. \u2022 Behavioral Indicators: Measurement of behaviors such as help-seeking frequency, response length, and proactive questioning. \u2022 Human Expert Ratings: Educators will evaluate the authenti...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0004:c0000",
        "chunk_text": "4\nEvaluation Metrics:\n\u2022 Emotional Valence in Language: Sentiment analysis of the agent\u2019s generated text to track emotional shifts.\n\u2022 Behavioral Indicators: Measurement of behaviors such as help-seeking frequency, response length, and\nproactive questioning.\n\u2022 Human Expert Ratings: Educators will evaluate the authenticity of the agent\u2019s emotional and motivational\ntrajectory.\nTestable Hypotheses:\n\u2022 H1 : An agent whose \u2018Control Appraisal\u2019 is systematically lowered through negative feedback will transition\nto negative emotional states (e.g., frustration, anxiety) and exhibit a corresponding increase in negative-valence\nlanguage.\n\u2022 H2 : Agents equipped with the Affective Module will be perceived by human raters as demonstrating more\nbelievable and differentiated student personalities than agents defined only by a static dispositional prompt.\n2) The Attention Module (Focus Manager):\nTheoretical Grounding: This module is grounded in hierarchical models of attention that differentiate between\nsustained attention (maintaining focus over time) and selective attention (filtering distractions) [10][50]. It also\ndraws from cognitive models of executive function, such as Baddeley\u2019s model of working memory [5], which\nincludes a central executive responsible for attentional control, and Load Theory of Attention [34] which posits that\nthe ability to ignore distractions depends on the perceptual load of the current task. These concepts are quite central\nto understanding various underlying conditions that affect Attention, such as ADHD [9]. Our model operationalizes\nthese concepts to simulate how an agent\u2019s focus can decay, be captured by competing stimuli, or be reset by direct\nengagement.\nOperational Definition: The module will be a state-based system governed by the following parameters:\n\u2022 Attention Span: A numerical value representing the number of conversational turns an agent can maintain\nfocus on a single topic before a potential \u201cattention lapse.\u201d This will be implemented as a timer that resets\nupon re-engagement.\n\u2022 Distractibility Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an\noff-topic comment, a complex thought) shifting the agent\u2019s focus.\n\u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum,\nthe probability of an attention lapse increases, simulating mental fatigue.\nEvaluation Metrics:\n\u2022 Time-on-Task: Percentage of conversational turns where the"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 3,
        "char_start": 0,
        "char_end": 2687,
        "quote": "3 c) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules that dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM that is trained to read this Control Panel and generate behavior that matches the specified cognit...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0003:c0000",
        "chunk_text": "3\nc) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules\nthat dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM\nthat is trained to read this Control Panel and generate behavior that matches the specified cognitive state to simulate\ncore cognitive functions and individual differences. The modules function as follows:\n\u2022 Affect Module (Emotional State Modulator): A state-machine implementation tracking dynamic emotional\nstates (confident, confused, anxious, engaged) based on interaction history and performance outcomes. This\nmodule will modify LLM prompt components in real-time to reflect emotional influences on communication\nstyle, response length, and cognitive processing approach. Here we update the emotion field of the Control\nPanel (e.g., \u201cemotion\u201d: \u201cprimary\u201d: \u201canxious\u201d, \u201cconfidence\u201d: 0.3), which the LLM interprets to modulate its\ncommunication style, response length, and cognitive processing approach.\n\u2022 Attention Module (Focus Manager): A dynamic attention modeling system that simulates varying atten-\ntion spans and distractibility patterns. Drawing from cognitive models of attention and executive function\n[6][56][56], this module will implement time-decay functions for attention, threshold-based coherence degra-\ndation, and attention reset mechanisms triggered by direct engagement or environmental changes. Here we\nupdate the focus field of the Control Panel in real-time (e.g., \u201cfocus\u201d: \u201clevel\u201d: 0.4), which instructs the LLM\nto adjust its response coherence and relevance.\n\u2022 Memory and Decoding Module (Memory Filter): A cognitive processing filter that models information\nprocessing variations, particularly those reflecting a spectrum of information processing speeds and accuracies\n[33][43][19]. This system will introduce controlled character-level noise (e.g., b/d reversals, transposition errors)\nwith probability distributions calibrated to diverse reading patterns, affecting information encoding and retrieval\naccuracy. Here we update the memory field of the Control Panel (e.g., \u201cmemory\u201d: \u201cdecoding fidelity\u201d: 0.85),\naffecting information encoding and retrieval accuracy.\nd) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation\nthrough multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis-\ntency, quantitative analysis of behavioral differentiation between agent types, and integration testing within educa-\ntional simulation environments. We will also explore the proposed \u201cCommittee of Agents\u201d alternative architecture as\nan ablation study,"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 5,
        "char_start": 0,
        "char_end": 2714,
        "quote": "5 \u2022 Phonological Noise Parameter: A probability distribution that introduces controlled, character-level errors into the agent\u2019s \u201cinternal reading\u201d of text (e.g., b/d reversals, transposition errors) to simulate decoding difficulties. \u2022 Working Memory Buffer: A fixed-size buffer representing the agent\u2019s capacity to ...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0005:c0000",
        "chunk_text": "5\n\u2022 Phonological Noise Parameter: A probability distribution that introduces controlled, character-level errors into\nthe agent\u2019s \u201cinternal reading\u201d of text (e.g., b/d reversals, transposition errors) to simulate decoding difficulties.\n\u2022 Working Memory Buffer: A fixed-size buffer representing the agent\u2019s capacity to hold information. Effortful\ndecoding, as dictated by the noise parameter, will consume space in this buffer, reducing the resources available\nfor higher-level comprehension.\nEvaluation Metrics:\n\u2022 Reading Comprehension Accuracy: Agent performance on simulated comprehension tasks.\n\u2022 Error Analysis: Classification of agent errors into phonological, orthographic, or semantic categories.\n\u2022 Information Recall Fidelity: The accuracy with which an agent can recall facts presented in a passage, testing\nthe integrity of its working memory.\nTestable Hypotheses:\n\u2022 H5 : Agents with a high \u2018Phonological Noise\u2019 parameter will make significantly more phonologically-based\nerrors than agents with a low parameter.\n\u2022 H6 : A reduction in the \u2018Working Memory Buffer\u2019 size will be negatively correlated with an agent\u2019s ability\nto answer questions about long passages of text.\nIV. EXPECTED MILESTONES\nThe project is envisioned as a four-month intensive research program with clearly defined deliverables and\nevaluation checkpoints:\n\u2022 Month 1: Foundation and Architecture\nComplete a systematic review of educational psychology to formalize student archetypes, followed by the\ndesign of the complete Cognitive Twin architecture and development of initial meta-prompts.\nDeliverables: Comprehensive literature analysis, formal archetype specifications, and a complete architectural\ndesign with an initial meta-prompt library.\n\u2022 Month 2: Implementation and Preliminary Validation\nDevelop and implement the three core cognitive processing modules (Affect, Attention, Memory and Decod-\ning). Integrate all system components and conduct preliminary validation studies, including agent behavior\nconsistency testing.\nDeliverables: A functional, integrated system with documented performance characteristics and preliminary\nvalidation results.\n\u2022 Month 3: Comprehensive Evaluation and Alternative Architecture Exploration\nConduct full validation study including human expert evaluation, behavioral differentiation analysis, and\nCommittee of Agents implementation as a comparative study.\nDeliverables: Complete validation results and architectural comparison analysis.\n\u2022 Month 4: Analysis, Documentation, and Dissemination\nAnalyze all collected data, optimize system parameters based on evaluation results, and prepare comprehensive\ndocumentation, including academic paper preparation for submission to relevant conferences"
      }
    ],
    "severity": "Medium",
    "actions": [
      "Clarify if 'mental fatigue' in the claim refers to a simulated state or an emergent property of LLMs.",
      "Find evidence that investigates LLM performance degradation that is analogous to human mental fatigue.",
      "Explore research on the long-term performance and stability of LLMs under sustained operation."
    ]
  },
  {
    "id": "r0004",
    "topic": "What are some areas in which LLms today fail?",
    "claim_id": "c0002",
    "claim_text": "A 'Phonological Noise Parameter' can introduce character-level errors into an LLM's internal text processing, mimicking decoding challenges. These simulated decoding difficulties consume space in a fixed-size 'Working Memory Buffer,' reducing the LLM's capacity for higher-level comprehension. Reading comprehension accuracy is a key metric used to evaluate an LLM's performance on simulated comprehension tasks, revealing potential weaknesses in text processing. The LLM's ability to handle phonological and orthographic processing, including character-level decoding fidelity and working memory for text, is crucial for its overall accuracy.",
    "summary": "The claim emphasizes reading comprehension accuracy as a key metric for evaluating LLM weaknesses, but the evidence does not detail specific evaluation metrics for comprehension.",
    "detail": "The claim highlights 'reading comprehension accuracy' as a crucial metric for revealing LLM weaknesses in text processing. While E1 mentions 'Evaluation Metrics' such as 'Time-on-Task' and 'Frequency of Off-Topic Utterances,' these are related to conversational relevance and attention, not specifically reading comprehension. E2 is a list of citations and does not provide evaluation details. E3 describes implementing cognitive modules to simulate cognitive functions but does not specify how the resulting LLM behavior will be evaluated in terms of reading comprehension.",
    "evidence": [
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 4,
        "char_start": 2056,
        "char_end": 3798,
        "quote": "Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an off-topic comment, a complex thought) shifting the agent\u2019s focus. \u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum, the probability of an attention lapse increases...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0004:c0001",
        "chunk_text": "Threshold: A value that determines the likelihood of an external or internal stimulus (e.g., an\noff-topic comment, a complex thought) shifting the agent\u2019s focus.\n\u2022 Cognitive Load: A variable that increases with task complexity. As cognitive load approaches its maximum,\nthe probability of an attention lapse increases, simulating mental fatigue.\nEvaluation Metrics:\n\u2022 Time-on-Task: Percentage of conversational turns where the agent\u2019s output is semantically relevant to the\nongoing topic.\n\u2022 Frequency of Off-Topic Utterances: A count of instances where the agent\u2019s output deviates from the\nestablished topic.\n\u2022 Human Expert Ratings: Blind evaluators will rate the believability of an agent\u2019s attentional patterns.\nTestable Hypotheses:\n\u2022 H3 : Agents with a lower \u2018Attention Span\u2019 parameter will exhibit a statistically significant higher frequency\nof off-topic utterances compared to agents with higher attention spans.\n\u2022 H4 : Agents with a lower \u2018Distractibility Threshold\u2019 will show greater performance degradation in simulated\nenvironments with high levels of external distractions.\n3) The Memory and Decoding Module (Memory Filter):\nTheoretical Grounding: The module is based on the Dual-Route Cascade Model of Reading [11][35], which\nexplains how individuals decode familiar and unfamiliar words. Deficiencies in phonological (sound-based) and\northographic (visual-based) processing, which are often implicated in dyslexia, will be modeled [40]. Furthermore,\nit incorporates principles from Verbal Efficiency Theory [49], which posits that effortful decoding consumes finite\nworking memory resources needed for comprehension.\nOperational Definition: This module acts as a filter that pre-processes text input to the LLM. It is defined by:"
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 6,
        "char_start": 4062,
        "char_end": 6037,
        "quote": "Trends in Cognitive Sciences, 9(2):75\u201382, 2005. [35] Jonathan Levy, Tenniel Wydell, Mathew Thomas, John Duncan, Julie Wilson, Michael Richardson, and Catherine J. Mummery. Testing for the dual-route cascade reading model in the brain: An fmri effective connectivity account of an efficient reading style. PLoS ONE, 4(...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0006:c0002",
        "chunk_text": "Trends in Cognitive Sciences, 9(2):75\u201382, 2005.\n[35] Jonathan Levy, Tenniel Wydell, Mathew Thomas, John Duncan, Julie Wilson, Michael Richardson, and Catherine J. Mummery. Testing\nfor the dual-route cascade reading model in the brain: An fmri effective connectivity account of an efficient reading style. PLoS ONE,\n4(8):e6675, 2009.\n[36] GW Lindsay. Attention in psychology, neuroscience, and machine learning. Frontiers in Computational Neuroscience, 14:29, 2020.\n[37] Y. Liu et al. No for some, yes for others: Persona prompts and other sources of false refusal in language models. Semantic Scholar,\n2025.\n[38] M. Llorens-G\u00b4amez and L. Vicent. The impact of the design of learning spaces on attention and memory. Building and Environment,\n207:108483, 2022.\n[39] JM Lodge and G Kennedy. The role of attention in learning in the digital age. NPJ Science of Learning, 4:9, 2019.\n[40] Steven G. Luke, Hazel I. Blythe, James Kirkby, and Gordon Kambe. Dyslexics exhibit an orthographic, not a phonological, deficit:\nEvidence from lexical decision. Frontiers in Psychology, 14:1200329, 2023.\n[41] F. Marton and R. S\u00a8alj\u00a8o. On qualitative differences in learning: Outcome and process. British Journal of Educational Psychology,\n46(1):4\u201311, 1976.\n[42] S. Mart\u00b4\u0131nez-Briones and L. D. Shriberg. Working memory in children with learning disorders. European Child Adolescent Psychiatry,\n2020.\n[43] M. S. Mihaylova et al. Visual noise effect on reading in three developmental disorders: autism spectrum disorder, attention deficit\nhyperactivity disorder, and dyslexia. Scientific Reports, 12(1):18306, 2022.\n[44] Agnes Moors. Appraisal theories of emotion: State of the art and future development. Emotion Review, 5(2):119\u2013124, 2013.\n[45] JP et al. Morais. A general framework for reinforcement learning in cognitive architectures. Journal of Cognitive Neuroscience, 2025.\n[46] M. et al. Muhmenthaler. How attention and knowledge modulate memory. Frontiers in Cognition, 3:1125700, 2023."
      },
      {
        "source_id": "Cognitive_Twins_Practical_Work_Proposal (1)",
        "page": 3,
        "char_start": 0,
        "char_end": 2687,
        "quote": "3 c) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules that dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM that is trained to read this Control Panel and generate behavior that matches the specified cognit...",
        "chunk_id": "Cognitive_Twins_Practical_Work_Proposal (1):p0003:c0000",
        "chunk_text": "3\nc) Phase 3: Core Cognitive Module Implementation: We will implement three primary programmatic modules\nthat dynamically update a structured \u201cControl Panel\u201d input. This strategy leverages an Instruction fine-tuned LLM\nthat is trained to read this Control Panel and generate behavior that matches the specified cognitive state to simulate\ncore cognitive functions and individual differences. The modules function as follows:\n\u2022 Affect Module (Emotional State Modulator): A state-machine implementation tracking dynamic emotional\nstates (confident, confused, anxious, engaged) based on interaction history and performance outcomes. This\nmodule will modify LLM prompt components in real-time to reflect emotional influences on communication\nstyle, response length, and cognitive processing approach. Here we update the emotion field of the Control\nPanel (e.g., \u201cemotion\u201d: \u201cprimary\u201d: \u201canxious\u201d, \u201cconfidence\u201d: 0.3), which the LLM interprets to modulate its\ncommunication style, response length, and cognitive processing approach.\n\u2022 Attention Module (Focus Manager): A dynamic attention modeling system that simulates varying atten-\ntion spans and distractibility patterns. Drawing from cognitive models of attention and executive function\n[6][56][56], this module will implement time-decay functions for attention, threshold-based coherence degra-\ndation, and attention reset mechanisms triggered by direct engagement or environmental changes. Here we\nupdate the focus field of the Control Panel in real-time (e.g., \u201cfocus\u201d: \u201clevel\u201d: 0.4), which instructs the LLM\nto adjust its response coherence and relevance.\n\u2022 Memory and Decoding Module (Memory Filter): A cognitive processing filter that models information\nprocessing variations, particularly those reflecting a spectrum of information processing speeds and accuracies\n[33][43][19]. This system will introduce controlled character-level noise (e.g., b/d reversals, transposition errors)\nwith probability distributions calibrated to diverse reading patterns, affecting information encoding and retrieval\naccuracy. Here we update the memory field of the Control Panel (e.g., \u201cmemory\u201d: \u201cdecoding fidelity\u201d: 0.85),\naffecting information encoding and retrieval accuracy.\nd) Phase 4: Validation and Integration: The complete architecture will undergo comprehensive validation\nthrough multiple evaluation methodologies: human expert blind rating of agent believability and archetype consis-\ntency, quantitative analysis of behavioral differentiation between agent types, and integration testing within educa-\ntional simulation environments. We will also explore the proposed \u201cCommittee of Agents\u201d alternative architecture as\nan ablation study,"
      }
    ],
    "severity": "Medium",
    "actions": [
      "Search for evidence that explicitly discusses the evaluation of LLM reading comprehension accuracy.",
      "Determine if the 'Time-on-Task' or 'Frequency of Off-Topic Utterances' metrics in E1 could indirectly reflect comprehension issues.",
      "Investigate if the 'Human Expert Ratings' mentioned in E1 could be used to assess reading comprehension."
    ]
  }
]