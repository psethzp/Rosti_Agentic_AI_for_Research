[
  {
    "page": 1,
    "text": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement\nZhengxian Yang1*,\nShi Pan1*,\nShengqi Wang1*,\nHaoxiang Wang1,\nLi Lin2,\nGuanjun Li3,\nZhengqi Wen1\u2020,\nBorong Lin1\u2020,\nJianhua Tao1\u2020,\nTao Yu1\u2020\n1Tsinghua University\n2Migu Beijing Research Institute\n3Institute of Automation, Chinese Academy of Science\n{zx-yang23, ps23, shengqi-21, whx22}@mails.tsinghua.edu.cn\nlilin@migu.chinamobile.com\nguanjun.li@nlpr.ia.ac.cn\n{zqwen, linbr, jhtao, ytrock}@tsinghua.edu.cn\nFigure 1. We introduce ImViD, a dataset for immersive volumetric videos. ImViD records dynamic scenes using a multi-view audio-video\ncapture rig moving in a space-oriented manner, which provides a new benchmark for volumetric video reconstruction and its application.\nAbstract\nUser engagement is greatly enhanced by fully immersive\nmulti-modal experiences that combine visual and auditory\nstimuli.\nConsequently, the next frontier in VR/AR tech-\nnologies lies in immersive volumetric videos with com-\nplete scene capture, large 6-DoF interaction space, multi-\nmodal feedback, and high resolution&frame-rate contents.\nTo stimulate the reconstruction of immersive volumetric\nvideos, we introduce ImViD, a multi-view, multi-modal\ndataset featuring complete space-oriented data capture and\nvarious indoor/outdoor scenarios. Our capture rig supports\nmulti-view video-audio capture while on the move, a capa-\nbility absent in existing datasets, significantly enhancing the\ncompleteness, flexibility, and efficiency of data capture.\nThe captured multi-view videos (with synchronized au-\ndios) are in 5K resolution at 60FPS, lasting from 1-5 min-\nutes, and include rich foreground-background elements,\nand complex dynamics. We benchmark existing methods us-\ning our dataset and establish a base pipeline for construct-\n*Equal contributions. \u2020Corresponding author.\nZhengxian Yang, Shengqi Wang, Haoxiang Wang, Jianhua Tao and\nTao Yu are at BNRist&Department of Automation; Shi Pan and Borong\nLin are at Department of Building Science; Zhengqi Wen is at BNRist.\ning immersive volumetric videos from multi-view audiovi-\nsual inputs for 6-DoF multi-modal immersive VR experi-\nences. The benchmark and the reconstruction and interac-\ntion results demonstrate the effectiveness of our dataset and\nbaseline method, which we believe will stimulate future re-\nsearch on immersive volumetric video production.\n1. Introduction\nHigh-quality and high-fidelity digital modeling of the real\nworld has long been a topic of significant interest. With\nbreakthroughs in 3D reconstruction algorithms and virtual\nreality technology, the generation of such visually com-\npelling content is gradually becoming feasible, whether for\nreproducing past events and memories or remotely viewing\ncurrent occurrences. Achieving complete user immersion is\na goal pursued by many researchers.\nWhat defines an immersive volumetric video?\nDwelling on previous tests [1, 6] and user feedback [2,\n42], we believe that immersive media should have the fol-\nlowing four characteristics: i) Full 360\u00b0 foreground and\nbackground, ii) High-quality 6-DoF interaction, which cur-\nrent volumetric video can achieve, iii) Multimodal experi-\nence with light and sound, and iv) High frame rate and long-\narXiv:2503.14359v1  [cs.CV]  18 Mar 2025"
  },
  {
    "page": 2,
    "text": "duration dynamic content. We refer to a media format that\nmeets all these criteria as immersive volumetric video.\nHowever, with the development of camera equipment,\ndespite many efforts using monocular handheld cameras or\nfixed camera arrays to build datasets, none of these solutions\nfully address all four requirements. For example, Panoptic\nSports [19], ZJU-Mocap [44] and DiVa-360 [37] achieved\nhuman/object-centric 360\u00b0 capture but lack complex back-\ngrounds and multimodal data; Google\u2019s immersive light\nfield [6] achieves 6-DoF volumetric video in an inside-\nlooking-out manner but lacks multimodality and high frame\nrate, long-duration dynamic content; Replay [49] introduces\nsound, but the camera setup is not suitable for human view-\ning habits thus cannot achieve high-quality 6-DoF interac-\ntion. Both academia and industry require an appropriate\ncapture method to support the development of immersive\nvolumetric video technology.\nAdditionally, NeRF [41]\u2019s emergence has increased at-\ntention to reconstruction and rendering techniques, mak-\ning it easier to generate high-fidelity interactive 3D scenes\nfrom multiple views. 3DGS [23] and its subsequent im-\nprovements [10, 29, 38, 40, 57, 59, 61] have further boosted\nrendering speed and quality, fueling peoples\u2019 expectations\nfor rapid real-world modeling and its applications in in-\ndustries such as education, healthcare, and entertainment.\nThis can be seen as an evolutionary opportunity for immer-\nsive volumetric video. However, current technologies face\nchallenges. In dynamic light field reconstruction, neither\nexplicit [22, 24, 30, 35, 55] nor implicit neural represen-\ntation based [4, 8, 14, 28, 48, 51, 53] methods have been\nable to achieve a practical balance between foreground-\nbackground reconstruction accuracy, rendering speed, spa-\ntiotemporal consistency, and storage efficiency in dynamic\nscenes; In the realm of sound field reconstruction, it is typ-\nically necessary to collect a large amount of binaural audio\nor room impulse response data from different locations for\nmodel training, which is user-unfriendly in practical use.\nTo better support the development of immersive volu-\nmetric video technology, we have developed a moving cap-\nture rig (with 46 synchronized cameras, as shown in Fig.3)\nand designed two space-oriented data capture strategies. We\nhave constructed an audiovisual dataset containing 7 in-\ndoor and outdoor scenes, with a 360\u00b0 field of view, large\n6-DoF interaction space, and high-resolution, long-duration\ndynamic contents. We also validate this dataset using the\nlatest novel view synthesis and 3D reconstruction methods.\nFinally, a complete pipeline for constructing light and sound\nfields from this type of data was proposed, which initially\nachieved immersive volume video.\nOur main contributions can be summarized as follows:\n\u2022 We propose to our knowledge the first and probably the\nhighest-quality multi-view dataset ImViD for immersive\nvolumetric video. It features 360\u00b0 foreground and back-\nground capture, long-duration video&audio data under\nhigh resolution@5K, high frame rate@60FPS.\n\u2022 We conduct extensive experiments to validate the effec-\ntiveness of the proposed datasets by evaluating and ana-\nlyzing the performance of several current baselines.\n\u2022 Follow the proposed pipeline shown in Figure 2, We fi-\nnally achieve the reconstruction of immersive volumetric\nvideo and integrate it for immersive VR experience.\n2. Related Work\nDatasets for Dynamic Novel View Synthesis.\nThe\ndatasets for dynamic scene reconstruction can be classified\nas monocular-based and multi-view-based.\nMonocular acquisition systems are popular due to their\nlow cost and ease of construction. such as HyperNeRF [43],\nDynamic Scene Dataset [60], and D2NeRF [56]. However,\nthese datasets suffer from low resolution, limited capture\nspace, and the limited durations. Although NeRF On-the-\ngo [45] allows for larger capture ranges by walking while\nshooting, high-quality reconstructions are confined to the\ncapture path, and the small field of view (FOV) limits pro-\nlonged observations of specific scene positions.\nMulti-camera data collection has gained significant at-\ntention due to its ability to provide a larger FOV and richer\ndetails, such as Immersive Light Field dataset [6], Tech-\nnicolor [46], UCSD Dynamic Scene Dataset [34] and The\nPlenoptic Dataset [28], followed by [27, 33, 54]. How-\never, all these setups remain static during capture, limiting\nthem to frontal views and hindering 360\u00b0 reconstruction.\nAdditionally, the video sequences are typically short, with a\nmaximum duration of 2 minutes (often less than 30 seconds)\nand a maximum resolution of 4K, which is insufficient for\nimmersive VR experiences.\nMoreover, the previously mentioned datasets lack sound\nrecordings, leading to the loss of immersion without mul-\ntimodality. The Replay dataset [49] addresses this by fo-\ncusing on long sequences with professional actors in famil-\niar settings. However, aside from the head-mounted cam-\neras that enables rotation with head, all other cameras re-\nmain static. Furthermore, the DSLR arrangement does not\nalign with human viewing habits in VR, making them un-\nsuitable as benchmarks for novel view synthesis tasks. Re-\ncent work [9] presents a dataset with a 360\u00b0 camera and au-\ndio capture device but constrained by a fixed-point shooting\nstrategy, resulting in sparse viewpoints that hinder the re-\nconstruction of high-quality dynamic scenes. More details\nabout datasets can be found in Tab. 1.\nDynamic Light&Sound Field Reconstruction.\nTempo-\nral variations in input images make reconstructing dynamic\nscenes more challenging than static ones. The key issue\nis efficiently representing 4D scenes while ensuring spa-\ntiotemporal consistency and high accuracy.\nTraditional"
  },
  {
    "page": 3,
    "text": "Table 1. Existing real-world datasets for dynamic novel view synthesis (full version in the supplementary materials).\nDatasets\nNo.Scene\nOutdoor/Indoor\nCameras\nMobility\nResolution\nAngles\nDuration\nFPS\nMultimodality\nContent\nPanopticSports [19]\n65\nIndoor\n480 cameras\nStatic\n640\u00d7480\n360\u00b0\n5mins\n25\n%\nHuman-centric actions\nTechnicolor [46]\n12\nIndoor\n16 cameras\nStatic\n2048\u00d71088\nFrontal\n2s\n30\n%\nHas a number of close-ups sequences, captured medium\nangle scenes and other animated scenes where the move-\nment does not come from a human\nImmersive-Lightfield [6]\n15\nboth\n46 cameras\nStatic\n2560\u00d71920\nFrontal\n10-30s\n30\n%\nSimple and slow motion of human,animals,objects\nDynamic Scene Datasets\n(NVIDIA) [60]\n8\nOutdoor\n1 Mobile phone\n/12 cameras\nFixed-point Waving\n/Static\n1920\u00d71080\nFrontal\n5s\n60\n%\nSimple body motions (facial, jump, etc)\nUCSD Dynamic [34]\n96\nOutdoor\n10 cameras\nStatic\n1920\u00d71080\nFrontal\n1-2mins\n120\n%\nVarious visual effects and human interactions\nZJU-Mocap [44]\n10\nIndoor\n21 cameras\nStatic\n1024\u00d71024\n360\u00b0\n20s\n50\n%\nSimple body motions (punch, kick, etc.)\nPlenoptic Dataset (DyN-\neRF/Neural 3D) [28]\n6\nIndoor\n21 cameras\nStatic\n2704\u00d72028\nFrontal\n10-30s\n30\n%\nContains high specularity, translucency and transparency\nobjects, motions with changing topology, selfcast moving\nshadows, various lighting conditions and multiple people\nmoving around in open living room space\niPhone Datasets [15]\n14\nboth\n1 hand-held phone\n/2 cameras\nFixed-point Waving\n/Static\n640\u00d7480\nFrontal\n8-15s\n30/60\n%\nFeaturing non-repetitive motion, from various categories\nsuch as generic objects, humans, and pets\nENeRF-Outdoor [33]\n4\nOutdoor\n18 cameras\nStatic\n1920\u00d71080\nFrontal\n20-40s\n30\n%\nComplex human motions\nReplay [49]\n46\nIndoor\n12 cameras\nStatic\n3840\u00d72160\n360\u00b0\n5mins\n30\n\u2713(Audio)\nDancing, chatting, playing video games, unwrapping\npresents, playing ping pong\n360+X [9]\n28\nboth\n1 360\u00b0cameras and\n1 Spectacles cameras\nStatic\n5760\u00d72880\n360\u00b0\n10s (2152 sequence)\n30\n\u2713(Audio)\nCapture in 17 cities across 5 countries.Panoptic perspec-\ntive to scene understanding with audio\nIm-ViD(Ours)\n7\nboth\n46 cameras\nMoveable\n5312\u00d72988\nFrontal and 360\u00b0\n1-5mins\n60\n\u2713(Audio)\nSeven common indoor and outdoor scenes in daily life,\nincluding opera, face-to-face communication, teaching,\ndiscussion, music performance, interaction with pets, and\nplaying. Each scene has high-quality synchronized multi-\nview video and audio with a duration of more than 1\nminute, and contains rich elements such as various small\nobjects, glass, and changes in light and shadow\nmethods estimate varying geometries like surfaces [11] and\ndepth maps [21] to handle dynamics. Recently, efforts have\nshifted towards adapting neural radiance field representa-\ntions for dynamic scenes. DyNeRF [28] pioneer in the dy-\nnamic reconstruction with Neural radiance field. Followed\nworks such as [4, 8, 14, 27, 48, 51, 53] make efforts to im-\nprove the high-fidelity and efficiency in the 4D neural repre-\nsentation. However, the common challenges these methods\nface are memory cost and the modeling of spatiotemporal\ncomplexity. The latest work have extended 3DGS to dy-\nnamic scenes for high-quality real-time rendering. While\nmodeling dynamic scenes as static ones is straightforward,\nmethods relying solely on monocular data [20, 32] is not\nsuitable for this paradigm, and face challenges of adding\nconstraints or priors to better supervise training.\nOther\nworks support multi-view data as input. [39] and [52] ini-\ntialize each frame from the previous one to build Gaussian\npoint clouds per-frame, leading to high memory and sud-\ndenly appearing or disappearing artifacts. 4DGS [55] in-\ntroduces Hexplane to encode spatiotemporal information,\nusing MLPs to predict Gaussian properties change. Sub-\nsequent works [13, 17, 26] further add geometric and opti-\ncal flow constraints to improve frame-to-frame motion but\nperform well only on short sequences with small actions.\nOther approaches [22, 24, 30, 35] employ the explicit para-\nmetric form (such as radial basis functions) of 3DGS to en-\nhance spatiotemporal consistency. Some methods [12, 58]\nfocus on representation with dual quaternions, 4D spheri-\ncal harmonics, or rotors for end-to-end training. More im-\nprovements such as parallel training [50] and using code-\nbooks and masks to enhance scene compactness [25], show\nthe potential in efficient and high-quality representation of\nlong-term complex dynamic scenes.\nIn addition to light field reconstruction, reconstructing\nthe sound field from multimodal data is equally important,\noften referred to as novel-view acoustic synthesis. For the\nnovel-view acoustic synthesis task, AV-NeRF [31] synthe-\nsizes spatial audio by leveraging visual features from ren-\ndered images. NeRAF [7] further decouples the camera\nand microphone poses used for training. AV-GS [5] incor-\nporates a 3D scene representation during sound field syn-\nthesis, addressing the issue where previous methods over-\nlooked the contribution of the broader 3D scene geometry.\nSOAF [16] considers occlusion in the light field when syn-\nthesizing the sound field. However, the above novel-view\nacoustic synthesis tasks typically require a large amount of\nmultimodal training data collected from different locations,\nwhich is often user-unfriendly in practical applications.\nMoreover, these methods generally cannot solve the prob-\nlem of sound field synthesis for moving sound sources. This\npaper proposes a training-free novel-view acoustic synthe-\nsis approach for our dataset, which also supports sound field\nsynthesis for moving sound sources.\n3. ImViD Dataset\nOur goal is to build an immersive volumetric video dataset\nof real world that integrates both foreground and back-\nground, providing assistance for the development of re-"
  },
  {
    "page": 4,
    "text": "Global / Temporal Color Transformation\nC' = W * C + T\nAmplitude \nScaling\n\u00b7Temporal Opacity\n\u00b7Polynomial Motion\n\u00b7Polynomial Rotation\nSpacetime GS Based\n\u00b7SHs Color\nHRTF Dataset\nSpatialization\nRaw Sound Input\na) Multi-View Video & Audio Capture\n    (Fixed Rig / Moving Rig)\nc) Immersive Volumetric Video for \n....Deep VR Immersion\nTimecode Based -\nVideo & Audio Synchronization\nFixed / Moving Rig\nDual Capture Strategies\nb2) Sound Field Reconstruction \nSound Localization\nKeypoints Detection\nVR\nPosition: xyz\nDirection: \u03b8\nSpatial Sound Generation\n1.Sound Direction \nMapping\n2.Sound Distance \nCalculation\nKeypoints Triangulation\nReference \nCoordinate \nOrigin: O\nSpatial \nSound\nHRTF\nSound\nDirection\nSound\nDistance\n3.Sound\nAuralization\nb1) Dynamic Light Field Reconstruction \nCamera Calibration\nFully Explicit 4D Gaussian Splatting\nInitialization\nFrame Extraction\nFigure 2. The pipeline to realize the multimodal 6-DoF immersive VR experiences. We applied a carefully designed rig to (a) simulta-\nneously capture multi-view video and audio. The (b1) presents our reconstruction of dynamic light field based on STG [30] while (b2)\ndemonstrates the construction process of sound field. We have achieved better results than the original algorithm in long-term dynamic\nscenes by incorporating affine color transformation and t-dimensional density control. Ultimately, we achieve a 6-DoF immersive experi-\nence in both light and sound fields, and also benchmark on recent representative methods like 4DGS [55] and 4Drotor [12] to demonstrate\nthe effectiveness of both our dataset and baseline method.\nlated research such as spatial video and VR/AR applica-\ntion. Therefore, we have meticulously refined every aspect\nof the process, from equipment setup and scene selection\nto shooting strategies and data processing, ensuring main-\ntaining the elements of interest to researchers to the greatest\nextent. The released ImViD dataset comprises a total of\n7 large scenes, including 5 indoor and 2 outdoor environ-\nments. The content features a diverse range of common ac-\ntivities, such as opera, meeting, teaching, and playing on the\ngrass. Each scene contains a set of static photos, multi-view\ntime-synchronized video&audio sequences at 5K resolution\nand 60 FPS, employing two different collection strategies,\nalong with calibrated camera poses.\n3.1. Data Acquisition\nThe handheld monocular camera is easy to move, providing\nlimited perspectives from various locations. In contrast, the\nfixed camera arrays, while stationary, offers dense perspec-\ntives within a limited range. We aim to combine the advan-\ntages of both to design an effective data collection system\nand strategy for fully immersive VR experience\u2014a feature\nnot found in existing datasets. As shown in Figure 3, our\ncapture system consists of more than 40 GoPro cameras in-\nstalled on a hemispherical surface mounted on a remotely\ncontrollable mobile car, all of which are connected to a PC\nto receive synchronous control commands. The system is\ncarefully designed to be at a height similar to that of a hu-\nman, and the camera arrangement better aligns with human\nviewing habits compared to other existing devices. This rig\ncan synchronously collect about 1,000 images at 5K reso-\nlution within a space of at least 6 m\u00b3 in 2 min. It is also\ncapable of synchronously capturing video and audio at 60\nFPS and 5K resolution for 30 min (limited by heat dissipa-\ntion). Thus, our capture strategy is as follows:\nStep1: Efficient high-quality, high-density 360\u00b0 image ac-\nquisition of static environments.\nStep2:\nContinuous synchronous collection of dynamic\nscenes under two different strategies.\ni) Fixed-point shooting. The cart remains stationary dur-\ning the capture, focusing on capturing dynamic details.\nii) Moving shooting. The cart slowly moves while the\nscene is in motion, providing additional level of details and\nlarger exploratory space.\nThroughout the capture process, we ensure strict syn-\nchronization of the world time codes across all cameras.\nWe carefully maintained consistent focal lengths, exposure,"
  },
  {
    "page": 5,
    "text": "Figure 3. Our rig support two kinds of capturing strategies for high\nresolution, high frame rate and 360\u00b0 dynamic data acquisition.\nand white balance settings for each scene, based on the re-\nsults of manual iterative testing of camera settings, thereby\nminimizing hardware-induced variations in data quality. To\nachieve high-quality audio, we increased the noise reduc-\ntion coefficient of the cameras and minimized noise gener-\nation as much as possible.\n3.2. Data Processing\nFor the photos from Step1, we completed the calibration in\nCOLMAP [47] by combining the cameras\u2019 prior parame-\nters and their relative positions. For the video&audio from\nStep2, we first aligned the multi-view sequences for each\nscene using the world time codes. This step is crucial as\nthe quality of the alignment directly determines the quality\nof dynamic scene reconstruction and significantly impacts\nwhether the multi-view audio can be used for constructing\na 6-DoF sound field. Our time code alignment error is at\nthe level of approximately 2 milliseconds. Subsequently,\nwe performed frame extraction from the videos, again uti-\nlizing the GoPro camera parameters as priors to complete\nthe calibration.\nIt is important to note that for the dynamic scene data\ncaptured with a moving rig, we also performed time code\nalignment. However, this data poses significant challenges\nfor existing calibration methods, often resulting in errors\nand floaters. There are also no solutions for reconstruction\nalgorithms specifically addressing dynamic scenes from\nmoving rig currently. Therefore, we do not provide the cal-\nibrated results for this type of data at this time. But we\nbelieve that this data will greatly contribute to the advance-\nment of the field, and thus we will also publicly releasing\nthe data which has been time-synchronized.\n3.3. Dataset Analysis\nDiversity. Our dataset, as depicted in Table 2, includes 7\ncommon indoor and outdoor scenes, each comprising one\nor more takes, totaling 16 video sequences. Each scene is\nFigure 4. Calculation method for spatiotemporal capture den-\nsity. 1) the capture strategy of handheld monocular camera 2) rep-\nresents the fixed camera array 3) Our rig covers a volume of 0.6\u00d7\n\u03c0 \u00d70.52 m\u00b3 over 5 seconds.\nrich in foreground and background elements, meticulously\ncrafted to preserve the authenticity of the environment. The\ncharacters, clothing, and motions within each scene exhibit\na diverse range, set against varying ambient lighting condi-\ntions. We also included areas of interest like tables, chairs,\nwindows as much as possible for researchers.\nQuality. Leveraging a total of 39 cameras, we captured dy-\nnamic scenes at a resolution of 5312\u00d72988 and 60 FPS. The\nimages of static environment is 5568\u00d74176.\nDuration. As outlined in Table 2, each scene in our dataset\ncontains at least 1 minute of footage. The longest sequence\nin our forthcoming public dataset spans approximately 5\nminutes. The total processed data duration amounts to 38\nminutes 46 seconds, including 139,560 frames per camera.\nCoverage. As highlighted in Table 2, we conducted im-\nage acquisition from over 1,000 viewpoints before record-\ning dynamic content, thereby offering a 180\u00b0-360\u00b0 view-\ning space in VR. For scenes employing the mobile shoot-\ning strategy, we introduced a metric termed \u2019spatiotemporal\ncapture density,\u2019 quantifying the captured volume covered\nby our rig per second, as shown in Figure 4. This metric,\nunavailable in current datasets (where static camera arrays\nare deemed 0 and handheld monocular cameras follow a\nsingle trajectory without capturing volume), registers at ap-\nproximately 0.10 m3/s in our dataset.\n4. Benchmarks and Experiments\nTo comprehensively assess the effectiveness of the dataset,\nwe selected three scenes (2 indoor and 1 outdoor) to test the\nlatest dynamic light field reconstruction algorithms, provid-\ning a benchmark for research in areas such as novel view\nsynthesis. We also proposed an improvement strategy based\non one of the methods.\nFurthermore, we presented and\nvalidated a feasible pipeline for constructing 6-DoF sound\nfields from multi-view audio sequences. In this section, we"
  },
  {
    "page": 6,
    "text": "Table 2. The detailed statistics of our ImViD dataset. #Take represents the number of different episodes we collected in this scene.\n#Strategy represents the strategies used in Step2. 1 indicates only (i) was used in Step 2, 2 indicates that both were used. Avg.S-T Density\nindicates the spatiotemporal coverage density of our collection method, measured by the captured volume size per second.\nName\n#Camera\n#Static viewpoints\n#Take\n#Strategy\nAvg.S-T Density(m\u00b3/s)\nViewing Space\nAvg.Duration\nStorage(GB)\nScene1 Opera\n39\n1152\n2\n1\n-\n180\u00b0\n3min22s\n226\nScene2 Laboratory\n39\n1225\n2\n2\n0.10\n360\u00b0\n1min42s\n137.3\nScene3 Classroom\n39\n1223\n2\n2\n0.10\n360\u00b0\n4min42s\n497\nScene4 Meeting\n39\n1223\n1\n1\n-\n360\u00b0\n3min16s\n114\nScene5 Rendition\n39\n1620\n4\n2\n0.10\n360\u00b0\n2min02s\n516\nScene6 Puppy\n39\n1404\n3\n2\n0.10\n360\u00b0\n1min50s\n359\nScene7 Playing\n39\n1224\n2\n2\n0.10\n360\u00b0\n1min10s\n220\nTotal\n-\n-\n16\n-\n-\n-\n38min46s\n2069.3\nwill first introduce the selected light field pipeline and our\nproposed sound field pipeline (Sec 4.1), then provide details\non the experimental setup (Sec 4.2).\n4.1. Baseline\nDynamic Light Field Reconstruction.\nWe conducted a\nthorough survey of the current dynamic reconstruction algo-\nrithms, prioritizing rendering speed and support for multi-\nview data input. We finally decided to establish a bench-\nmark on 3DGS-based method from three paradigms.\n4DGS [55] introduces Hexplane to encode spatiotem-\nporal information to feature fd and use MLPs decoders\nD = {\u03d5x, \u03d5r, \u03d5s} to predict gaussians\u2019 changes in posi-\ntion \u2206\u03c7 = \u03d5x(fd) , rotation \u2206r = \u03d5r(fd), and scaling\n\u2206s = \u03d5s(fd). Utilizing the sparse point cloud from frame\n0 as initialization and training static gaussians for the first\n3000 iterations, the dynamic scene G is represented as:\n  G(\\ chi  ', r ',s ' )=G ( \\ chi  + \\Delta \\chi ,r+\\Delta r, s+\\Delta s) \n(1)\n4DrotorGS [12] is a representative method of dimension-\nality expansion.\nIt extends 3DGS to a four-dimensional\nGaussian ellipsoid by introducing the concept of rotor, ex-\npanding the covariance matrix to four dimensions, i.e.:\n  G_{4 D }( x\n) =e^{-\\fr ac \n{1}{2}(x-\\mu _{4D})^{T}\\Sigma _{4D}^{-1}(x-\\mu _{4D}) } \n(2)\nEach frame of the scene corresponds to a time slice of the\n4D Gaussian ellipsoids followed by \u03b1-blending.\nSTG [30], on the other hand, use polynomial fitting for\nthe motion and rotation of the 3DGS, and employs an RBF\nfunction to model the opacity changes over time. It con-\ncatenates the sparse point clouds from all frames as the ini-\ntialization, and the final representation of each point in the\nscene is:\n  \\al p ha _{ i}(t)=\n\\si g ma _{i} (t) \\exp ( -\\frac {1}{2}(x-\\mu _{i}(t) )^{T}\\Sigma _{i}(t)^{-1} (x-\\mu _{i}(t) )) \n(3)\nAlthough we ensured consistent white balance and other\nsettings across cameras based on the characteristics of\nscenes during shooting, color differences still exist due to\nfactors like occlusions when capturing from different an-\ngles.\nDuring experiments, we found that while existing\nmethods fit well to the training views, there is a noticeable\ncolor shift during viewpoint transitions in the reconstructed\nlight field, manifesting as flickering and floaters. This is\nespecially apparent when training in segments, where the\ndifferences between segments are more pronounced, signif-\nicantly reducing immersion. We improved the original STG\n(named as STG++) by introducing global, time-invariant\ncolor mapping during training.\nDetailed implementation\ncan be seen in supplementary materials.\nSound Field Reconstruction.\nTaking the recording mi-\ncrophone as the origin of the coordinate system, we de-\nnote the coordinates of the sound source as s(t)\n=\n{xs(t), ys(t)}, the user wearing the VR device as l(t) =\n{xl(t), yl(t)}, and the angle of deviation from the y-axis in\nthe counterclockwise direction as \u03b8l(t), where t is the time\nindex. Based on these premises, generating the user\u2019s bin-\naural spatial audio can be divided into three parts: sound\ndirection mapping, sound distance mapping, and sound au-\nralization. Note that we assume there is only one dominat-\ning sound source (omnidirectional emitting) to simplify the\nsound field reconstruction.\nIn the sound direction mapping part, we calculate the di-\nrection of the sound source relative to the user\u2019s ears, \u03b8s(t),\n  \\th e ta _s(\nt) = \\arcc\nos \\frac {\\mathbf {v}_1(t)\\mathbf {v}_2(t)}{|\\mathbf {v}_1(t)||\\mathbf {v}_2(t)|}, \n(4)\nwhere\n  \\ma t hbf {v } _1(t) &= [x _ s(t)-x_ l\n(t)\n, y_s ( t) -y_ l(t)]^T, \\\\ \\mathbf { v}_2(t) &= [-\\sin {\\left (\\theta _l(t)\\right )}, \\cos {\\left (\\theta _l(t)\\right )}]^T,\n(6)\nIn the sound distance mapping part, we calculate the scaling\nof the sound source when it reaches the user\u2019s ears, relative\nto the original recording. We denote this scaling parameter\nas \u03bb(t),\n  \\l a\nm\nbda (t )  = \\fr\na\nc {\\sq r t {x_s^ 2 (t)+y_ s ^2(t)}}{\\sqrt {(x_s(t)-x_l(t))^2+(y_s(t)-y_l(t))^2}}\n(7)"
  },
  {
    "page": 7,
    "text": "In the sound auralization part, we convert the original\nrecording into spatial audio based on \u03b8s(t) and \u03bb(t). In\nthe short-time Fourier transformation (STFT) domain, we\ndenote the audio from the left and right ears as AL(t, f)\nand AR(t, f), respectively, where f is the frequency index.\nThen AL(t, f) and AR(t, f) can be written as\n  A_L (t , f) &= F_L(\\the ta _s(t ))\n F_\nl(t,f ) A _O(t,f) \\label  {left} \\\\ A_R(t,f) &= F_R(\\theta _s(t)) F_l(t,f) A_O(t,f), \\label {right}\n(9)\nwhere FL(\u03b8s(t)) and FR(\u03b8s(t)) are head-related transfer\nfunctions (HRTF) based on \u03b8s(t), Fl(t) denotes the room\nimpulse response (RIR) at the user\u2019s location and AO(t, f)\nis the sound emitted from the speaker in the STFT do-\nmain. We utilize \u03b8s(t) to retrieve FL(\u03b8s(t)) and FR(\u03b8s(t))\nfrom the SADIE II dataset [3]. In order to model Fl(t)\nand AO(t, f), we make the following assumptions: Fl(t)\nchange between different location at the same time index t\nonly depending on the parameter \u03bb(t), i.e.,\n  F_l (t , f) = \\la mbda (t) F(t,f),\n(10)\nwhere F(t) is the RIR at the recording microphone. Using\nthe above assumption, Eq.(8) and Eq.(9) can be rewritten as\n  A_L (t , f) &= \\lambda (t)  F_\nL(\\t\nheta _s ( t)) A(t,f), \\\\ A_ R(t,f) &= \\lambda (t) F_R(\\theta _s(t)) A(t,f),\n(12)\nwhere A(t, f) = F(t, f)AO(t, f) is the audio at the record-\ning microphone.\n4.2. Experimental Settings\nTraining setups. The training for all light field data was\ncompleted on a single NVIDIA A100 GPU. Due to memory\nlimitations, we split the video and trained a model every\n60 frames, total 300 frames. We also followed the original\ntrain-test-split strategy of the baseline for our experiments.\nMetric.\nWe analyzed the quality of dynamic light field\nreconstruction using the most common evaluation metrics\nPSNR, SSIM, and LPIPS(alex).\n5. Results and Analysis\n5.1. Validation of Light Field Data\nQuantitative Results.\nAs illustrated in Table 3, 4 and Fig-\nure 5, our indoor and outdoor data both yield correct re-\nsults across three 3DGS-based paradigms. STG++ outper-\nforms all baselines in all scenes, delivering slightly better\nresults, the highest rendering speed, and a moderate model\nsize. We also found that, since 4DGS trains using the sparse\npoint cloud from the 0th frame and limits the number of fi-\nnal points, its performance is significantly lower than other\npipelines on texture-richer outdoor scenes, such as Scene6\nTable 3. Test views performance of 3DGS-based dynamic scene\nreconstruction method on ImViD dataset.\nMethod\nScene1 Opera girl\nScene2 Laboratory\nScene6 Puppy\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\n4DGS\n23.227\n0.753\n0.410\n25.798\n0.890\n0.176\n18.121\n0.222\n0.711\n4DRotor\n27.263\n0.775\n0.328\n28.007\n0.917\n0.098\n17.916\n0.298\n0.331\nSTG\n28.482\n0.786\n0.287\n26.306\n0.910\n0.114\n20.497\n0.594\n0.211\nSTG++\n31.240\n0.799\n0.277\n27.581\n0.916\n0.107\n20.533\n0.597\n0.202\nTable 4. The storage size of training results from different methods\nand their rendering speed on a single A100 GPU. *CPU version.\nScene1 Opera girl\nScene2 Laboratory\nScene6 Puppy\nMem(MB)\nFPS\nMem(MB)\nFPS\nMem(MB)\nFPS\n4DGS\n101.05\n46.22\n105.02\n44.93\n101.65\n41.86\n4DRotor*\n5818.61\n1.00\n6867.18\n0.67\n4415.82\n0.56\nSTG\n382.41\n96.10\n318.41\n133.65\n1287.55\n100.00\nSTG++\n387.17\n108.89\n329.07\n137.78\n1276.21\n110.47\nTable 5. User study for the sound field construction.\nAuditory Spatial Perception\nSound Quality\nImmersiveness\nVery poor\n0.00%\n0.00%\n0.00%\nPoor\n0.00%\n0.00%\n0.00%\nFair\n14.28%\n19.04%\n9.52%\nGood\n23.80%\n33.33%\n42.85%\nExcellent\n61.90%\n47.61%\n47.61%\npuppy. Additionally, due to the density control in the time\ndimension implemented by 4Drotor, it performs better in\nframes and regions with more significant motion, which is\nan aspect worth referencing and adopting in future work.\nMore results can be seen in the supplementary material.\nQualitative Results.\nAs shown in Figure 6, Our STG++\nachieves better temporal continuity in the color of pixels at\nthe same location. There are almost no abrupt transitions\nboth within and between segments. Details are showed in\nthe supplementary for a more intuitive experience.\n5.2. Effectiveness of Sound Field Data\nQuantitative Results.\nSince our sound field reconstruc-\ntion algorithm is non-trainable, the reconstructed spatial au-\ndio lacks a corresponding ground truth, making it difficult\nto evaluate the results using quantitative metrics. Therefore,\nwe conducted a user study with 21 experts to assess the per-\nformance of the sound field reconstruction.\nEach participant rated the sound field reconstruction on\nthree dimensions: auditory spatial perception, sound qual-\nity, and immersiveness. Auditory spatial perception refers\nto the listener\u2019s ability to perceive the distribution and local-\nization of sound in space. Sound quality refers to the audio\nquality of the spatial audio compared to the corresponding\nmicrophone-recorded signal. Immersiveness refers to the\nlistener\u2019s sense of being in a space where the sound source\ngenuinely exists.\nThe results of the user study are shown in Table 5, with"
  },
  {
    "page": 8,
    "text": "Scene1 \nOpera\n4DGS\nGround Truth\n4Drotor\nSTG\nSTG++\nScene2 \nLab.\nScene6 \nPuppy\nCamera: 00    Frame: 0\nPSNR: 34.826    SSIM: 0.928  LPIPS:0.177\nPSNR: 35.501    SSIM: 0.930  LPIPS:0.174\nPSNR: 25.569    SSIM:0.873   LPIPS:0.199\nPSNR: 32.656   SSIM: 0.916  LPIPS: 0.184\nPSNR: 32.880    SSIM: 0.956  LPIPS:0.050\nPSNR: 33.551   SSIM: 0.959   LPIPS:0.047\nPSNR: 25.749    SSIM: 0.949  LPIPS:0.051\nPSNR: 30.806   SSIM: 0.933  LPIPS: 0.083\nCamera: 00    Frame: 60\nPSNR: 21.138   SSIM:  0.447  LPIPS:0.556\nCamera: 00    Frame: 120\nPSNR: 23.514    SSIM: 0.706   LPIPS:0.214\nPSNR: 23.518    SSIM: 0.706   LPIPS:0.213\nPSNR: 20.877   SSIM: 0.587   LPIPS: 0.354\nFigure 5. Comparison of the rendering results of four baselines on Scene 1 Opera, Scene 2 Laboratory, and Scene 6 Puppy.\nScene1 Opera\nOriginal STG\n300 frames\nWith Color Mapping Across Frames\n300 frames\nFigure 6. The continuity of pixels at the same location across dif-\nferent frames and segments. The visualizations present the RGB\n(middle) and brightness (right) variations.\nnumbers as percentages of participants. The majority of the\nparticipants (61.90%) rated the auditory spatial perception\nas excellent, 80.94% participants felt that the generated spa-\ntial audio did not exhibit significant degradation in qual-\nity, and 90.46% found the audio immersive, which demon-\nstrates the effectiveness of our data capture and sound field\nreconstruction methods.\n5.3. Multimodal VR Experiences.\nWe finally integrated light field and sound field reconstruc-\ntion results to achieve a 6-DoF multimodal immersive VR\nexperience with real-time rendering speed at 60 FPS on a\nsingle 3090 GPU. As shown in Figure 7 , the visual and\nauditory experiences correspond uniquely to the user\u2019s po-\nsition and viewing direction. The experience can be further\nexplored by listening through headphones with the supple-\nmentary video provided.\n3\n1\n2\n4\n5\n1\n2\n3\n4\n5\nFigure 7.\nVisualization of the interaction trajectory and corre-\nsponding visual&auditory results. Note that the multi-modal feed-\nback changes consistently with the positions and orientations.\n6. Conclusion\nIn this work, we introduce ImViD, to our knowledge, the\nfirst dataset for immersive volumetric video. This dataset\nencompasses seven large indoor and outdoor scenes, fea-\nturing rich foreground and background content that show-\ncases engaging and commonly encountered daily scenarios,\nalong with high-quality synchronized audio sequences. We\nvalidated the effectiveness of the dataset using the latest dy-\nnamic light field reconstruction algorithms, providing both\nquantitative and qualitative analyses of current limitations\nand potential improvements. Moreover, we propose a base\npipeline for constructing immersive volumetric video from\nthis data. We believe ImViD will facilitate the exploration\nof the limitations of current volumetric video reconstruction\nalgorithms and drive immersive VR/AR experiences."
  },
  {
    "page": 9,
    "text": "Acknowledgment\nThis work was supported by National Key R&D Program\nof China (No.2022YFF0902204), NSFC (No.62171255,\n52425801), Tsinghua University-Migu Xinkong Culture\nTechnology (Xiamen) Co.Ltd. Joint Research Center for In-\ntelligent Light Field and Interaction Technology, Guoqiang\nInstitute of Tsinghua University (No.2021GQG0001).\nReferences\n[1] https://www.apple.com/newsroom/2024/07/\nnew- apple- immersive- video- series- and-\nfilms-premiere-on-vision-pro/. 1\n[2] https://www.gracia.ai/. 1\n[3] Cal Armstrong, Lewis Thresh, Damian Murphy, and Gavin\nKearney.\nA perceptual evaluation of individual and non-\nindividual hrtfs: A case study of the sadie ii database. Ap-\nplied Sciences, 8(11):2029, 2018. 7\n[4] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16610\u201316620, 2023. 2, 3\n[5] Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiankang\nDeng, and Xiatian Zhu. Av-gs: Learning material and ge-\nometry aware priors for novel view acoustic synthesis. arXiv\npreprint arXiv:2406.08920, 2024. 3\n[6] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-\nson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay\nBusch, Matt Whalen, and Paul Debevec. Immersive light\nfield video with a layered mesh representation. ACM Trans-\nactions on Graphics (TOG), 39(4):86\u20131, 2020. 1, 2, 3, 4,\n8\n[7] Amandine\nBrunetto,\nSascha\nHornauer,\nand\nFabien\nMoutarde.\nNeraf:\n3d scene infused neural radiance\nand acoustic fields. arXiv preprint arXiv:2405.18213, 2024.\n3\n[8] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130\u2013141, 2023. 2, 3\n[9] Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiao-\nhan Hong, and Jianbo Jiao.\n360+x: A panoptic multi-\nmodal scene understanding dataset. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19373\u201319382, 2024. 2, 3, 1, 8\n[10] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin,\nYuexin Ma, Wenping Wang, and Xuejin Chen. Gaussianpro:\n3d gaussian splatting with progressive propagation. arXiv\npreprint arXiv:2402.14650, 2024. 2\n[11] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Den-\nnis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo. ACM Transactions on Graphics (ToG), 34(4):1\u201313,\n2015. 3\n[12] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wen-\nzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting:\nTowards efficient novel view synthesis for dynamic scenes.\nIn ACM SIGGRAPH 2024 Conference Papers, pages 1\u201311,\n2024. 3, 4, 6\n[13] Bardienus P Duisterhof, Zhao Mandi, Yunchao Yao, Jia-\nWei Liu, Mike Zheng Shou, Shuran Song, and Jeffrey Ich-\nnowski.\nMd-splatting: Learning metric deformation from\n4d gaussians in highly deformable scenes. arXiv preprint\narXiv:2312.00583, 2023. 3\n[14] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12479\u201312488, 2023. 2,\n3\n[15] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell,\nand Angjoo Kanazawa. Monocular dynamic view synthesis:\nA reality check. Advances in Neural Information Processing\nSystems, 35:33768\u201333780, 2022. 3, 8\n[16] Huiyu Gao, Jiahao Ma, David Ahmedt-Aristizabal, Chuong\nNguyen, and Miaomiao Liu. Soaf: Scene occlusion-aware\nneural acoustic field.\narXiv preprint arXiv:2407.02264,\n2024. 3\n[17] Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, and\nHouqiang Li.\nMotion-aware 3d gaussian splatting for\nefficient dynamic scene reconstruction.\narXiv preprint\narXiv:2403.11447, 2024. 3\n[18] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3. 6m: Large scale datasets and pre-\ndictive methods for 3d human sensing in natural environ-\nments. IEEE transactions on pattern analysis and machine\nintelligence, 36(7):1325\u20131339, 2013. 1\n[19] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei\nTan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart\nNabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and\nYaser Sheikh. Panoptic studio: A massively multiview sys-\ntem for social interaction capture. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 2017. 2, 3, 1, 8\n[20] HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-\nPellitero, Yiren Zhou, Zhihao Li, Nassir Navab, and Ben-\njamin Busam. Deformable 3d gaussian splatting for animat-\nable human avatars. arXiv preprint arXiv:2312.15059, 2023.\n3\n[21] Takeo Kanade, Peter Rander, and PJ Narayanan. Virtualized\nreality: Constructing virtual worlds from real scenes. IEEE\nmultimedia, 4(1):34\u201347, 1997. 3\n[22] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. A\ncompact dynamic 3d gaussian representation for real-time\ndynamic view synthesis, 2024. 2, 3\n[23] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler,\nand George Drettakis. 3D Gaussian Splatting for Real-Time\nRadiance Field Rendering. ACM Transactions on Graphics,\n42(4):1\u201314, 2023. 2\n[24] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis.\nDynmf: Neural motion factorization for real-time dynamic\nview synthesis with 3d gaussian splatting.\narXiv preprint\narXiv:2312.00112, 2023. 2, 3"
  },
  {
    "page": 10,
    "text": "[25] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan\nKo, and Eunbyung Park.\nCompact 3d gaussian splat-\nting for static and dynamic radiance fields. arXiv preprint\narXiv:2408.03822, 2024. 3\n[26] Deqi Li, Shi-Sheng Huang, Zhiyuan Lu, Xinran Duan, and\nHua Huang. St-4dgs: Spatial-temporally consistent 4d gaus-\nsian splatting for efficient dynamic scene rendering. In ACM\nSIGGRAPH 2024 Conference Papers, pages 1\u201311, 2024. 3\n[27] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and\nPing Tan.\nStreaming radiance fields for 3d video synthe-\nsis. Advances in Neural Information Processing Systems, 35:\n13485\u201313498, 2022. 2, 3, 1, 8\n[28] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5521\u20135531, 2022. 2, 3,\n1, 8\n[29] Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee\nLee, and Federico Tombari.\nGeogaussian:\nGeometry-\naware gaussian splatting for scene rendering. arXiv preprint\narXiv:2403.11324, 2024. 2\n[30] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaus-\nsian feature splatting for real-time dynamic view synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8508\u20138520, 2024. 2,\n3, 4, 6\n[31] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and\nChenliang Xu. Av-nerf: Learning neural fields for real-world\naudio-visual scene synthesis. Advances in Neural Informa-\ntion Processing Systems, 36:37472\u201337490, 2023. 3, 5\n[32] Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-\nPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao.\nGaufre: Gaussian deformation fields for real-time dynamic\nnovel view synthesis.\narXiv preprint arXiv:2312.11458,\n2023. 3\n[33] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou.\nEfficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia 2022 Conference Papers, pages 1\u20139, 2022. 2, 3, 1, 8\n[34] Kai-En Lin, Lei Xiao, Feng Liu, Guowei Yang, and Ravi Ra-\nmamoorthi. Deep 3d mask volume for view synthesis of dy-\nnamic scenes. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1749\u20131758, 2021. 2,\n3, 1, 8\n[35] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao.\nGaussian-flow: 4d reconstruction with dynamic 3d gaus-\nsian particle. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 21136\u2013\n21145, 2024. 2, 3\n[36] Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv,\nPeng Wang, Wenping Wang, and Junhui Hou. Modgs: Dy-\nnamic gaussian splatting from causually-captured monocular\nvideos. arXiv preprint arXiv:2406.00434, 2024. 8\n[37] Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep\nPokhariya, Arnab Dey, Ishaan Nikhil Shah, Rugved Mavidi-\npalli, Dylan Hu, Andrew I Comport, Kefan Chen, et al. Diva-\n360: The dynamic visual dataset for immersive neural fields.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 22466\u201322476, 2024.\n2, 8\n[38] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin\nWang, Dahua Lin, and Bo Dai.\nScaffold-gs: Structured\n3d gaussians for view-adaptive rendering.\narXiv preprint\narXiv:2312.00109, 2023. 2\n[39] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 3\n[40] Dawid Malarz, Weronika Smolak, Jacek Tabor, S\u0142awomir\nTadeja, and Przemys\u0142aw Spurek.\nGaussian splatting with\nnerf-based color and opacity. 2\n[41] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[42] David Narciso, Maximino Bessa, Miguel Melo, Ant\u00b4onio\nCoelho, and Jos\u00b4e Vasconcelos-Raposo.\nImmersive 360\u02c6\u00b0\nvideo user experience: impact of different variables in the\nsense of presence and cybersickness. Universal Access in\nthe Information Society, 18:77\u201387, 2019. 1\n[43] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021. 2,\n1, 8\n[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\n2021. 2, 3, 1, 8\n[45] Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc\nPollefeys, and Songyou Peng. Nerf on-the-go: Exploiting\nuncertainty for distractor-free nerfs in the wild. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8931\u20138940, 2024. 2, 1, 8\n[46] Neus Sabater, Guillaume Boisson, Benoit Vandame, Paul\nKerbiriou, Frederic Babon, Matthieu Hog, Remy Gendrot,\nTristan Langlois, Olivier Bureller, Arno Schubert, et al.\nDataset and pipeline for multi-view light-field video. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition Workshops, pages 30\u201340, 2017. 2, 3, 1, 8\n[47] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 5\n[48] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16632\u2013\n16642, 2023. 2, 3, 1"
  },
  {
    "page": 11,
    "text": "[49] Roman Shapovalov, Yanir Kleiman, Ignacio Rocco, David\nNovotny, Andrea Vedaldi, Changan Chen, Filippos Kokki-\nnos, Ben Graham, and Natalia Neverova. Replay: Multi-\nmodal multi-view acted videos for casual holography.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 20338\u201320348, 2023. 2, 3, 1, 8\n[50] Richard Shaw,\nMichal Nazarczuk,\nJifei Song,\nArthur\nMoreau, Sibi Catley-Chandar, Helisa Dhamo, and Eduardo\nPerez-Pellitero. Swings: Sliding windows for dynamic 3d\ngaussian splatting, 2024. 3\n[51] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n2, 3\n[52] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei\nZhao, and Wei Xing.\n3dgstream: On-the-fly training of\n3d gaussians for efficient streaming of photo-realistic free-\nviewpoint videos.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n20675\u201320685, 2024. 3\n[53] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei\nSong, and Huaping Liu. Mixed neural voxels for fast multi-\nview video synthesis. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 19706\u2013\n19716, 2023. 2, 3\n[54] Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, and\nHuaping Liu. Masked space-time hash encoding for efficient\ndynamic scene reconstruction. Advances in Neural Informa-\ntion Processing Systems, 36, 2024. 2, 1, 8\n[55] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 20310\u201320320, 2024.\n2, 3, 4, 6, 7\n[56] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-\nrester Cole, and Cengiz Oztireli. D\u02c6 2nerf: Self-supervised\ndecoupling of dynamic and static objects from a monocular\nvideo. Advances in neural information processing systems,\n35:32653\u201332666, 2022. 2, 1, 8\n[57] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang,\nHaiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou,\nand Sida Peng. Street gaussians for modeling dynamic ur-\nban scenes. arXiv preprint arXiv:2401.01339, 2024. 2\n[58] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting. arXiv preprint\narXiv:2310.10642, 2023. 3\n[59] Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xi-\naoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, and\nXiaogang Jin. Spec-gaussian: Anisotropic view-dependent\nappearance for 3d gaussian splatting.\narXiv preprint\narXiv:2402.15870, 2024. 2\n[60] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park,\nand Jan Kautz. Novel view synthesis of dynamic scenes with\nglobally coherent depths from a monocular camera. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5336\u20135345, 2020. 2, 3, 1, 8\n[61] Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li,\nMinghan Qin, and Haoqian Wang. Gaussian in the wild: 3d\ngaussian splatting for unconstrained image collections. arXiv\npreprint arXiv:2403.15704, 2024. 2"
  },
  {
    "page": 12,
    "text": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement\nSupplementary Material\nA. Overview\nWithin the supplementary material, we provide:\n\u2022 A more detailed introduction and analysis of existing\ndatasets for Dynamic Novel View Synthesis (NVS) tasks\nin Appendix B;\n\u2022 More benchmark results and discussion in Appendix C;\n\u2022 Additional experiments details and STG++ implement\ndetails in Appendix D.\n\u2022 Some clarifications and more descriptions of technical de-\ntails regarding capture rig in Appendix E.\n\u2022 Real-time immersive volumetric video demos and other\ndynamic scene reconstruction results are in our video.\nYou will be able to see a video demo on our homepage\n(currently under construction, stay tuned).\nB. Comprehensive Summary of Datasets for\nDynamic Novel View Synthesis Tasks\nThe earliest studies on dynamic reconstruction have natu-\nrally focused on human digital avatars. Datasets such as Hu-\nman3.6M [18], Panoptic Sports [19], ZJ-Mocap [44], and\nTensor-4D [48] primarily focus on depicting simple human\nactions but do not include backgrounds, which is crucial\nto the immersive application experiences. We will intro-\nduce more complex datasets that include environments from\nmonocular based and multi-view based.\nMonocular acquisition systems are popular due to their\nlow cost and ease of construction. Datasets such as HyperN-\neRF [43], Dynamic Scene Dataset [60], and D2NeRF [56]\nuse a mobile phone as devices, capturing dynamic scenes\nby waving the phone. However, these datasets suffer from\nresolutions below 1080p, limited capture space (similar\nto fixed-point shooting), and durations under one minute.\nAlthough NeRF On-the-go [45] allows for larger capture\nranges by walking while shooting, high-quality reconstruc-\ntions are confined to the vicinity of the capture path, and the\nsmall field of view (FOV) limits prolonged observations of\nspecific scene positions.\nMulti-camera data collection has gained significant at-\ntention due to its ability to provide a larger FOV and richer\ndetails. For instance, the Immersive Light Field dataset [6]\nemploys 46 cameras to capture 15 indoor and outdoor\nscenes, while Technicolor [46] uses a 4\u00d74 camera rig for 12\nindoor sequences. The UCSD Dynamic Scene Dataset [34]\nconsists of 96 outdoor videos focused on single-person ac-\ntivities captured by 10 cameras. The Plenoptic Dataset [28]\nuses 21 cameras for 6 indoor scenes. Similarly, datasets\nlike [27, 33, 54] utilize 13, 18, and 24 cameras, respec-\ntively, to capture dynamic scenes. However, all these setups\nremain static during capture, limiting them to frontal views\nand hindering 360\u00b0 reconstruction. Additionally, the video\nsequences are typically short, with a maximum duration of\n2 minutes (often less than 30 seconds) and a maximum res-\nolution of 3840\u00d72160, which is insufficient for immersive\nVR experiences.\nMoreover, the previously mentioned datasets, whether\nmonocular or multi-view, lack sound recordings, despite the\nimportance of multimodality for immersion. The Replay\ndataset [49] addresses this by focusing on long sequences\nwith professional actors in familiar settings. It employs a\nring of 8 static DSLR cameras paired with binaural micro-\nphones and 3 head-mounted GoPro cameras, providing 46\nvideos at 4K. However, aside from the head-mounted cam-\neras, which can rotate slightly with head movements, all\nother cameras remain static. Furthermore, the DSLR ar-\nrangement does not align with human viewing habits in VR,\nmaking them unsuitable as benchmarks for novel view syn-\nthesis tasks. The latest work [9] presents a dataset of 28\nscenes captured with a 360\u00b0 camera, each including mul-\ntiple audio and video sequences. However, this dataset is\nconstrained by a fixed-point shooting strategy, resulting in\nsparse viewpoints that hinder the reconstruction of high-\nquality dynamic scenes. Further comparisons between our\nwork and these datasets can be found in Tab. 8.\nC. More Benchmark Results and Analysis\nFor fair evaluation, all of our experiments use the default\nparameters recommended by these works.\nC.1. Quantitative & Qualitative Results\nQuantitative Results.\nTable 6 is an extension of Table 5.\nTable 7 displays the train view performance of baselines and\nSTG++ on each 60-frame segment across different scenes.\nQualitative Results.\nFigure 8 shows the results of the test\nview, while Figure 9 presents more results of the train view.\nHere, we also include a comparison with the latest work\nEx4DGS *, whose code was released shortly before our sub-\nmission, limiting our ability to investigate thoroughly. Fig-\nure 10 presents the Ex4DGS\u2019s results from the same view-\npoints as Figure 5 showed in the main text. It performs\nslightly worse in the challenging motion area due to incom-\nplete dynamic and static partitioning.\n*Lee J et al. Fully Explicit Dynamic Gaussian Splatting. Advances in\nNeural Information Processing Systems, 2024, 37: 5384-5409."
  },
  {
    "page": 13,
    "text": "GT\n4DGS\n4Drotor\nSTG\nSTG++\nPSNR: 23.198     SSIM: 0.754    LPIPS:0.409\nPSNR: 27.118     SSIM: 0.773    LPIPS:0.352\n  PSNR:28.319     SSIM: 0.779    LPIPS:0.305\n    PSNR: 31.030     SSIM: 0.792    LPIPS:0.295\nPSNR: 26.004     SSIM: 0.891    LPIPS:0.172\nPSNR: 24.239     SSIM: 0.892    LPIPS:0.109\nPSNR: 27.004     SSIM: 0.910    LPIPS:0.115\n  PSNR: 27.990     SSIM: 0.916    LPIPS:0.111\nPSNR: 26.680     SSIM: 0.844    LPIPS:0.354\nPSNR: 22.802     SSIM: 0.841    LPIPS:0.332\nPSNR: 24.055     SSIM: 0.847    LPIPS:0.319\n   PSNR:25.725     SSIM: 0.829    LPIPS:0.316\nPSNR: 18.241     SSIM: 0.222    LPIPS:0.708\nPSNR: 18.013     SSIM: 0.302    LPIPS:0.274\nPSNR: 20.447     SSIM: 0.568    LPIPS:0.218\n   PSNR: 20.314     SSIM: 0.569    LPIPS:0.213\nFigure 8. Test view results of three baselines and STG++ on Scene1, Scene2, Scene5, Scene6.\nC.2. Analysis\n4DGS proposes a two-stage training approach. In the first\nstage, the algorithm initializes a static scene using the 0th\nframe and limits the number of final points. It maintains\nthe number and color attributes of the Gaussians while only\npredicting changes in their positions, rotations, and scal-\ning.\nThis results in minimal model storage, leading to\nbetter performance in the static parts of the scene com-\npared to other baselines, with reduced flickering.\nHow-\never, its performance declines significantly in scenes requir-\ning more points for detailed representation, and it cannot\naddress the floaters caused by inconsistent colors in adja-\ncent views. The fitting of larger and faster motions and\nsuddenly-appear/disappear objects is particularly poor.\n4Drotor uses dense point clouds as input, which in-\ncreases memory requirements, especially in large scenes,\nleading to longer training times and a higher risk of mem-\nory overflow. However, by introducing rotors to extend 3D\nGaussians to 4D, the authors can directly adapt the density\ncontrol strategy of the original 3DGS to the t-dimensional\nspace. Consequently, it may perform better in areas with\nsignificant motion, such as Figure 9 Scene2 Laboratory\naround human hands.\nD. STG++(Color Mapping) Details\nAlthough STG is not the smallest in terms of storage\namong all baselines and cannot directly train a model with\n300 frames (requiring splitting into multiple 60-frame seg-\nments), it achieves better results under the train-views com-\npared to other baselines. Therefore, we delve deeper into\nits study, hoping it can serve as the foundational architec-\nture for our initial implementation of immersive volumetric\nvideo. However, when viewing in SIBR Viewer, we notice\ntwo significant drawbacks:\n1) In each 60-frame segment, when the viewpoint\nchanges, there is a noticeable flickering of scene points and\nthe presence of floaters, especially when the ground truth\nof the train-views shows significant color differences due to\nlighting occlusions and other objective reasons.\n2)\nBesides\nthe\ncolor\ninconsistency\nduring\nview-\npoint changes within each segment, when we modify\nSIBR Viewer to continuously load multiple segments, the\ntransitions between segments become even more abrupt.\nThis is a drawback of segmented training, as the appear-\nance of the Gaussians cannot remain consistent between\nsegments.\nThus, we propose a learnable viewpoint-dependent affine\ncolor transformation function \u03d5i(W, T) and maintain its\nvalues across different segments. Here, i is the index of\nthe camera, W is a 3\u00d73 transformation matrix, and T is a"
  },
  {
    "page": 14,
    "text": "4DGS\n4Drotor\nSTG\nGround Truth\nCam00_10\nPSNR: 35.797     SSIM: 0.940    LPIPS:0.145\nPSNR: 33.061     SSIM: 0.927    LPIPS:0.152\nPSNR: 33.324     SSIM: 0.927    LPIPS:0.156\n(a) The results of train views for three baselines on Scene1 Opera girl.\n4DGS\n4Drotor\nSTG\nGround Truth\nCam00_30\nPSNR: 31.814     SSIM: 0.948    LPIPS:0.078\nPSNR: 33.145     SSIM: 0.960    LPIPS:0.049\nPSNR: 33.267     SSIM: 0.961    LPIPS:0.045\n(b) The results of train views for three baselines on Scene2 Laboratory.\nFigure 9. More benchmark results visualization (Part 1).\n(1,3) offset vector. Just like the affine transformation, the\ncolors in rendered images C\u2032\ni are related to the colors in real\nscenes (SIBR Viewer) Ci as follows:\n  \nC ' _ { i}  =\\mathbf {W} \\cdot C_{i}+ \\mathbf {T} \n(13)\nThe loss is calculated between the rendered images C\u2032\ni and\nthe ground truth as:\n  Lo s s= ( 1-\\lambda  _\n{1 }  )L_{1} (gt ,C\n'_{i} ) +\\lambda _{1}D_{SSIM}(gt,C'_{i} ) \n(14)\nYou can get a more intuitive sense of the improvement"
  },
  {
    "page": 15,
    "text": "4DGS\n4Drotor\nSTG\nGround Truth\nCam05_10\nPSNR: 28.473     SSIM: 0.899    LPIPS:0.159\nPSNR: 33.195    SSIM: 0.935    LPIPS:0.105\nPSNR: 30.731     SSIM: 0.924    LPIPS:0.094\n(c) The results of train views for three baselines on Scene5 Rendition.\n4DGS\n4Drotor\nSTG\nGround Truth\nCam0_30\nPSNR: 21.833     SSIM: 0.613    LPIPS:0.319\nPSNR: 23.625    SSIM: 0.720    LPIPS:0.239\nPSNR: 21.482     SSIM: 0.535    LPIPS:0.453\n(d) The results of train views for three baselines on Scene6 Puppy.\nFigure 9. More benchmark results visualization (Part 2).\nfrom the video in supplementary materials.\nE. Clarifications and Technical Details.\nE.1. About \"Volumetric\" Term\nIn fact, both academia and industry have yet to provide a\nclear definition of volumetric video capture methods. Our\noriginal intention was to define videos that use 3D recon-\nstruction technologies and provide a 6-DOF experience as\nvolumetric video, which is the future of media. Most ex-\nisting volumetric videos are constructed from an outside-\nlooking-in manner, often lacking natural backgrounds and\nlighting, which reduces immersion. Inspired by Google\u2019s\nwork [6], we aim to develop videos offering a multi-modal,\ninside-looking-out 6-DoF experience, and thus call it \u201cim-\nmersive volumetric video\u201d.\nE.2. STG++ Limitations\nWe introduce viewpoint-based color transformation to ad-\ndress global color inconsistencies caused by varying light-\ning conditions between cameras in real-world scenes. How-\never, local flickering remains a complex issue due to varia-\ntions in materials and environmental lighting changes. It is\nstill a significant challenge for the current community, re-"
  },
  {
    "page": 16,
    "text": "PSNR: 34.662  SSIM:0.920  LPIPS:0.164\nPSNR: 33.166  SSIM:0.955  LPIPS:0.046\nPSNR: 23.716  SSIM:0.704  LPIPS:0.230\n\u00b7Background points: Better remain static \nthan existing methods.\n\u00b7Training time\uff1a6+ hours every 60 frames, \nwhich is much longer than STG.\n\u00b7No color mapping\uff1aStill facing color \ninconsistentcy when transitioning between \nadjacent perspectives.\nFigure 10. Ex4DGS\u2019s performance on the same views as Figure 5\nshowed in the main text.\nFigure 11. Ablation Studies Illustrate the Effectiveness of Our\nProposed Sound Field Reconstruction (SFR) Baseline. A total of\n58 participants participated in the user study, rating their sense of\ndirection and distance based on results from various algorithms,\nusing a scale from 1 to 5 (1 being low and 5 being high).\nquiring more adaptive and fine-grained processing. We will\nconsider this in future work.\nE.3. More Validation of Sound Field Reconstruction\nIt is worth noting that, AV-NeRF [31] is the closest exist-\ning work to our goal of sound field reconstruction (SFR),\nwhich means it also targets sound synthesis in a novel loca-\ntion. However, it focuses solely on sound synthesis and uses\nprofessional binaural audio acquisition equipment (which is\nentirely different from our capture system) to collect sound\nfrom various locations in space. As a result, its SFR method\ncannot directly leverage data collected by our camera rig.\nWe will try to modify and adapt its approach for compari-\nson with our baseline in the future.\nBut to further assess the effectiveness of each module in\nour proposed SFR method, we have conducted an ablation\nstudy based on user feedback, as shown in Figure 11. The\naverage scores for both metrics indicate that incorporating\ndirection and distance modeling in sound field reconstruc-\ntion significantly enhances participants\u2019 immersive video\nexperiences, further demonstrating our SFR algorithm as a\npractical baseline.\nFigure 12. Sound Localization w & w/o Noise Reduction. The\nyellow areas in the image represent the highest sound intensity\nin each frame, clearly indicating the sound source\u2019s angle change\nrelative to a specific camera.\nE.4. Capture Rig Setups and Calibration\nTime-Synchronized.\nWe used GoPro\u2019s official QR con-\ntrol app on mobile phone, enabling each camera to scan a\ndynamically updating QR code for time synchronization.\nNoise Reduction and Cart Speed.\nOur cart only gener-\nates noise from axle rotation during sharp turns. The de-\nscription in main text L.250-253 mitigates this noise and\ngreatly reduces the impact of environmental sounds (e.g.,\nwind) on sound quality. As shown in Figure 12, the de-\nnoised sound achieves clearer localization (left), while the\nnoisy sound results in more divergent localization (right).\nThis indicates that the sound quality we collected can not\nonly construct multi-modal volumetric videos but also con-\ntribute to sound field, inspiring future work on sound lo-\ncalization and reconstruction from multiple sound sources.\nAdditionally, while the cart experiences slight shaking on\nuneven terrain, it moves as a rigid body, so its speed is not\nlimited. This prior knowledge can even accelerate our cal-\nibration process. The slow speed in this work is primarily\ndue to safety considerations, and we plan to collect faster-\nmoving data in the future.\nCalibration.\nAlthough we have completed the calibration\nof the data intended for release, including both fixed-point\nand mobile shooting, before the submission deadline, it is\nimportant to note that, for moving shots, we have tested\nvarious open-source algorithms, but none offer an efficient\nsolution for moving multi-view data.\nUsing the original\nCOLMAP takes days to calibrate poses for each frame in\nlong videos. A feasible approach to speed up may refer to\n\u2020. We also look forward to working with colleagues in this\ncommunity to explore more efficient and accurate calibra-\ntion solutions using this dataset.\n\u2020Bernhard Kerbl et al. A Hierarchical 3D Gaussian Representation for\nReal-Time Rendering of Very Large Datasets. TOG, 2024.1"
  },
  {
    "page": 17,
    "text": "E.5. Continuously Updated Dataset\nCurrently, other segments in Scene1 include high-speed\nmotions, as shown in Figure 13. And we will continue to\nupdate the dataset, increase its richness to make more con-\ntributions to the development of the community."
  },
  {
    "page": 18,
    "text": "Table 6. Performance of three baseline methods and STG++ on the ImViD Dataset. All methods selected cam10 as the test view.\nScene1 Opera girl\nScene2 Laboratory\nScene5 Violin\nScene6 Puppy\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\n4DGS\n23.227\n0.753\n0.410\n25.798\n0.889\n0.176\n26.586\n0.842\n0.356\n18.121\n0.222\n0.711\n4DRotor\n27.263\n0.775\n0.328\n28.007\n0.918\n0.098\n24.083\n0.850\n0.296\n17.916\n0.298\n0.331\nSTG\n28.482\n0.786\n0.287\n26.306\n0.910\n0.114\n23.144\n0.846\n0.317\n20.497\n0.594\n0.211\nSTG++\n31.240\n0.799\n0.277\n27.581\n0.916\n0.107\n25.747\n0.834\n0.310\n20.533\n0.598\n0.202\nTable 7. Comparison of average metrics for three baselines and STG++ across four scenes. Due to its smaller model size, 4DGS [55] can\ntrain 300 frames at once, so there are no segmented results.\nFrames1-60\nFrames60-120\nFrames120-180\nFrames180-240\nFrames240-300\nAvarage\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\n4DGS\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n35.005\n0.930\n0.156\n4DRotor\n33.502\n0.912\n0.142\n33.735\n0.913\n0.137\n33.749\n0.913\n0.137\n31.460\n0.893\n0.155\n33.718\n0.913\n0.139\n33.233\n0.909\n0.142\nSTG\n34.915\n0.920\n0.127\n35.343\n0.922\n0.125\n35.478\n0.924\n0.124\n35.496\n0.922\n0.125\n34.913\n0.921\n0.126\n35.229\n0.922\n0.125\nScene1 Opera\nSTG++\n35.195\n0.922\n0.125\n35.603\n0.923\n0.123\n35.822\n0.924\n0.122\n35.738\n0.924\n0.123\n35.738\n0.925\n0.123\n35.619\n0.924\n0.123\n4DGS\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n32.701\n0.949\n0.078\n4DRotor\n36.207\n0.967\n0.049\n36.519\n0.967\n0.046\n34.593\n0.96\n0.067\n36.484\n0.968\n0.046\n36.679\n0.967\n0.049\n36.096\n0.966\n0.051\nSTG\n33.405\n0.949\n0.077\n33.257\n0.949\n0.078\n33.641\n0.951\n0.077\n33.140\n0.948\n0.081\n33.298\n0.950\n0.078\n33.348\n0.949\n0.078\nScene2 Laboratory\nSTG++\n33.450\n0.950\n0.079\n33.666\n0.951\n0.076\n33.616\n0.951\n0.077\n33.452\n0.950\n0.079\n33.748\n0.952\n0.073\n33.586\n0.951\n0.076\n4DGS\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n33.645\n0.918\n0.183\n4DRotor\n33.398\n0.935\n0.135\n33.361\n0.935\n0.133\n33.255\n0.934\n0.136\n33.398\n0.935\n0.132\n32.638\n0.932\n0.136\n33.210\n0.934\n0.134\nSTG\n34.508\n0.930\n0.158\n34.029\n0.929\n0.161\n33.900\n0.928\n0.165\n34.178\n0.929\n0.163\n34.222\n0.929\n0.161\n34.167\n0.929\n0.162\nScene5 Rendition\nSTG++\n34.426\n0.928\n0.160\n34.277\n0.929\n0.163\n34.169\n0.928\n0.165\n34.407\n0.929\n0.161\n34.203\n0.927\n0.159\n34.296\n0.928\n0.162\n4DGS\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n21.117\n0.450\n0.561\n4DRotor\n21.902\n0.643\n0.301\n21.988\n0.646\n0.297\n21.719\n0.629\n0.319\n21.884\n0.644\n0.297\n21.844\n0.645\n0.300\n21.867\n0.641\n0.303\nSTG\n23.307\n0.714\n0.247\n23.381\n0.717\n0.243\n23.387\n0.718\n0.241\n23.331\n0.717\n0.245\n23.413\n0.718\n0.241\n23.364\n0.716\n0.243\nScene6 Puppy\nSTG++\n23.316\n0.714\n0.246\n23.423\n0.719\n0.240\n23.392\n0.719\n0.241\n23.314\n0.716\n0.246\n23.438\n0.719\n0.240\n23.377\n0.717\n0.242\nFigure 13. High-Speed Motions Data in Our Dataset ImViD. Scene 1: opera boy spinning kick."
  },
  {
    "page": 19,
    "text": "Table 8. Existing real-world datasets for dynamic novel view synthesis.\nDatasets\nNo.Scene\nOutdoor/Indoor\nCameras\nMobility\nResolution\nAngles\nDuration\nFPS\nMultimodality\nContent\nPanopticSports [19]\n65\nIndoor\n480 cameras\nStatic\n640\u00d7480\n360\u00b0\n5mins\n25\n%\nHuman-centric actions\nTechnicolor [46]\n12\nIndoor\n16 cameras\nStatic\n2048\u00d71088\nFrontal\n2s\n30\n%\nHas a number of close-ups sequences, captured\nmedium angle scenes and other animated scenes\nwhere the movement does not come from a human.\nImmersive-Lightfield [6]\n15\nboth\n46 cameras\nStatic\n2560\u00d71920\nFrontal\n10-30s\n30\n%\nSimple and slow motion of human,animals,objects\nHyperNeRF [43]\n17\nIndoor\n1 hand-held phone\nFixed-point Waving\n1920\u00d71080\nFrontal\n30-60s\n30\n%\nWaving a mobile phone in front of a moving scene,\nobject-centric\nDynamic Scene Datasets\n(NVIDIA) [60]\n8\nOutdoor\n1 Mobile phone\n/12 cameras\nFixed-point Waving\n/Static\n1920\u00d71080\nFrontal\n5s\n60\n%\nSimple body motions (facial, jump, etc)\nUCSD Dynamic [34]\n96\nOutdoor\n10 cameras\nStatic\n1920\u00d71080\nFrontal\n1-2mins\n120\n%\nVarious visual effects and human interactions\nZJU-Mocap [44]\n10\nIndoor\n21 cameras\nStatic\n1024\u00d71024\n360\u00b0\n20s\n50\n%\nSimple body motions (punch, kick, etc.)\nPlenoptic Dataset (DyN-\neRF/Neural 3D) [28]\n6\nIndoor\n21 cameras\nStatic\n2704\u00d72028\nFrontal\n10-30s\n30\n%\nContains high specularity, translucency and trans-\nparency objects, motions with changing topology,\nselfcast moving shadows, volumetric effects, vari-\nous lighting conditions and multiple people moving\naround in open living room space\nD2NeRF [56]\n10\nIndoor\ndual-hold phone\nFixed-point Waving\n1920\u00d71080\nFrontal\n5s\n30\n%\nContains more challenging scenarios with rapid mo-\ntion and non-trivial dynamic shadows\niPhone Datasets [15]\n14\nboth\n1 hand-held phone\n/2 cameras\nFixed-point Waving\n/Static\n640\u00d7480\nFrontal\n8-15s\n30/60\n%\nFeaturing non-repetitive motion, from various cate-\ngories such as generic objects, humans, and pets\nMeetroom Datasets [27]\n4\nIndoor\n13 cameras\nStatic\n1280\u00d7720\nFrontal\n10s\n30\n%\nOne or three persons have discussion, working, trim-\nming in a meeting room\nENeRF-Outdoor [33]\n4\nOutdoor\n18 cameras\nStatic\n1920\u00d71080\nFrontal\n20-40s\n30\n%\nComplex human motions\nReplay [49]\n46\nIndoor\n12 cameras\nStatic\n3840\u00d72160\n360\u00b0\n5mins\n30\n\u2713(Audio)\nDancing, chatting, playing video games, unwrapping\npresents, playing ping pong\nCampus Datasets [54]\n6\nOutdoor\n24 cameras\nStatic\n3840\u00d72160\nFrontal\n5-10s\n30\n%\nIncludes more realistic observations such as pedestri-\nans, moving cars, and grasses with people playing\nMoDGS [36]\n6\nboth\n1 cameras\nStatic\n\u2013\nFrontal\n\u2013\n\u2013\n%\nContains diverse subjects like skating, a dog eating\nfood, YOGA, etc.\nDiVa-360 [37]\n53\nIndoor\n53 cameras\nStatic\n1280\u00d7720\nFrontal\n51s\n120\n\u2713(Audio)\nFor Object-centric tasks. Contains dynamic objects\nand intricate hand-object interactions.\nNeRF On-the-go [45]\n12\nboth\n1 hand-held phone\nMoveable\n4032\u00d73024\n360\u00b0\n5-10s\n30\n%\nIncluding 10 outdoor and 2 indoor scenes, features a\nwide range of dynamic objects including pedestrians,\ncyclists, strollers, toys, cars, robots, and trams), along\nwith diverse occlusion ratios ranging from 5% to 30%\n360+X [9]\n28\nboth\n1 360\u00b0cameras and\n1 Spectacles cameras\nStatic\n5760\u00d72880\n360\u00b0\n10s (2152 sequence)\n30\n\u2713(Audio)\nCapture in 17 cities across 5 countries.Panoptic per-\nspective to scene understanding with audio\nImViD(Ours)\n7\nboth\n46 cameras\nMoveable\n5312\u00d72988\nFrontal and 360\u00b0\n1-5mins\n60\n\u2713(Audio)\nSeven common indoor and outdoor scenes in daily\nlife, including opera, face-to-face communication,\nteaching, discussion, music performance, interaction\nwith pets, and playing. Each scene has high-quality\nsynchronized multi-view video and audio with a dura-\ntion of more than 1 minute, and contains rich elements\nsuch as various small objects, glass, and changes in\nlight and shadow"
  }
]