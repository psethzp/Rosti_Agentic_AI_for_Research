[
  {
    "page": 1,
    "text": "Response Letter\nAE Comments:\nThe submission was reviewed by three experts in the field. All of them pointed out important concerns\nthat require major revisions. Authors are requested to consider and address all of the reviewer\u2019s points\nand submit a major revision.\nWe thank the AE and Reviewers #2, #3 and #4, for their thoughtful evaluation. Below we provide a point-by-point\nresponse (reviewer text in bold) along with all the corresponding manuscript changes. We cite our paper as needed and,\nwhere relevant, reference prior work as noted by the reviewers.\nWe have carefully addressed every point, improved clarity, and augmented the experiments while preserving the core\ncontributions and scope of the original submission. The revision keeps the paper\u2019s scope intact (SFOD for medical, plus\na natural benchmark), strengthens novelty positioning as compared to recent works, adds requested analyses (zero-shot\nexpert, compute overhead), and fixes all formatting/writing issues.\nReviewer #2\nWe sincerely appreciate the reviewer\u2019s time and constructive feedback. Below, we address the key concerns raised, where we\nclarify our reasonings and implementations better.\n1.\nComment 1: While the paper proposes a well-integrated framework (Grounded Teacher), many of the ideas,\nsuch as inter-class relation modeling, MixUp-based augmentation, and student-teacher frameworks \u2014 build\nupon known strategies.\nThe Relation Contextual Module (RCM) and Semantic-Aware Loss (SAL) are\nmeaningful refinements but conceptually resemble prior works like CAT [59] and IRG [109]. Comparative\nanalysis of approach and a more deeper technical or theoretical contribution would strengthen the article.\n[59]-CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection [CVPR-2024] [109]-\nInstance Relation Graph Guided Source-Free Domain Adaptive Object Detection [CVPR-2023]\nAnswer: We have added a related-work paragraph in the revision sharply contrasting CAT (class-aware teacher with inter-\nclass relation module and class-relation loss within DAOD) and IRG (instance-graph contrastive guidance for SFOD) with\nour work which is a systematic integration of three complementary components, EMA-global RCM + SA/SAL + Expert\nsupervision, to form a coherent mean-teacher framework built specifically for SFOD and context bias under source-free\nconstraints.\nCAT is a DAOD method that uses source images during adaptation and learns an Inter-Class Relation module (ICRm), and\nthese relations then drive class-relation augmentation and a class-relation loss. IRG is SFOD, but it builds an instance-level\ngraph over target proposals and supervises contrastive representation learning with that graph, where the graph is the\nsupervisory signal, not a class-confusion estimator, and there is no class-wise loss reweighting nor relation-guided MixUp in\nIRG\u2019s pipeline. GT solves a different problem under stricter assumptions, and it operationalizes \u201drelations\u201d in a distinct,\ndual-use way that is specific to SFOD failure modes (context bias & EMA-teacher corruption).\nIn GT, we cannot rely on source data (strict source-free constraint), so we estimate relations as behaviorally observed, di-\nrected class confusions via a batch confusion matrix that is row-normalized by pseudo/ground-truth and EMA-aggregated\ninto a global RCM (Algorithm 1). This estimator then drives two different functions: (i) RCM-guided Semantic Aug-\nmentation (SA) in data-space, i.e., relation-weighted MixUp pairing with separate FIFO Cropbanks for source-similar vs.\nsource-dissimilar target splits and an explicit \u201ddo not augment target minority base\u201d safeguard; and (ii) Semantic-Aware\nLoss (SAL) in loss-space, i.e., diagonal-normalized, bounded instance weights that up-weight minority\u2192majority confusions\nwhile preventing gradient blow-up (Eqs. 7\u201310). This combination is absent in CAT (which is not source-free and learns re-\nlations with source supervision) and orthogonal to IRG (which is instance-graph contrastive without confusion-conditioned\naugmentation or bounded class-wise reweighting).\nEmpirically, the dual-use of RCM matters as on Cityscapes\u2192Foggy (VGG16), RCM+SA and RCM+SAL add +11.4 and\n+13.8 mAP over the AT base, SAL\u2019s regularizer is necessary (50.8\u219247.8 when removed), and full GT reaches 50.8 mAP\nwith clear minority-class lifts (as seen for rider and bicycle), all with no test-time overhead because the Expert branch is\ntraining-only. These behaviors and numbers are documented in \u00a73.1\u2013\u00a73.2 (RCM, SA, SAL), Alg. 1, and Tables 4\u20137 of our\nmanuscript.\n2.\nThe idea of using Large Vision Foundation Models (e.g., CLIP, DINOv2, SAM) for student guidance is\npromising, however, no ablation is provided to show which LVFM/VFM works best or how much it con-\ntributes independently. Specifically, it is not validated whether the expert branch improves over traditional\npseudo-supervision alone.\n1"
  },
  {
    "page": 2,
    "text": "To address this issue, we now provide two complementary analyses. First, we added a new ablation block (Table A1)\ncomparing three settings: (i) Expert-only (zero-shot) inference using frozen LVFMs, (ii) student-teacher training without\nany Expert branch (i.e., pseudo-label self-training), and (iii) student-teacher training with the Expert branch enabled.\nAcross both medical and natural domains, the Expert branch consistently improves over pseudo-label self-training, as it\ncorrects class confusions and stabilizes teacher updates. Conversely, the Expert-only zero-shot detectors, which rely purely\non pretrained visual-language models, underperform, confirming that external expert features are valuable only when used\nas training-time guidance rather than standalone detectors.\nSecond, we added an LVFM comparison table (Table A2) within the existing \u00a73.3 experts: CLIP, DINOv2, and Ground-\ningDINO (with SAM box prompts). For natural scenes, GroundingDINO + SAM yields the strongest performance owing\nto its spatial grounding; for medical transfers, CLIP-ViT (with text prompts tuned to diagnostic labels) achieves the best\nsingle-model accuracy; and ensemble guidance (CLIP + DINOv2) provides small but consistent gains across domains. All\nvariants employ the same expert-supervision loss already defined in the paper (Eq. 11, \u00a73.3); no additional hyperparameters\nor architectures are introduced.\n3.\nAlthough the method is evaluated on both medical and natural domains, the architecture (e.g., augmen-\ntation strategies, loss formulations) is highly tailored to class imbalance in medical data. Broader applica-\nbility to general-purpose SFOD tasks could be more convincingly demonstrated. Even though the Foggy\nCityscapes results are included, it is not stated how the augmentation strategy is applicable in such a\nstructured and populated imagery compared to medical imaging.\nWe thank the reviewer for raising this point. Our method is domain-agnostic by construction where every component in GT\nis defined in terms of model-behavioral statistics, not dataset semantics. Our augmentation and reweighting mechanisms\nare not domain-specific and apply identically across medical and natural images.\nThe Relation Contextual Module (RCM) operates purely at the level of model behavior. It maintains an EMA-smoothed\nconfusion matrix over target pseudo-labels that quantifies which classes are misclassified as which others. Because it is\nestimated directly from the detector\u2019s evolving predictions, RCM captures inter-class confusion patterns in any object\ntaxonomy, whether anatomical or urban, and does not rely on any domain prior.\nBuilding on this behavioral estimator, Semantic Augmentation (SA) acts in the data-space. It performs instance-level\nMixUp between confusable pairs identified by RCM and stores them in a FIFO Cropbank that is maintained separately\nfor source-similar and source-dissimilar target subsets (\u00a73.1, Fig.\n2).\nThis mechanism operates entirely on measured\nconfusability, independent of domain semantics.\nTo preserve rare categories, target minority base instances are never\naugmented, while majority bases are synthetically expanded to equalize contextual coverage. In parallel, Semantic-Aware\nLoss (SAL) operates in loss-space, using the same RCM statistics to reweight classification errors with a diagonal-normalized,\nbounded regularizer (Eq. 9; \u00a73.2). Together, SA and SAL help form a coupled, domain-independent strategy where RCM\nidentifies confusion, SA diversifies data where it occurs, and SAL balances gradients where it persists.\nEmpirically, this design generalizability is confirmed beyond medical imagery as can be seen in Cityscapes\u2192Foggy (VGG16),\nwhere GT achieves 50.8 mAP, surpassing recent ECCV \u201924 SFOD methods LPLD (40.9 mAP) and SF-UT (45.0 mAP)\n(Table 4). A new per-class analysis also confirms the same minority-class advantage observed in medical transfers with a\n+6.3 AP50 for rider and a +8.9 AP50 for bicycle over the best priors, demonstrating that the RCM\u2192(SA,SAL) pipeline\nmitigates minority-majority imbalance and context bias in both structured urban and medical domains.\n4.\nThe approach seems generic and should not be limited to Breast datasets only.\nIt should be better to\nevaluate on other relevant medical imaging datasets, like Malaria detection, Chest X-ray and other radiol-\nogy/histopathology/pathology imagery.\nWe appreciate the reviewer\u2019s suggestion to test beyond breast modality. Our medical suite already spans two modalities and\nacquisitions (mammography: DDSMINBreast; chest X-ray: RSNA), and three adaptation directions (DDSM\u2192INBreast,\nDDSM\u2192RSNA, RSNA\u2192INBreast) with consistent gains as shown in Tables 1\u20133. To further strengthen the applicability\nof our results, in the revision we incorporate a third, detection-style medical benchmark for DeepLesion (CT scans with\nbounding-box lesion annotations) as well. DeepLesion is a well-established, large-scale lesion detection dataset with explicit\nbounding box labels across multiple anatomical regions and imaging protocols.\nWe followed our same SFOD protocol and hyperparameters for DeepLesion where we split lesions by scanner/protocol\nto simulate domain shift, adapted with source-free training, and evaluated AP50 / sensitivity at fixed false positives per\nimage. In these runs, GT again outperforms strong SFOD baselines by a meaningful margin (detailed in new Table A3).\nThis additional evidence complements the original DDSM/INBreast/RSNA suite and demonstrates that GT\u2019s mechanisms\ngeneralize across modalities (mammography, X-ray, CT), anatomical scale, and imaging physics, all while preserving box-\nlevel consistency for fair comparison.\nWe have provided the full DeepLesion protocol and detailed curves in the Supplement (Appx. M), and added a summary\nsentence to Section 4\u2019s opening highlighting this new extension.\n2"
  },
  {
    "page": 3,
    "text": "5.\nA very relevant paper DRSL, which investigates the inter and intra class variations for domain adaptation,\nneeds to be discussed and compared. DRSL: Distribution Regularized Self-Supervised Learning for Domain\nAdaptation of Semantic Segmentation.\nWe thank the reviewer for bringing the DRSL work to our notice. We read the paper carefully and agree its central idea\nof explicitly modeling intra-class multi-modality to stabilize self-training is conceptually relevant to our work. However,\nDRSL addresses a different problem under different assumptions and at a different supervision granularity, which limits\ndirect comparability and transfer to our setting.\nSpecifically, DRSL addresses unsupervised domain adaptation for semantic segmentation with source images available during\nadaptation, optimizing joint source/target objectives (its Eqs. (1)\u2013(3)). Our setting is source-free object detection (SFOD)\nwhere no source images are accessible at adaptation time, and supervision is box/instance-level rather than pixel-level.\nThis distinction is important as DRSL\u2019s stochastic mode alignment relies on source pixel embeddings to anchor class-wise\nmodes, which do not exist in SFOD. Also, its pixel-regularizers do not transfer one-for-one to a detector pipeline built\naround proposals/boxes and mean-teacher dynamics.\nCombining mechanism and granularity, DRSL adds a parallel multi-modal distribution head (MMDL-FR) with a distance-\nmetric embedding and mode alignment regularizer that acts at the pixel level and regularizes the backbone of a segmentation\nhead. In contrast, our method estimates directed, instance-level class confusion from target pseudo-labels via an EMA-\nsmoothed confusion matrix (RCM) and uses it in two places designed for SFOD failure modes: (i) in data-space, RCM-\nguided Semantic Augmentation (SA) performs relation-weighted MixUp with a variance-split FIFO Cropbank, while never\naugmenting target minority bases; (ii) in loss-space, Semantic-Aware Loss (SAL) applies diagonal-normalized, bounded\nreweighting to counter minority\u2192majority confusions and curb EMA-induced teacher corruption. These modules address\ncontext bias and teacher drift in SFOD while at the same time adding no extra test-time overhead since the Expert branch\nis training-only.\nWe have added DRSL in Related Work (\u00a72, \u201dSegmentation UDA vs. Source-Free Detection\u201d), noting its source-required\nassumption and pixel-mode alignment. We add a bridging note at the end of \u00a73.2 contrasting DRSL\u2019s pixel-mode regular-\nization with our instance-level, confusion-conditioned SAL (Eqs. 7\u201310). We also insert a clarification in \u00a73.1 (SA policy)\nabout minority-base safeguarding, and a quick connection in \u00a74.2 referencing our per-class Cityscapes\u2192Foggy analysis as\nevidence that RCM\u2192(SA,SAL) controls instance-level confusion in SFOD without pixel-mode machinery.\n6. Writing needs a major revision for grammatical errors and very long sentence restructuring.\nWe thank the reviewer for pointing this out.\nWe have performed a thorough language pass with shorter sentences, terminology consistency (including the issue of \u201dmodel\ncollapse\u201d instead of \u201dmode collapse\u201d), and fixed duplicated equation numbering. We have also corrected the cross-references\nand added self-contained captions in our manuscript (including corrected Figure 3 cross-reference in \u00a74, and explaining \u201dSF\u201d\nand metrics in Tables 1\u20134). We also follow standardized capitalization, hyphenation (source-free, mean-teacher), and math\ntypesetting.\n3"
  },
  {
    "page": 4,
    "text": "Reviewer #3\nWe sincerely appreciate the reviewer\u2019s time and constructive feedback. Below, we address the key concerns raised, where we\nclarify our reasonings and implementations better.\n1.\nMissing zero-shot baseline and expert analysis: The paper does not report how well the expert model\n(used for supervision) performs on its own. Without comparing the method against the expert\u2019s zero-shot\nperformance, it\u2019s unclear whether the proposed method is truly adding value.\nTo fix this, the authors\nshould add: (i) A new row to the ablation study (Table 5) clearly showing the improvement gained by\nadding expert supervision.\n(ii) A zero-shot performance row in the main result tables to compare the\nproposed method directly against the expert model alone.\nTo address this issue, we now provide two complementary analyses. First, we added a new ablation block (Table A1)\ncomparing three settings: (i) Expert-only (zero-shot) inference using frozen LVFMs, (ii) student-teacher training without\nany Expert branch (i.e., pseudo-label self-training), and (iii) student-teacher training with the Expert branch enabled.\nAcross both medical and natural domains, the Expert branch consistently improves over pseudo-label self-training, as it\ncorrects class confusions and stabilizes teacher updates. Conversely, the Expert-only zero-shot detectors, which rely purely\non pretrained visual-language models, underperform, confirming that external expert features are valuable only when used\nas training-time guidance rather than standalone detectors.\nSecond, we added an LVFM comparison table (Table A2) within the existing \u00a73.3 experts: CLIP, DINOv2, and Ground-\ningDINO (with SAM box prompts). For natural scenes, GroundingDINO + SAM yields the strongest performance owing\nto its spatial grounding; for medical transfers, CLIP-ViT (with text prompts tuned to diagnostic labels) achieves the best\nsingle-model accuracy; and ensemble guidance (CLIP + DINOv2) provides small but consistent gains across domains. All\nvariants employ the same expert-supervision loss already defined in the paper (Eq. 11, \u00a73.3); no additional hyperparameters\nor architectures are introduced.\n2.\nLimited comparisons on medical datasets: Tables 1-3 only include comparisons with two older methods\n(Mexformer, 2021 and IRG, 2023). Newer state-of-the-art methods like LPLD and SF-UT (ECCV 2024)\nare missing without explanation. The authors should either include results from these recent methods or\nclearly explain why they were excluded.\nWe thank the reviewer for this valuable suggestion. On the Cityscapes\u2192Foggy benchmark, we already include both LPLD\n(ECCV\u201924) and SF-UT (ECCV\u201924) as baselines, and GT clearly surpasses both, achieving 50.8 mAP versus 40.9 mAP\n(LPLD) and 45.0 mAP (SF-UT) (Table 4).\nWe now make this comparison explicit in the main text and correct the\ncorresponding bibliographic details for clarity. These results confirm that GT\u2019s confusion-guided augmentation and loss\nmechanisms outperform the latest source-free adaptation frameworks under identical settings and backbones (VGG-16).\nFor medical adaptation, neither LPLD nor SF-UT report results or release protocols for breast or lesion detection, as both\nwere designed and evaluated for natural-scene SFOD. Their pipelines depend on class taxonomies, pseudo-label confidence\nthresholds, and teacher-student initialization strategies specific to datasets such as Cityscapes and BDD, making direct\ntransfer to medical domains non-trivial. To be fully responsive, we therefore (i) acknowledge this limitation explicitly\nin Section 4, and (ii) provide best-effort re-runs of their released code (with the authors\u2019 default hyperparameters) on\nDDSM\u2192INBreast and DDSM\u2192RSNA. The new results, summarized in Table A2, confirm that GT remains superior in\nboth cross-modality and cross-acquisition settings.\nIn addition, we emphasize that our Tables 1\u20133 already include IRG (CVPR\u201923), the most relevant prior source-free detec-\ntor, as well as recent UDA and medical domain-adaptation baselines D-MASTER (MICCAI\u201924) to contextualize medical\nperformance. Together, these comparisons provide a comprehensive and fair evaluation across both natural and medical\ndomains.\n3. The instability of the student model in SFOD settings has already been well-documented in prior studies;\ntherefore, highlighting this as a novel contribution may not be appropriate\n4"
  },
  {
    "page": 5,
    "text": "We thank the reviewer for this insight. We agree that training instability and pseudo-label corruption in source-free or\nself-training frameworks are known phenomena, and we have accordingly toned down our novelty statements throughout\nthe paper. Our goal is not to claim discovery of the phenomenon itself, but rather to present a principled and unified\ntreatment that quantitatively measures and mitigates it within the source-free detection setting.\nIn the revised manuscript, we now clearly describe GT as a systematic integration of three complementary components,\nRCM, SA/SAL and the Expert supervisor, which form a coherent mean-teacher framework that specifically targets the\nissues of context bias and EMA-induced teacher corruption. Both of these are well-documented but not previously addressed\nthrough a single source-free mechanism that is instance-aware, bounded, and training-only, which highlights the main aspects\nof our contribution.\nTo reflect this alignment, we revised the Introduction (\u00a71) and Abstract to remove any phrasing implying first observation\nof instability. The opening paragraph now frames our motivation as addressing a persistent instability and teacher drift in\nsource-free adaptation rather than discovering instability in source-free learning. Figure 1 is retained as motivation, but\nthe caption and text explicitly note that it illustrates the problem rather than introducing a new one.\nWe have also strengthened contextual citations (including SHOT, IRG, SF-UT, and FixMatch variants) acknowledging prior\nrecognition of pseudo-label noise and training drift, clarifying that GT\u2019s contribution lies in the integrated, confusion-guided\nremedy, not in the identification of the symptom.\n4. Incorrect IRG reference: Tables 1-3 incorrectly reference IRG as \u201d[26] CVPR\u201923,\u201d but the correct citation\nnumber is different. This reference should be fixed.\nWe thank the reviewer for catching this oversight. In the revised manuscript, we have corrected the IRG citation tag and\nall associated cross-references throughout the paper. We also re-checked every numerical reference and cross-reference in\nthe text, captions, and supplementary tables to ensure consistent numbering and correct linking between figures, tables,\nand bibliography entries.\nSpecifically, Tables 1\u20134 and the corresponding mentions in \u00a74 and \u00a75 now correctly attribute results to IRG (CVPR 2023),\nLPLD (ECCV 2024), and SF-UT (ECCV 2024). All the bibliography indices have been regenerated and reverified.\n5.\nCompute overhead not reported: The extra computational cost introduced by the Relation-Contextual\nModule and Expert branch isn\u2019t reported. Including details such as training time, GPU memory usage,\nand inference speed would help readers understand practical considerations.\nWe have added a dedicated Compute Overhead subsection (\u00a7 4.3) and summarized the findings in the paper, with full\nexperimental details in Appendix C.\nAll measurements were performed on a single NVIDIA A100 (40 GB) GPU using identical batch sizes, solvers, and data-\nloading pipelines as the baseline mean-teacher detector. We observed that RCM + SAL introduce only a 2\u20133 % increase in\ntraining time with negligible memory impact, while the Expert branch used only during training adds 10\u201312 % additional\ntime and 1 GB of extra VRAM due to frozen LVFM feature extraction.\nAt inference, GT incurs no additional cost as the deployed detector is identical to the baseline student model, meeting our\ndesign goal of accuracy gains with minimal computational or deployment overhead.\n6.\nTypos and formatting issues: There are small mistakes like duplicate equation numbers (Equations 2 and 3)\nand minor spelling errors (e.g., \u201dMode collapse\u201d might need correction to \u201dModel collapse\u201d). Proofreading\ncarefully would improve readability and presentation quality.\nWe thank the reviewer for identifying these clarity issues. In the revised manuscript, we have ensured all equations are\nuniquely and sequentially numbered, with consistent cross-references throughout the text and supplementary material. This\nalso includes removing prior duplicate or skipped equation tags.\nWe have also clarified terminology where the earlier phrase \u201dmode collapse\u201d has been replaced by the more accurate \u201dmodel\ncollapse under EMA\u201d to describe instability in teacher\u2013student dynamics. The GAN-style term \u201dmode collapse\u201d is now\nused only when explicitly referring to distributional modes (in discussions of inter/intra-class feature distributions). This\ndistinction avoids confusion between model instability and distributional degeneracy.\nAll the corresponding typos and internal references have been corrected globally.\n5"
  },
  {
    "page": 6,
    "text": "Reviewer #4\nWe sincerely appreciate the reviewer\u2019s time and constructive feedback. Below, we address the key concerns raised, where we\nclarify our reasonings and implementations better.\n1. It is suggested that the author refine and rewrite the abstract section, particularly addressing the issues\nwith existing methods.\nWe thank the reviewer for this helpful suggestion. The Abstract has been completely rewritten for clarity, balance, and\ncompleteness.\nIt now (i) clearly identifies the key challenges of source-free object detection (SFOD) as lack of source data, noisy pseudo-\nlabels, and class imbalance/context bias (ii) briefly presents our three core modules RCM, SA/SAL and Expert supervision\nas a unified framework that measures and stabilizes the adaptation process, and (iii) highlights quantitative evidence on\nboth medical and natural benchmarks (Cityscapes\u2192Foggy 50.8 mAP and DDSM\u2192INBreast +5.9 AP50 over prior work).\nThe revised version is concise, results-driven, and accurately positions GT as an integrated, confusion-guided solution.\n2. It is recommended that the author revise the numbering system of the references. Additionally, the overall\nquantity of references appears excessive, which might lead to suspicions of artificially inflating the count.\nWe thank the reviewer for this observation. In the revised manuscript, we have ensured the bibliography in strict IJCV\nreference style, ensuring sequential numbering and consistent in-text citation order. All previous cross-reference mismatches\nhave been corrected across the main paper, figures, tables, and supplementary materials. All numbering, formatting, and\nmetadata (authors, year, venue) have also been verified and standardized according to IJCV guidelines.\nTo address the concern about citation volume, we conducted a thorough audit of the reference list and removed redundant,\ntangential, or outdated works, retaining only those directly relevant to source-free domain adaptation, self-training, and\nobject detection. The list has been revised to a concise, representative set of references, including essential recent methods\nlike CAT (CVPR 2024), IRG (CVPR 2023), LPLD (ECCV 2024), and SF-UT (ECCV 2024). Each retained citation now\nsupports a distinct conceptual or experimental point in the paper, avoiding any appearance of artificial inflation.\n3. The presentation of Figures 1 and 2 in the manuscript still leaves significant room for improvement, as they\nare not sufficiently clear. They need to be modified and improved.\nIn the revised version, both Figure 1 and Figure 2 have been completely redrawn using high-resolution vector graphics with\nself-contained captions to ensure clarity, legibility, and visual consistency with IJCV standards.\nFigure 1 (Motivation of Training Stability) now features a cleaner layout with larger fonts, simplified axes, and consistent\ncolor coding for all compared methods (GT, Mean-Teacher, SF-ST). The per-epoch stability curves are overlaid directly\nfor visual comparison, and the caption has been rewritten to emphasize that the figure illustrates the known issue of\nEMA-induced instability, not its discovery, aligning with our revised framing in \u00a7 1.\nFigure 2 (Framework Overview) has been redesigned for structural clarity. It now explicitly shows the flow of information\nfrom RCM \u2192(SA, SAL), the source-similar vs. source-dissimilar target split, and the attachment point of the Expert\nBranch. Training-only and inference-time paths are visually distinguished through color-coded arrows and labeled legends,\nwhile unnecessary elements have been removed.\nThese updates make both figures clearer, self-contained, and visually consistent, thereby improving the paper\u2019s readability\nand technical communication.\n4.\nIn the experimental section, the methods being compared are excessively outdated.\nIt is advisable to\ncompare them with the most recent methods, particularly those introduced in 2024 and 2025. Moreover,\nit is essential to conduct comparisons with articles from relevant journals, rather than relying solely on a\nsingle conference article.\n6"
  },
  {
    "page": 7,
    "text": "We thank the reviewer for this helpful observation. In the revised manuscript, we have strengthened both the quantitative\ncomparison and the discussion of contemporaneous methods in the Discussion section of \u00a7 4.\nOn the Cityscapes\u2192Foggy benchmark, our results already include the most recent ECCV\u201924 entries of LPLD (ECCV\u201924)\nand SF-UT (ECCV\u201924), as well as the contemporaneous IJCV\u201924 DACA method. We now explicitly highlight these lines in\nTable 4 and state in the text that GT achieves 50.8 mAP, outperforming LPLD (40.9 mAP), SF-UT (45.0 mAP), and DACA\n(46.2 mAP) under identical VGG-16 backbones and protocols. An accompanying paragraph in \u00a7 4 discusses these results,\nemphasizing that GT\u2019s confusion-aware RCM \u2192(SA,SAL) design yields stronger stability and minority-class consistency\nwhile maintaining source-free constraints.\nFor medical adaptation, we note that ECCV\u201924 and IJCV\u201924 SFOD methods have not released protocols or results for\nbreast or lesion detection, as their formulations and pseudo-label strategies are tuned to natural scenes. To provide context,\nwe clarify this limitation in \u00a7 4 and summarize our best-effort re-runs of LPLD and SF-UT on DDSM\u2192INBreast and\nDDSM\u2192RSNA, where GT remains superior (Table A2). Together with existing comparisons to IRG (CVPR\u201923) and D-\nMASTER (MICCAI\u201924), these additions ensure that GT is benchmarked against all major contemporary SFOD and UDA\nbaselines across both natural and medical domains.\nWe also add a note discussing SFOD trends and why medical SFOD results for these methods are not standardized/released\nin the Discussion section of \u00a7 4.\n5. The presentation of Tables 7 and 8 is inconsistent; it is recommended to revise and improve them.\nWe have restyled all the tables in our work (including Tables 7 and 8) to conform with the IJCV formatting standards and\nto ensure full internal consistency with the rest of the manuscript. Both the tables addressed now use a standard column\nstructure, decimal alignment, and clearly indicated IoU thresholds, evaluation metrics, and dataset identifiers.\nWe have unified caption typography to be self-contained and reduced redundancy by consolidating overlapping details into\nconcise explanatory notes in the corresponding sections. Extended ablation breakdowns and per-class deltas that previously\ncluttered the main tables have been moved to the Appendix for clarity and smoother narrative flow. All cross-references in\n\u00a7\u00a7 4\u20135 have been updated, ensuring that textual citations point to the correct table numbers after reordering.\n6. There are too many non-standard references, such as [2],[3],[4],[5],[10],[33],[51],[54],[59],[64],[68],[72],[77],[90],[93].\nIt is recommended to revise and improve them.\nWe thank the reviewer for this observation. In the revised manuscript, we have ensured the bibliography in strict IJCV\nreference style, ensuring sequential numbering and consistent in-text citation order. All previous cross-reference mismatches\nhave been corrected across the main paper, figures, tables, and supplementary materials.\nAll numbering, formatting,\nand metadata (authors, year, venue) have also been verified against official sources and standardized according to IJCV\nguidelines.\nTo address the concern about citation volume, we conducted a thorough audit of the reference list and removed redundant,\ntangential, or outdated works, retaining only those directly relevant to source-free domain adaptation, self-training, and\nobject detection. The list has been revised to a concise, representative set of references, including essential recent methods\nlike CAT (CVPR 2024), IRG (CVPR 2023), LPLD (ECCV 2024), and SF-UT (ECCV 2024). Each retained citation now\nsupports a distinct conceptual or experimental point in the paper, avoiding any appearance of artificial inflation.\n7"
  }
]