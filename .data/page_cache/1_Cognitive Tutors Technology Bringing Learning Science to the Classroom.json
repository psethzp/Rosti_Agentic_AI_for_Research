[
  {
    "page": 1,
    "text": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/255519934\nCognitive Tutors: Technology Bringing Learning Science to the Classroom\nArticle \u00b7 January 2006\nCITATIONS\n521\nREADS\n3,924\n2 authors, including:\nKenneth R. Koedinger\nCarnegie Mellon University\n498 PUBLICATIONS\u00a0\u00a0\u00a026,955 CITATIONS\u00a0\u00a0\u00a0\nSEE PROFILE\nAll content following this page was uploaded by Kenneth R. Koedinger on 04 April 2016.\nThe user has requested enhancement of the downloaded file."
  },
  {
    "page": 2,
    "text": "Chapter 5 \n \nPage 135 \nCognitive Tutors: Technology Bringing Learning Science to the Classroom  \nKenneth R. Koedinger \nAlbert Corbett \nIntroduction \nIndividual tutoring is perhaps the first instructional method.  It dates back \nat least to Socrates and the Socratic method.  Although one-to-one tutoring by \nexpert human tutors has been shown to be much more effective than typical one-\nto-many classroom instruction (Bloom, 1984), it has not been economical to \nprovide every child with an individual tutor.  Lectures and books became \npervasive in education to spread knowledge at lower cost.  However, increasing \ncapabilities of computer hardware and software have been creating new \nopportunities to bring one-to-one tutoring to more students.  Furthermore, \ncomputer technology provides an opportunity to systematically incorporate \nadvances in learning science into the classroom, to test associated principles of \nlearning, and best adapt them to the needs of students and teachers. \nEarly attempts to use computers for instruction included Computer-Aided \nInstruction (Eberts, 1997) and then, later Intelligent Computer-Aided Instruction \nor Intelligent Tutoring Systems (Sleeman & Brown, 1982; Wenger, 1987; \nCorbett, Koedinger, & Anderson, 1997).  Computer-based instruction has been \nshown to be effective in increasing student learning beyond normal classroom \ninstruction (e.g., Kulik & Kulik, 1991), however, not to the level of human tutors \n(Bloom, 1984).  Early attempts at Intelligent Tutoring Systems included \nmimicking Socratic dialog in teaching electronics trouble-shooting, adding \nintelligent questioning to an existing Computer-Aided Instruction system for"
  },
  {
    "page": 3,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 136 \nlearning South American geography, adding tutoring strategies to an existing \n\u201cexpert system\u201d for medical diagnosis, and adding tutoring strategies to an \nexisting educational game for mathematics (Sleeman & Brown, 1982).  \nIn a parallel development that dates back even earlier, cognitive theories \nof human learning, memory, and problem solving were being implemented as \ncomputational models with computers (Newell & Simon, 1972). In the mid-\n1980s, John R. Anderson and colleagues merged these two strands and introduced \na more interdisciplinary approach to Intelligent Tutoring System development and \ntesting (Anderson, Boyle, & Reiser, 1985) that added the discipline of cognitive \npsychology to the discipline of artificial intelligence that had previously been the \nprime mover. The Intelligent Tutors emerging from this approach were \nconstructed around computational cognitive models of the knowledge students \nwere acquiring and began to be called \u201cCognitive Tutors\u201d (Anderson et al., 1995).  \nThese cognitive models represent learner thinking or cognition in the domain of \ninterest, whether it is algebra, programming, scientific reasoning, or writing \nessays.  The cognitive model also includes a representation of the kinds of early \nlearner strategies and misconceptions that are steps in the trajectory from novice \nto expert. \nFull-scale Cognitive Tutors have been created to help students learn in a \nvariety of domains including middle and high school mathematics (Koedinger, \nAnderson, Hadley, & Mark, 1997; Koedinger, 2002), computer programming \n(Anderson et al, 1995; Mathan & Koedinger) and college-level genetics (Corbett \net al 2005).  Cognitive Tutors typically speed learning or yield greater learning \nrelative to conventional problem-based instruction (Anderson, et al., 1995) and"
  },
  {
    "page": 4,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 137 \napproach the effectiveness of good human tutors (Corbett, 2001). The most \nwidely distributed Cognitive Tutor is one for algebra, which is part of a complete \ncourse for high school algebra and in 2004-2005 was in use in some 2000 schools \nacross in the United States. As described later in the paper, students in Cognitive \nTutor Algebra I have been shown to score twice as high on end-of-course open-\nended problem solving tests and 15% higher on objective tests as students \nenrolled in a traditional Algebra course. A few of these schools are high \nperforming, resource rich suburban schools, but most of them are urban or rural \nschools, with average teachers and with a relatively large number of economically \ndisadvantaged, minority or learning disabled students. We roughly estimate some \nhalf million students have used the tutor for a total of about 20 million student-\nhours.   \nCognitive Tutors Provide Aspects of Human Tutoring \nCognitive Tutors support learning by doing, an essential aspect of human \ntutoring.  Learning by doing is the idea of putting students in performance \nsituations whereby the objective concepts and skills can be applied and \ninstruction can be provided in the context of or in response to student needs1. \nCognitive Tutors accomplish two of the principal tasks characteristic of human \ntutoring: (1) monitoring the student\u2019s performance and providing context-specific \ninstruction just when the individual student needs it, and (2) monitoring the \nstudent\u2019s learning and selecting problem-solving activities involving knowledge \ngoals just within the individual student\u2019s reach. \nThis monitoring of students\u2019 performance and learning makes use of the \ncognitive model and two key algorithms, model tracing and knowledge tracing. In"
  },
  {
    "page": 5,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 138 \nmodel tracing, the cognitive tutor runs the cognitive model forward step-by-step \nalong with the student to follow the student\u2019s individual path through complex \nproblem spaces, providing just-in-time accuracy feedback and context-specific \nadvice. In knowledge tracing, the tutor employs a simple Bayesian method of \nestimating the student\u2019s knowledge and employs this student model to select \nappropriate problems. \nChapter Overview \nIn the following section we describe Cognitive Tutors and their foundation \nin ACT-R theory. Extensive Cognitive Tutor research has served both to validate \nand modify the ACT-R cognitive architecture model (cf. Anderson & Lebiere, \n1998) and we review six general principles of intelligent tutor design that were \nderived from this theoretical framework (Anderson et al., 1995). While ACT-R \ntheory provides an important cognitive modeling framework, it does not prescribe \ncourse curriculum objectives and activities, it cannot precisely anticipate the prior \nknowledge that students bring with them to a course or a problem-solving \nactivity, and it cannot prescribe scaffolding activities to help students develop a \ndeep understanding of domain knowledge. In the final section of the chapter, we \ndescribe the learning science principles and methods that we have employed to \naddress these instructional design questions.  \n==INSERT FIGURE 1 HERE== \nCognitive Tutor Algebra: A Brief Example  \nA screen shot of a unit in Cognitive Tutor Algebra is shown in Figure 1.  \nCognitive Tutors tend to have relatively rich graphical user interfaces that provide \na workspace in which students can demonstrate a wide variety of problem solving"
  },
  {
    "page": 6,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 139 \nbehavior.  The workspace changes as students progress through units.  The \nworkspace in Figure 1 includes a problem scenario window in the upper left \nwhere students are presented with a problem situation, often with real facts or \ndata, that they are expected to analyze and model using the tools in the \nworkspace.  The tools illustrated in Figure 1 are the Worksheet, Grapher, and \nSolver.  In this unit, the Worksheet has automated features like a spreadsheet. \nOnce students write the algebraic expression for the height \u201c67+2.5T\u201d given the \ntime, then the worksheet computes a height value (e.g., 117) when a time value is \nentered (e.g., 20).  In earlier units, the Worksheet does not have these automated \nfeatures, but is more like a table representation on paper and students must \ndemonstrate they can perform the steps on their own..  Similarly, the Grapher and \nSolver tools change as students advance through tutor units.  Initially these \nbehave much like blank pieces of paper where students do all the work.  Later \nthese tools begin to automate lower level skills, like plotting points or performing \narithmetic and let students focus on acquiring higher-level concepts and skills, \nlike deciding what the symbolic function to graph or what algebraic manipulation \nto perform. As students work, the Cognitive Tutor monitors their performance and \nmay provide just-in-time feedback or on-demand solution-sensitive hints in the \nhint window.  The Cognitive Tutor also monitors student learning and displays \nthese results in the Skills chart, shown in the top center of Figure 1. \nIt is critical to consider the social context of use of any technology or \neducational innovation and Cognitive Tutors are no exception.  We have tended \nto create complete Cognitive Tutor courses whereby we apply learning sciences \ntheory to develop instructional materials, like consumable textbooks, in addition"
  },
  {
    "page": 7,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 140 \nto Cognitive Tutor software. Virtually all schools using our mathematics \nCognitive Tutors also use the curriculum and text materials.  The typical \nprocedure is to spend 2 days a week in the computer lab using the Cognitive \nTutor software and 3 days a week in the regular classroom using our text \nmaterials.  In the classroom, learning is active, student-centered, and focused \nprimarily on learning by doing.  Teachers spend less time in whole-group lecture \nand more time facilitating individual and cooperative problem solving and \nlearning.  In the classroom, students often work together in collaborative groups \nto solve problems similar to those presented by the tutor.  Teachers play a key \nrole in helping students to make connections between the computer tools and \npaper and pencil techniques. \nLearning Sciences Theory Behind Cognitive Tutors \nCognitive Tutors are based on the ACT-R theory of learning and \nperformance (Anderson & Lebiere, 1998). The theory distinguishes between \nimplicit performance knowledge, called \u201cprocedural knowledge\u201d and explicit \nverbal knowledge and visual images, called \u201cdeclarative knowledge\u201d.  According \nto ACT-R, performance knowledge can only be learned by doing, not by listening \nor watching.  In other words, it is induced from constructive experiences -- it \ncannot be directly placed in our heads.  Such performance knowledge is \nrepresented in the notation of if-then production rules that associate internal goals \nand/or external perceptual cues with new internal goals and/or external actions. \nExamples of English versions of production rules are shown in Table 1. \n==INSERT TABLE 1 HERE=="
  },
  {
    "page": 8,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 141 \nProduction rules characterize how both advanced and beginning students \nthink or reason in a domain.  Students may acquire informal, heuristic, or \nincorrect patterns of thinking that are different than the concepts and rules that are \nnormatively taught or presented in textbooks.   Learning sciences researchers \nhave identified \u201cinformal\u201d or \u201cintuitive\u201d forms of thinking that students may learn \nimplicitly and outside of school (cf. Lave, Murtaugh, de la Rocha, 1984; Resnick, \n1987).  Production rules can represent such thinking patterns as illustrated by \nproduction #1 in Table 1, which represents an informal alternative to the formal \napproach of using algebraic equations like \u201c8x = 40\u201d (cf., Koedinger & Nathan, \n2004).   Production rules can also represent heuristic methods for discovering \napproaches to solutions (Polya, 1957). Production #2 in Table 1 does not suggest \nany particular operation per se, but characterizes how a good problem solver may \nthink through a plan of action before selecting a particular operation or theorem to \napply.  Non-traditional strategies  can be represented in production rules, as \nillustrated by #3 in Table 1, which characterizes the use of a graphical rather than \nsymbolic strategy for solving an equation. \nThe if-part of a production rule can help identify when the knowledge \nstudents acquire is not at the right level of generality.   For instance, production \n#4 in Table 1 is too specific\u2014it represents when students can combine like terms \nin an equation when coefficients are present (e.g., 2x+3x -> 5x) but not when a \ncoefficient is missing (e.g., x \u2013 0.2x).  Alternatively, students sometimes acquire \nproductions that are too general. Production #3 in Table 1 represents how students \nmay learn to combine numbers by the operator between them (e.g., 2*3+4=x ->"
  },
  {
    "page": 9,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 142 \n6+4=x) without acquiring knowledge that prevents order of operations errors \n(e.g., x*3+4=10 -> x*7=10).  \nThe Cognitive Model and Model Tracing in Cognitive Tutors \nDeveloping Cognitive Tutor software involves the use of the ACT-R \ntheory and empirical studies of learners to create a \"cognitive model\".  A \ncognitive model uses a production system to represent the multiple strategies \nstudents might employ as well as their typical student misconceptions. To take a \nsimplified example from an Algebra equation solving problem, Figure 2 depicts 3 \nproductions that can apply in solving the equation 3(2X + 5) = 9. The production \nrule in Strategy 1 distributes (multiplies) \u201c3\u201d across the sum (2X + 5). The \nproduction rule in Strategy 2 divides both sides of the equation by \u201c3.\u201d The third \nrule is a \u201cbuggy\u201d production that represents a misconception (cf., Matz, 1982) and \nfails to fully distribute the \u201c3\u201d across the sum (2X + 5).  \n==INSERT FIGURE 2 HERE== \nBy representing alternative strategies for the same goal, the Cognitive \nTutor can follow different students down different problem solving paths of the \nstudents\u2019 own choosing, using an algorithm called \u201cmodel tracing.\u201d Model tracing \nallows the Cognitive Tutor to trace each student\u2019s problem-solving steps and \nprovide individualized assistance that is just-in-time and sensitive to the students\u2019 \nparticular approach to a problem.  When a student performs a step, it is compared \nagainst the alternative next steps that the cognitive model generates.  There are \nthree categories of response. For example, in Figure 2, if a student\u2019s problem \nsolving action matches either strategy 1 or strategy 2, the tutor highlights the step \nas correct and the student and tutor move on to the next step.  Second, if the"
  },
  {
    "page": 10,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 143 \nstudent action, like \u201c6x + 5 = 9\u201d, is matched by a buggy production the tutor \nhighlights the step as incorrect and presents a feedback message, like \"You need \nto multiply 5 by 3 also.\" This message is generated from a template attached to \nthe buggy rule, with the variables \u201cc\u201d and \u201ca\u201d getting context-specific values from \nthe matching of the production rule to the current situation.  Third, if the student \nperforms a problem-solving action that does not match the action of any rule in \nthe cognitive model, the tutor simply flags the action as an error, for instance, by \nmaking the text red and italicized.  \nAt any time the student can request a hint (e.g., by clicking on the \u201c?\u201d \nbutton shown in figure 1).  Again the tutor runs the model forward one step, \nselects one of the model productions that matches and presents advice text \nattached to the production. For instance, the hint associated with strategy 1 in \nFigure 2 will say \u201cDistribute 3 across the parentheses\u201d because the production \nvariable \u201ca\u201d has the value 2 in this case. \nKnowledge Tracing in Cognitive Tutors \nACT-R theory holds that knowledge is acquired gradually and the brain \nessentially keeps statistics on the frequency, recency, and utility of knowledge \ncomponents including production rules (Anderson & Lebiere, 1998). The \nKnowledge Tracing algorithm in Cognitive Tutors monitors students\u2019 gradual \nacquisition of production rules across problem solving activities. At each \nopportunity to apply a production rule in problem solving, the tutor updates its \nestimate of the probability the student knows the rule based on whether the \nstudent applies the rule correctly. Knowledge Tracing employs a Bayesian update \nand has been shown to predict students\u2019 performance and posttest accuracy"
  },
  {
    "page": 11,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 144 \n(Corbett & Anderson, 1995). These probability estimates are displayed with \"skill \nbars\" in the computer tutor interface (see the lower right in Figure 1).  The \nCognitive Tutor uses these estimates to determine when a student is ready to \nmove on to the next section of the curriculum, thus adapting the pacing of \ninstruction to individual student needs.  New problems are individually selected \nfor students to provide more instruction and practice on the skills that have not yet \nbeen mastered (i.e., the ones for which the estimate is less than 95% that the \nstudent knows that skill).  \nWhy Production Rule Cognitive Models are Powerful \nA key feature of production rules is that they are modular, that is, they \nrepresent knowledge components that can be flexibly recombined.  It does not \nmatter how a student reached the state of \u201c3(2x + 5) = 9\u201d shown in Figure 2.  It \nmay have been the result, for instance, of translating a story problem into this \nequation or of simplifying a more complex equation (e.g., \u201c3(2x + 5) + 10 = 19\u201d) \ninto this form. In any case, the production rule model is always applied to the \ncurrent state of the problem regardless of how the student reached the current \nstate. \nThis modularity makes developing Cognitive Tutors more feasible as the \nproduction rules can be reused and recombined in different ways to follow \nstudents in a potentially infinite variety of problems within a course unit or even \nacross courses (e.g., the equation solving cognitive model is used in the Geometry \nCognitive Tutor as well as in Algebra).  In addition to facilitating development, \nmodularity is a key scientific claim of ACT-R that yields empirically testable \npredictions.  For instance, knowledge will transfer from a learning activity to an"
  },
  {
    "page": 12,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 145 \nassessment activity to the extent that the kind and number of productions needed \nin the learning activity are also applicable in the assessment activity (Singley and \nAnderson, 1989). \nModel and Knowledge Tracing Implement Features of Human Tutoring \nModel and knowledge tracing algorithms implement key features of \nhuman tutoring and apprenticeship training (cf., Bloom, 1984; Collins, Brown & \nNewman, 1989; McArthur, Stasz, & Zmuidzinas,1990; Vygotsky, 1978).  The \ntutor gives the student a task and monitors how well the student is performing the \ntask.  Model tracing is a form of such monitoring.  When the student strays too far \nfrom the tutor\u2019s model of desired performance, the tutor may intervene and \nprovide feedback.  If the student is stuck, the tutor can provide hints or \nperformance assistance based on his or her own domain expertise.  The cognitive \nmodel provides the domain expertise or model of desired performance in a \nCognitive Tutor.  After the student finishes the task, the tutor selects a next task \nbased on the tutor\u2019s sense of what the student knows and does not know.  \nKnowledge tracing implements a method for determining, over time, what each \nstudent seems to knows and not know. \nPrinciples and Methods for Cognitive Tutor Design  \nCognitive Tutor Design Principles \nIn 1995, we published a report on the status of the lessons learned from \nCognitive Tutor development (Anderson,et al., 1995).  We described some \ngeneral Cognitive Tutor design principles consistent with ACT-R and our \nresearch and development experience to that date.  Here we review the status of \nthe six most frequently used of those principles.  Table 2 lists these six principles,"
  },
  {
    "page": 13,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 146 \nslightly rephrased based on our experiences since the 1995 paper.  We will briefly \ndescribe these principles and then provide more extended examples of two of \nthem. \n==INSERT TABLE 2 HERE== \n1. Represent student competence as a production set \nThe principle \"represent student competence as a production set\" suggests \nthat the instructional designer guides design based on an analysis not of domain \ncontent per se, but of the way in which students think about the content. \nAcquiring competence in a domain is complex and we tend to be surprisingly \nunaware of the immense number of details and subtle decision capabilities we \nimplicitly acquire on the way to expertise (Berry & Dienes, 1993). Complex tasks \nlike reading become second nature to us with time and we forget or perhaps are \nnever quite aware of the learning experiences and resulting knowledge changes \nthat led to such competence.  Skinner (1968) estimated that to perform at 4th grade \nlevel in math, a student must acquire about 25,000 \u201cchunks\u201d of knowledge (p. \n17).   Production rules provide a way to represent such chunks of knowledge and \ndecision capabilities.   \nThe modularity of production rules in a production set predicts that we can \ndiagnose specific student weaknesses and focus instructional activities on \nimproving these.   The context-specific nature of production rules means that \ninstruction cannot be effective if it does not connect knowledge with its contexts \nof use.  Students need true problem solving experiences to learn the if-part of \nproductions, the conditions for appropriate use of domain principles."
  },
  {
    "page": 14,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 147 \n2. Provide instruction in a problem-solving context \nA fundamental assumption of ACT-R is that people learn by doing as the \nbrain generalizes from one's explicit and implicit interpretations or \"encodings\" of \none's experiences.  It is not the information or even the instructional activities \nstudents are given per se that matters, but how students experience and engage in \nsuch information and activities that determines what knowledge they construct \nfrom them. Thus, another principle in Anderson et al. (1995) was \"provide \ninstruction in the problem-solving context\". This principle is consistent with the \nlearning sciences finding that instruction should be situated in authentic tasks. \n3. Communicate the goal structure underlying the problem solving \nAmong the formidable challenges facing novice problem solvers in \ncomplex problem solving is decomposing an initial problem statement into \nsuccessive subgoals and keeping track of these subgoals (Singley, 1990). The \nunderlying goal structure of a problem solution often remains hidden in \ntraditional problem-solving representations. We have employed two methods for \nmaking the goal structure explicit. First, we develop interfaces that make the goal \nstructure visible in the problem-solving interface (Collins et al., 1989). The most \nnotable examples of this strategy are the geometry proof tutors (Koedinger & \nAnderson, 1993). A variety of studies have shown that explicit goal-structure \nscaffolding in the problem-solving interface can speed problem solving even in \nsimple problems (Corbett & Trask, 2000) and result in better learning outcomes \nin more complex problem solving (Scheines and Sieg, 1994; Singley,1990). \nSecond, the underlying goal structure of the problem can be communicated \nthrough help messages. Typically in model tracing tutors, the first level of help is"
  },
  {
    "page": 15,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 148 \na description of the current goal in the context of the overall problem. Subsequent \nhelp messages advise on how to achieve the goal.  \n4. Promote a correct and general understanding of the problem-solving \nknowledge \nIn learning to problem solve, students construct production rules based on \ntheir own, perhaps idiosyncratic understanding or encoding of problem-solving \nactivities and examples. For example novices have been shown to encode physics \nproblems based on superficial features of the problems, rather than the underlying \nphysical principles that apply (Chi, Feltovich and Glaser, 1981).  In geometry \nproblem solving, students notoriously will conclude that angles are equal in \nmeasure because they look equal rather than because structurally they must be \n(Aleven & Koedinger, 2002). As illustrated above, students can acquire overly \ngeneral productions that generate errors outside of the situations very similar to \nthose in which they were acquired and overly specific rules that fail to transfer as \nexpected across problem solving contexts. We have successfully deployed \ndifferent strategies to help students generate a more general understanding and \nillustrate one below.  \n5. Minimize working memory load that is extraneous to learning \nIt has been documented that errors in complex problem solving can stem \nfrom loss of information from working memory (Anderson & Jeffries, 1988) and \nthat high working memory load or \u201ccognitive load\u201d can impede learning (Sweller, \n1988). As a result, we have employed multiple tactics in cognitive tutors to \nreduce such load. Efforts to make the goal structure visible (principle 3) can help \nreduce working memory load. Another strategy is to simplify problem-solving"
  },
  {
    "page": 16,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 149 \nactions in the interface that are irrelevant to the current learning goals. For \nexample, the equation solver in our mathematics tutors has an auto-arithmetic \nmode in which students only indicate the algebraic operation to perform in each \nsolution step, but they are not required to perform the arithmetic (Ritter and \nAnderson, 1995).  \nSimilarly, our programming cognitive tutors employ structure editors to \nreduce the working memory load of remembering surface syntax. Littman (1991) \nreports human tutor behavior that is similar to this strategy.  Human tutors avoid \ninterrupting students and disrupting their working memory state to point out \nrelatively minor errors that have little consequence for the overall problem \nsolution.  \n6. Provide immediate feedback on errors relative to the model of desired \nperformance \nStudies have shown that human tutors tend to provide immediate feedback \nafter each problem-solving step (Merrill, Reiser, Ranney & Trafton, 1992), \nalthough the feedback may be minimal (Lepper, Aspinwall, Mumme & Chabay, \n1990; Fox 1991) and provided only on \u201cimportant\u201d errors (Littman, 1991). But \nthese studies don\u2019t reveal the relative benefits of immediate feedback. In a study \nwith the Lisp Cognitive Tutor immediate feedback led to significantly faster \nlearning (Corbett & Anderson, 2001). Not only can immediate feedback make \nlearning more efficient, but it can also be motivating for students (Schofield, \n1995). Mathan and Koedinger (2003) demonstrate benefits of providing \nimmediate feedback relative to an \u201cintelligent novice\u201d model of desired"
  },
  {
    "page": 17,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 150 \nperformance that allows for certain student errors and so appears like delayed \nfeedback when compared to an error-free expert model of desired performance. \nCognitive Tutor Meta-Design Principles \nThe strengths of ACT-R and the Cognitive Tutor principles are that they \nare general and can apply in multiple domains.  However, these principles beg \nsome higher-level curriculum design questions: What should students be \nlearning? What problem-solving activities support that learning? What relevant \nknowledge do students bring with them? We need to design cognitive tutor \nactivities that not only \u201cwork,\u201d but work well within the curricular and social \ncontext of course objectives, teacher practices, and classroom use.  Here we \nabstract that experience in a set of Cognitive Tutor \u201cMeta-Design\u201d principles.  \n1. Design with Instructors and Classroom Use from the Start \nAn experienced classroom teacher plays many key roles in a Cognitive \nTutor development project, contributing hard-won knowledge of the specific \nlearning hurdles students face and tactics for helping students past those hurdles. \nAn experienced teacher also plays essential roles in integrating Cognitive Tutor \nactivities with other course activities. First, an experienced teacher will help guide \nthe initial tutor development so that the tutor dovetails with other course \nactivities. Second, an experienced teacher who is part of the design team is best \npositioned to take the technology into the classroom and provide informed \nobservations on situations in which the classroom activities and tutor activities do \nnot mesh."
  },
  {
    "page": 18,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 151 \n2. Design the Full Course Experience \nWe encountered a high-level curriculum compatibility problem in the \nANGLE Geometry Proof Project (Koedinger & Anderson, 1993). Between the \nstart of the project and the first classroom piloting, the school district had adopted \na new curriculum that de-emphasized proofs and thus, it became difficult to \nintegrate the tutor into the curriculum. The project teacher who was intimately \nfamiliar with the tutor\u2019s curriculum objectives was more successful in integrating \nthe tutor into the new curriculum than other teachers and obtained greater learning \neffects. In the aftermath of the ANGLE project, we have developed the full set of \ncourse activities in all our Cognitive Tutor math courses, including the course text \nand assignments for two related reasons. First, it spares the classroom teacher \nfrom having to figure out how to integrate the Cognitive Tutor activities into the \ncourse. Second, it enables us to develop a course that more fully emphasizes \nproblem-based learning throughout. \n3. All Design Phases should be Empirically-Based \nThe designer should collect student data to guide and test the application \nof principles, including (a) design experiments that guide initial development, (b) \nformative evaluations that analyze the successes and failures of problem-solving \nactivities at a fine grain-size, and (c) summative evaluations that examine whether \ncourse-level curriculum objectives are being achieved.  A spectrum of empirical \nresearch methods are available from lower cost, lower reliability to higher cost, \nhigher reliability (e.g., Koedinger, 2002)."
  },
  {
    "page": 19,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 152 \nDesign Research Examples \nAn important message of this chapter is that we not only need to make \nprogress in better articulating theory and principles, but also in specifying \nassociated empirical and analytic methods that better ensure these principles will \nbe appropriately applied. \nThe following three sections provide extended examples of each of these \nthree classes of empirical research. The first section describes the use of design \nstudies to guide application of the fifth principle, reduce working memory load. \nThe second section describes the use of design studies to guide application of the \nfourth principle, promote a general understanding. The third section describes \nsummative evaluations of the Cognitive Tutor Algebra course. \nDesign Research Guides Reduction of Working Memory Load \nIn addition to the strategies discussed above to \u201cminimize working \nmemory load\u201d, we employed the strategy to design instruction that builds on \nstudents\u2019 prior knowledge  (cf., Bransford, Brown, & Cocking, 1999).  When \ninstruction makes connections to what students already know, they need less \ncognitive load to process, understand, and integrate new knowledge into long-\nterm memory.   \nWhile building on prior knowledge is a more specific strategy for \nminimizing working memory load, how do we know what prior knowledge \nstudents have?  Sometimes theoretical analysis of domain content is used to \npredict prior knowledge under the assumption that smaller component tasks are \nmore likely to tap prior knowledge than larger whole tasks (cf., Van Merrienboer, \n1997). However, while smaller tasks typically involve fewer knowledge"
  },
  {
    "page": 20,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 153 \ncomponents, they are not always simpler for students. It is not the surface form of \ntasks that determine how accessible they are to students. Instead, it is the internal \nmental representations that students acquire and use in task performance that \ndetermines what will be simple or not.  Thus, to identify what prior knowledge \nstudents have and which tasks are most likely to tap prior knowledge, it is not \nsufficient to analyze the content domain.  Instead it is critical to study how \nstudents actually perform on tasks \u2013 to see student thinking as it really is, not as a \ncontent analysis might assume it to be. \nConsider the three problems shown below, a story problem, a word \nproblem, and an equation, all with the same underlying quantitative structure and \nthe same solution.   \nStory Problem: As a waiter, Ted gets $6 per hour.  One night he made \n$66 in tips and earned a total of $81.90.  How many hours did Ted \nwork? \nWord Problem: Starting with some number, if I multiply it by 6 and then \nadd 66, I get 81.90.  What number did I start with? \nEquation:  x * 6 + 66 = 81.90 \nWhich would be most difficult for high school students in a first year algebra \ncourse?  Nathan & Koedinger (2000) discussed results of surveys of mathematics \nteachers on a variation of this question.  The survey respondents tended to predict \nthat such story problems would be most difficult and such equations would be \neasiest.  Typical justifications for this prediction include that the story problem \nrequires more reading or that the way the story problem is solved is by translating \nto the equation."
  },
  {
    "page": 21,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 154 \nIn contrast, Koedinger & Nathan (2004) found that students perform best at \nthe story and word problems (70% and 61% respectively) like these and worst at \nthe analogous equations (42%).  Clearly many students were not solving the story \nand word problems using equation solving.  Instead they used alternative informal \nstrategies like guess-and-test and \"unwinding\", working backwards from the \nresult, inverting operations to find the unknown starting quantity.  Students had \ndifficulty in comprehending equations and, when they did succeed in \ncomprehending, they often had difficulty in reliably executing the equation \nsolving strategy.  \nThis result is important within the algebra domain.  It indicates that if we want \nto create instruction that builds on prior knowledge, we should make use of the \nfact that beginning algebra students have quantitative reasoning skills that can be \ntapped through verbal or situational contexts.  Unlike many textbooks that teach \nequation solving prior to story problem solving (Nathan, Long, & Alibali, 2002), \nit may be better to use story problem situations and verbal descriptions first to \nhelp students informally understand quantitative relationships before moving to \nmore abstract processing of formal representations. \nThis study illustrates why we advocate the mantra \u201cthe student is not like \nme\u201d. We need empirical methods to see past our biases or \u201cexpert blind spot\u201d to \nwhat students are really like. \nTable 3 further illustrates why it is important to use empirical methods to \ndetermine when and how to employ a principle.  Using problem situations to \nbuild on prior knowledge and reduce working memory load will not work if those \nproblem situations are not familiar.  In our development of Cognitive Tutor Math"
  },
  {
    "page": 22,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 155 \n6 (Koedinger, 2002), we used a Difficulty Factors Assessment to find which kinds \nof problem situations make problems easier for students and which kinds do not. \nTable 3 illustrates different content areas in middle school math where we \ncompared concrete story problem situations with abstract context-free problems.  \n==INSERT TABLE 3 HERE== \nTable 3 shows 6th graders' average percent correct on multiple pre-test \nitems in each content area. In three of the areas, the problem situation consistently \nfacilitates performance significantly above the abstract problem. These are \ndecimal place value, decimal arithmetic, and fraction addition.  In data analysis, \nthe situation facilitated performance on a global interpretation task, but not on a \nlocal interpretation task.  In the area of factors and multiples, the situation \nreduced performance. \nThus using situations to build on prior knowledge may not be effective for \nconcepts and procedures related to factors and multiples unless situations can be \nfound that are easier to understand than abstract problems. While one might still \nwant to use such a problem situation as motivation for learning, given this data, it \ndoes not appear that such a situation will provide a student with an easier, less \ncognitively taxing access to understanding of the domain content.  \nReflection Promotes General Understanding \nOne well-researched approach to promoting a correct and general \nencoding is called \u201cself-explanation\u201d whereby students explain to themselves \nsteps in problem solutions (e.g., Chi, de Leeuw, Chiu, & Lavancher, 1994). \nAleven and Koedinger (2002) implemented a version of self-explanation in \nCognitive Tutor Geometry and experimented with its effectiveness. Figure 3"
  },
  {
    "page": 23,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 156 \nillustrates the \u201cexplanation by reference\u201d approach employed whereby students \nprovided explanations for problem solving steps by making reference to geometry \nrules or reasons in an on-line glossary.  Students could either type the name of the \nrule or select it from the glossary.  This form of explanation is different from the \nspeech-based explanations in most prior experiments on self-explanation, but has \nthe benefit of the explanations being machine readable without the need to \nimplement natural language understanding.   \nPrior Difficulty Factors Assessments had indicated that students were \nbetter able to perform a problem-solving step, like determine that angle ARN in \nFigure 3 is equal to 43.5 degrees, than to explain that step by referring to the \n\u201cAlternate Interior Angles\u201d rule.  One reason for this difference is that students\u2019 \nprior knowledge includes over-generalized production rules like \u201cif an angle looks \nequal to another, then it is\u201d that can provide correct answers to steps, but without \nan understanding of when such steps are justified and why. Such over-generalized \nproductions may result from shallow encoding and learning.  According to \nAleven & Koedinger\u2019s ACT-R interpretation, self-explanation promotes more \ngeneral encoding because students think more deliberately, with greater explicit \nreflection, about the verbal declarative representation of domain rules.  This \ndeliberate reflection helps identify key features of the domain rules and thus \nimproves the accuracy of the implicit inductive process of \u201ccompiling\u201d (a form of \nlearning) production rules from examples and visual input.  Indeed, Aleven and \nKoedinger (2002) found that students using the self-explanation version of \nCognitive Tutor Geometry were not only better able to provide accurate \nexplanations, but learned the domain rules with greater understanding such that"
  },
  {
    "page": 24,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 157 \nthey could better transfer to novel problems and avoid shallow inferences, like the \n\u201clooks equal\u201d production illustrated above. \nSummative Field Study Evaluations and Classroom Observations \nWe originally assessed Cognitive Tutor Algebra in experimental field \nstudies in city schools in Pittsburgh and Milwaukee, replicated over 3 different \nschool years.  The assessments used in these field studies targeted both 1) higher \norder conceptual achievement as measured by performance assessments of \nproblem solving and representation use and 2) basic skills achievement as \nmeasured by standardized test items, for instance, from the math SAT.  In \ncomparison with traditional algebra classes at the same and similar schools, we \nhave found that students using Cognitive Tutor Algebra perform 15-25% better \nthan control classes on standardized test items and 50-100% better on problem \nsolving & representation use (Koedinger, et al., 1997; Corbett, Koedinger, & \nHadley, 2001; also see http://www.carnegielearning.com/results/reports). More \nrecent studies have continued to confirm the validity of the approach. For \nexample, the Moore (Oklahoma) Independent School District conducted a within-\nteacher experiment (Morgan & Ritter,2002). Eight teachers at four junior high \nschools taught some of their classes using Cognitive Tutor Algebra I and others \nusing their traditional textbook.  On the ETS End-of-Course Algebra exam, \nstudents taking the Cognitive Tutor curriculum scored significantly higher than \ncontrol students.  Cognitive Tutor students also earned higher course grades and \nhad more positive attitudes towards mathematics. \nThe deployment of educational technology in the classroom has an impact \nbeyond learning outcomes. Following the observations of Schofield, Evans-"
  },
  {
    "page": 25,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 158 \nRhodes, & Huber (1990) and Wertheimer (1990), we have also observed the \nimpact of the use of Cognitive Tutor Algebra on changes in classroom social and \nmotivational processes (Corbett et al., 2001).  Visitors to these classrooms often \ncomment on how engaged students are. Cognitive Tutor Algebra may enhance \nstudent motivation for a number of different reasons.  First, authentic problem \nsituations make mathematics more interesting, sensible, or relevant.  Second, \nstudents on the average would rather be doing than listening, and the incremental \nachievement and feedback within Cognitive Tutor Algebra problems provide a \nvideo-game-like appeal.  Third, the safety net provided by the tutor reduces \npotential for frustration and provides assistance on errors without social stigma.  \nFinally, the longer-term achievement of mastering the mathematics is \nempowering. \nConclusions and Future Work \nHuman tutoring is an extremely effective and enjoyable way to learn.  But, \nbuying one computer per student is a lot more cost effective than hiring a teacher \nfor every student.  Before a computer program can function as a tutor, it has to be \nable to do several key things that human tutors can do: 1) use domain knowledge \nto solve problems and reason as we want students to do; 2) have knowledge of \ntypical student, misconceptions, relevant prior informal knowledge, and learning \ntrajectories; 3) follow student reasoning step by step and understand when and \nwhere students reveal a lack of knowledge or understanding; 4) provide \nappropriate scaffolding, feedback and assistance to students when they need it and \nin the context of that need; 5) adapt instruction to individual student needs based \non an on-going assessment of those needs.  The cognitive model, model tracing"
  },
  {
    "page": 26,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 159 \nand knowledge tracing algorithms of the Cognitive Tutor architecture provide \nthese key behaviors of good tutoring. \nCognitive Tutors and ACT-R are one manifestation of the many advances \nin cognitive psychology and instructional design made by learning scientists.  \nCognitive Tutors implement a decidedly simple form of tutoring.  More \nsophisticated tutoring strategies can be imagined (cf., Collins et al., 1989) and \nsome of these strategies, such as natural language tutorial dialog, are being \nimplemented in increasingly practical forms (e.g.,  Jordan, Ros\u00e9, & VanLehn, \n2001; Wiemer-Hastings, Wiemer-Hastings,  & Graesser, 1999).  Experimental \nstudies are testing whether such added sophistication leads to increased student \nlearning (e.g., Aleven, Popescu, & Koedinger, 2003).  There is also substantial \nresearch on whether even simpler forms of instruction, like worked examples of \nproblem solutions, are as effective or more effective than more complex forms \n(e.g., Clark & Mayer, 2003).  \nCognitive Tutor research is actively advancing along a number of \ndimensions that build off of the prior work demonstrating the value of Cognitive \nTutors for delivering individual tutoring to aid the acquisition of cognitive skills.  \nA major current research topic is tutoring meta-cognitive skills in addition to \ncognitive skills, including self-explanation (Aleven & Koedinger, 2002), error \ndetection and correction (Mathan & Koedinger, 2003), and learning and help-\nseeking skills (Aleven, McLaren, Roll, & Koedinger,  2004).  Cognitive Tutors \nare also being deployed to support required state testing and school accountability \n(http://assistment.org) and authoring tools are being created to speed Cognitive \nTutor development (http://ctat.pact.cs.cmu.edu). Finally, Cognitive Tutors are"
  },
  {
    "page": 27,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 160 \nbeing employed as research platforms to allow rigorous experimental tests of \nlearning principles \u201cin vivo\u201d, that is, in classrooms with real students and real \ncourses. (http://learnlab.org). \nAcknowledgements \nThe National Science Foundation (NSF), Department of Education, and \nDARPA supported Algebra Cognitive Tutor research. Carnegie Learning, Inc \nsupported Cognitive Tutor Math 6. NSF\u2019s Science of Learning Center is \nsupporting the Pittsburgh Science of Learning Center and creation of LearnLab."
  },
  {
    "page": 28,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 161 \nReferences \nAleven, V., & Koedinger, K.R. (2002). An effective meta-cognitive \nstrategy: Learning by doing and explaining with a computer-based Cognitive \nTutor. Cognitive Science, 26, 147-179. \nAleven, V., McLaren, B., Roll, I., & Koedinger, K. R. (2004). Toward \ntutoring help seeking. In Lester, Vicari, & Parguacu (Eds.) Proceedings of the 7th \nInternational Conference on Intelligent Tutoring Systems, 227-239. Berlin: \nSpringer-Verlag \nAleven, V., Popescu, O. & Koedinger, K. R. (2003). A tutorial dialog \nsystem to support self-explanation: Evaluation and open questions. In U. Hoppe, \nF. Verdejo, & J. Kay (Eds.), Artificial Intelligence in Education: Shaping the \nFuture of Learning through Intelligent Technologies, Proceedings of AI-ED 2003 \n(pp. 39-46). Amsterdam, IOS Press. \nAnderson, J. R., Boyle, C. F., & Reiser, B. J. (1985).  Intelligent tutoring \nsystems.  Science, 228, 456-468. \nAnderson, J.R. & Jeffries, R. (1988). Novice LISP errors: Undetected \nlosses of information from working memory. Human-Computer Interaction, 1, \n107-131. \nAnderson, J. R., & Lebiere, C. (1998).  The Atomic Components of \nThought. Hillsdale, NJ: Erlbaum. \nBerry, D. C. & Dienes, Z. (1993).  Implicit Learning: Theoretical and \nEmpirical Issues.  Hove, UK: Erlbaum."
  },
  {
    "page": 29,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 162 \nBloom, B. S. (1984).  The 2 sigma problem: The search for methods of \ngroup instruction as effective as one-to-one tutoring.  Educational Researcher, \n13, 3-16. \nBransford, J. D., Brown, A. L, & Cocking, R. R. (1999).  How People \nLearn: Brain, Mind, Experience, and School.  Committee on Developments in the \nScience of Learning Commission on Behavioral and Social Sciences and \nEducation.  National Research Council.  Washington, D.C.: National Academy \nPress.  \nChi, M.T.H., Feltovich, P.J., & Glaser, R. (1981). Categorization and \nrepresentation of physics problems by experts and novices. Cognitive Science, 5, \n121-152. \nChi, M. T. H., de Leeuw, N., Chiu, M., & Lavancher, C. (1994). Eliciting \nself-explanations improves understanding. Cognitive Science, 18, 439\u2013477. \nClark, R. C., & Mayer, R. E. (2003). e-Learning and the Science of \nInstruction : Proven Guidelines for Consumers and Designers of Multimedia \nLearning. San Francisco: Jossey-Bass. \nCollins, A., Brown, J. S., & Newman, S.E. (1989).  Cognitive \napprenticeship: Teaching the crafts of reading, writing, and mathematics.  In L. B. \nResnick (Ed.), Knowing, learning, and instruction: Essays in honor of Robert \nGlaser (pp. 453-494).  Hillsdale, NJ: Lawrence Erlbaum Associates. \nCorbett, A.T. (2001). Cognitive computer tutors:  Solving the two-sigma \nproblem. User Modeling: Proceedings of the Eighth International Conference, \nUM 2001, 137-147."
  },
  {
    "page": 30,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 163 \nCorbett, A.T. & Anderson, J.R. (1995).  Knowledge tracing:  Modeling \nthe acquisition of procedural knowledge.  User modeling and user-adapted \ninteraction, 4, 253-278. \nCorbett, A.T. & Anderson, J.R. (2001).  Locus of feedback control in \ncomputer-based tutoring:  Impact on learning rate, achievement and attitudes. \nProceedings of ACM CHI 2001 Conference on Human Factors in Computing \nSystems, 245-252. \nCorbett, A. T., Koedinger, K. R., & Anderson, J. R. (1997).  Intelligent \ntutoring systems. In M. G. Helander, T. K. Landauer, & P. V. Prabhu, (Eds.), \nHandbook of human-computer interaction (pp. 849\u2013874).  Amsterdam: Elsevier. \nCorbett, A. T., Koedinger, K. R., & Hadley, W. H. (2001).  Cognitive \nTutors: From the research classroom to all classrooms.  In Goodman, P. S. (Ed.), \nTechnology-enhanced learning: Opportunities for change (pp. 235-263). \nMahwah, NJ: Erlbaum. \nCorbett, A.T., MacLaren, B., Kauffman, L., Wagner, A., Jones, E., & \nKoedinger, K.R. (2005). Evaluating a genetics cognitive tutor: Modeling and \nsupporting a pedigree analysis task. Carnegie Mellon University Technical \nReport. \nCorbett, A.T. & Trask, H.  (2000).  Instructional interventions in \ncomputer-based tutoring:  Differential impact on learning time and accuracy. \nProceedings of ACM CHI 2000 Conference on Human Factors in Computing \nSystems, 97-104."
  },
  {
    "page": 31,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 164 \nEberts, R. E. (1997).  Computer-based instruction.  In Helander, M. G., \nLandauer, T. K., & Prabhu, P. V. (Ed.s) Handbook of Human-Computer \nInteraction, (pp. 825-847).  Amsterdam, The Netherlands: Elsevier Science B. V. \nFox, B. (1991).  Cognitive and interactional aspects of correction in \ntutoring.  In P. Goodyear (Ed.)  Teaching knowledge and intelligent tutoring, 149-\n172.  Norwood, NJ:  Ablex Publishing. \nGriffin, S., Case, R., & Siegler, R. S. (1994). Rightstart: Providing the \ncentral conceptual prerequisites for first formal learning of arithmetic to students \nat risk for school failure. In K. McGilly (Ed.), Classroom Lessons (pp. 25-49). \nCambridge, MA: MIT Press. \nJordan, P. W.; Ros\u00e9, C. P.; and VanLehn, K (2001) Tools for Authoring \nTutorial Dialogue Knowledge. In J. D. Moore, C. L. Redfield, & W. L. Johnson \n(Eds.), Artificial Intelligence in Education: AI-ED in the Wired and Wireless \nFuture, Proceedings of AI-ED 2001. Amsterdam, IOS Press. \nKoedinger, K. R. (2002). Toward evidence for instructional design \nprinciples: Examples from Cognitive Tutor Math 6. In Proceedings of PME-NA \nXXXIII (The North American Chapter of the International Group for the \nPsychology of Mathematics Education). \nKoedinger, K.R. & Anderson, J.R. (1993). Effective use of intelligent \nsoftware in high school math classrooms. In P. Brna, S. Ohlsson and H. Pain \n(Eds.) Proceedings of AIED 93 World Conference on Artificial Intelligence in \nEducation, 241-248."
  },
  {
    "page": 32,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 165 \nKoedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997).  \nIntelligent tutoring goes to school in the big city.  International Journal of \nArtificial Intelligence in Education, 8, 30-43. \nKoedinger, K. R. & Nathan, M. J. (2004).  The real story behind story \nproblems: Effects of representations on quantitative reasoning.  The Journal of the \nLearning Sciences, 13 (2), 129-164. \nKulik, C. C. & Kulik, J. A. (1991). Effectiveness of Computer-Based \nInstruction: An Updated Analysis. Computers in Human Behavior, 7, 75-95. \nLave, J., Murtaugh, M., & de la Rocha, O. (1984).  The dialectic of \narithmetic in grocery shopping.  In B. Rogoff & J. Lave (Eds.), Everyday \nCognition (pp. 67-94). Cambridge, MA: Harvard University Press. \nLepper, M.R., Aspinwall, L., Mumme, D. & Chabay, R.W. (1990).  Self-\nperception and social perception processes in tutoring:  Subtle social control \nstrategies of expert tutors.  In J. Olson & M. Zanna (Eds.) Self inference \nprocesses:  The sixth Ontario symposium in social psychology (pp. 217-237).  \nHillsdale, NJ:  Lawrence Erlbaum Associates. \nLittman, D. (1991). Tutorial planning schemas. In P. Goodyear (Ed.) \nTeaching knowledge and intelligent tutoring, 107-122. Norwood, NJ: Ablex \nPublishing. \nMathan, S. & Koedinger, K. R. (2003). Recasting the feedback debate: \nBenefits of tutoring error detection and correction skills. In Hoppe, Verdejo, & \nKay (Eds.), Artificial Intelligence in Education, Proceedings of AI-ED 2003 (pp. \n13-18). Amsterdam, IOS Press."
  },
  {
    "page": 33,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 166 \nMatz, M. (1982).  Towards a process model for high school algebra errors.  \nIn D. Sleeman and J. S. Brown (Eds.)  Intelligent  tutoring systems (pp. 25-50).  \nNew York: Academic Press. \nMcArthur, D., Stasz, C., & Zmuidzinas, M. (1990). Tutoring techniques in \nalgebra. Cognition and Instruction, 7, 197-244. \nMerrill, D.C., Reiser, B.J., Ranney, M. & Trafton, G.J. (1992).  Effective \ntutoring techniques:  A comparison of human tutors and intelligent tutoring \nsystems.  The Journal of the Learning Sciences,  2, 277-305. \nMorgan, P., & Ritter, S. (2002). An experimental study of the effects of \nCognitive Tutor\u00ae Algebra I on student knowledge and attitude. Pittsburgh, PA: \nCarnegie Learning Inc. Retrieved April 14, 2005 from \nhttp://www.carnegielearning.com/wwc/originalstudy.pdf \nNathan, M. J. & Koedinger, K. R. (2000). An investigation of teachers' \nbeliefs of students' algebra development. Cognition and Instruction, 18(2), 207-\n235. \nNathan, M. J., Long, S. D., & Alibali, M. W. (2002). Symbol precedence \nin mathematics textbooks: A corpus analysis. Discourse Processes, 33, 1-21. \nNewell, A., & Simon, H. A. (1972).  Human Problem Solving.  \nEnglewood Cliffs, NJ: Prentice-Hall. \nPolya, G. (1957). How to Solve It:  A New Aspect of Mathematical \nMethod. (2nd ed.). Princeton, NJ: Princeton University Press. \nResnick, L. B. (1987). Learning in school and out. Educational \nResearcher, 16(9), 13-20."
  },
  {
    "page": 34,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 167 \nRitter, S. and Anderson, J. R. (1995). Calculation and strategy in the \nequation solving tutor. In J.D. Moore & J.F. Lehman (Eds.), Proceedings of the \nSeventeenth Annual Conference of the Cognitive Science Society.  (pp. 413-418). \nHillsdale, NJ: Erlbaum. \nScheines, R. & Seig, W. (1994). Computer environments for proof \nconstruction. Interactive Learning Environments, 4, 159-169. \nSingley, M.K. (1990). The reification of goal structures in a calculus tutor: \nEffects on problem solving performance. Interactive Learning Environments, 1, \n102-123. \nSingley, M. K., & Anderson, J. R. (1989). Transfer of Cognitive Skill. \nHillsdale, NJ: Erlbaum. \nSkinner, B. F. (1968). The technology of teaching. New York: Appleton-\nCentury-Crofts. \nSleeman, D. H., & Brown, J. S. (1982).  Intelligent Tutoring Systems.  \nNew York, NY: Academic Press. \nSweller, J. (1988). Cognitive load during problem solving: Effects on \nlearning. Cognitive Science, 12, 257-285. \nvan Merrienboer, J. J. G. (1997). Training complex cognitive skills: A \nFour-Component Instructional Design Model for Technical Training.  Englewood \nCliffs, NJ: Educational Technology Publications. \nVygotsky, L. S. (1978). Mind in Society. Cambridge, MA: Harvard \nUniversity Press."
  },
  {
    "page": 35,
    "text": "Chapter 5 \n \nCognitive tutors \n \n \nPage 168 \nWenger, E. (1987). Artificial Intelligence and Tutoring Systems: \nComputational and Cognitive Approaches to the Communication of Knowledge. \nLos Altos, CA: Morgan Kaufmann. \nWertheimer, R. (1990).  The geometry proof tutor: An \u201cintelligent\u201d \ncomputer-based tutor in the classroom.  Mathematics Teacher, 83(4), 308-317. \nWiemer-Hastings, P., Wiemer-Hastings, K., & Graesser, A. C. (1999). \nImproving an intelligent tutor\u2019s comprehension of students with latent semantic \nanalysis. In Lajoie, S. P. and Vivet, M. eds. Artificial Intelligence in Education, \nOpen Learning Environments: New Computational Technologies to Support \nLearning, Exploration, and Collaboration, Proceedings of AIED-99, 535-542. \nAmsterdam, Netherlands: IOS Press. \n                                                 \n1 In this chapter we will often refer to such performance situations as \u201cproblems\u201d or \n\u201cproblem-solving activities\u201d though we believe that many of the ideas about tutoring expressed in \nthis chapter are relevant to performances that are not usually described as problem solving, like \nwriting an essay, making a scientific discovery, communicating in a foreign language."
  },
  {
    "page": 36,
    "text": "Figures and Tables\nFigure 1. A screen shot of a problem solving activity within Cognitive Tutor\nAlgebra.  Students are presented a problem situation and use various tools, like the\nWorksheet, Grapher, and Solver shown here, to analyze and model the problem\nsituation.  As they work, \u201cmodel tracing\u201d is used to provide just-in-time feedback or\non-demand solution-sensitive hints through the Messages window.  The results of\n\u201cknowledge tracing\u201d are displayed in the skills chart in the top center."
  },
  {
    "page": 37,
    "text": "Figure 2. How model tracing uses production rules to trace different possible actions\nstudents may take. Here the student has reached the state \u201c3(2x + 5) = 9\u201d.  The \"?\"\nat the top indicates that these production rules work no matter how the student\nreached this state.  The figure shows how the production rules apply to generate\nthree possible next steps. Attached to the production rules are feedback messages\nfor common errors or next-step hints that students can request if they are stuck."
  },
  {
    "page": 38,
    "text": "Figure 3. A screen image of Cognitive Tutor Geometry with support for self-\nexplanation of solution steps."
  },
  {
    "page": 39,
    "text": "Table 1. Example Production Rules\nProduction Rules in English\nExample of its application\n1. Correct production possibly acquired implicitly\nIF the goal is to find the value of quantity Q\n  and Q divided by Num1 is Num2\nTHEN find Q by multiplying Num1 and Num2.\nTo solve \u201cYou have some money\nthat you divide evenly among 8\npeople and each gets 40\u201d find the\noriginal amount of money by\nmultiplying 8 and 40.\n2. Correct production that does heuristic planning\nIF the goal is to prove two triangles congruent\nand the triangles share a side\nTHEN check for other corresponding sides or angles that\nmay congruent.\nTry to prove triangles ABC and\nDBC are congruent by checking\nwhether any of the corresponding\nangles, like BCA and BCD, or any\nof the corresponding sides, like AB\nand DB, are congruent.\n3. Correct production for a non-traditional strategy\nIF\nthe goal is to solve an equation in X\nTHEN graph the left and right sides of the equation\nand find the intersection point(s).\nSolve equation sin x = x2 by\ngraphing both sin x and x2 and\nfinding where the lines cross.\n4. Correct but overly specific production\nIF \u201cax + bx\u201d appears in an expression and c = a + b\nTHEN replace it with \u201ccx\u201d\nWorks for \u201c2x + 3x\u201d\nbut not for \u201cx + 3x\u201d\n5. Incorrect, overly general production\nIF \u201cNum1 + Num2\u201d appears in an expression\nTHEN replace it with the sum\nLeads to order of operations error:\n\u201cx * 3 + 4\u201d  is rewritten as \u201cx * 7\u201d"
  },
  {
    "page": 40,
    "text": "Table 2. Six Instructional Design Principles for Cognitive Tutors\n1. Represent student competence as a production set\n2. Provide instruction in a problem-solving context\n3. Communicate the goal structure underlying the problem solving\n4. Promote a correct and general understanding of the problem-solving knowledge\n5. Minimize working memory load that is extraneous to learning\n6. Provide immediate feedback on errors relative to the model of desired\nperformance"
  },
  {
    "page": 41,
    "text": "Table 3. Comparisons of Situational and Abstract Problems in Five Content Areas\nDecimal place\nvalue\nDecimal arith\nFraction addition\nData interp-global\nSituation\nShow 5\ndifferent ways\nthat you can\ngive Ben $4.07.\n[A place value\ntable was\nprovided.]\nYou had $8.72.\nYour\ngrandmother\ngave you $25\nfor your\nbirthday.  How\nmuch money do\nyou have now?\nMrs. Jules bought\neach of her\nchildren a\nchocolate bar.\nJarren ate 1/4 of a\nchocolate bar and\nAlicia ate 1/5 of a\nchocolate bar.\nHow much of a\nchocolate bar did\nthey eat\naltogether?\n[2 scatterplots\ngiven]\nDo students sell\nmore boxes of\nCandy Bars or\nCookies as the\nmonths pass?\n% correct\n61%\n65%\n32%\n62%\nAbstract\nList 5 different\nways to show\nthe amount\n4.07. [Place\nvalue table\ngiven.]\nAdd: 8.72 + 25\nAdd:  1/4 + 1/5\n[Scatterplots given]\nAre there more\nMoops per Zog in\nthe Left graph or\nthe Right graph?\n% correct\n20%\n35%\n22%\n48%\nView publication stats"
  }
]