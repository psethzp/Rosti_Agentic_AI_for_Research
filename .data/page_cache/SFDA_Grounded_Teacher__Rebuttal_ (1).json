[
  {
    "page": 1,
    "text": "Context Aware Grounded Teacher for Source Free Object\nDetection\nTajamul Ashraf1,2*, Rajes Manna4:, Partha Sarathi Purkayastha3:,\nTavaheed Tariq4, Janibul Bashir4*\n1*Department of Computer Vision, MBZUAI, Masdar City, 11058, Abu Dhabi, UAE.\n2*School of Information Technology, IIT Delhi, Hauz Khas, 180037, New Delhi, India.\n3*Microsoft Research India, Bengaluru, 560001, Karnataka, India.\n4*Department of Information Technology, NIT Srinagar, Hazratbal, 190007, J&K, India.\n*Corresponding author(s). E-mail(s): tajamul.ashraf@mbzuai.ac.ae;\njanibbashir@nitsri.ac.in;\nContributing authors: rajes 2021bite063@nitsri.ac.in; t-ppurkayast@microsoft.com;\ntavaheed 2022bite008@nitsri.ac.in;\n:These authors contributed equally to this work.\nAbstract\nSource-free domain adaptation for object detection (SFOD) remains a challenging paradigm due to\nthe dual impact of context bias arising from class imbalance and inter-class correlations, and the\ninstability of teacher-student frameworks under noisy pseudo-labels associated with domain shifts. To\ntackle the problem of context bias and the significant performance drop of the student model in the\nSFOD setting, we introduce Grounded Teacher (GT), a bias-aware semi-supervised adaptation frame-\nwork that grounds the teacher model through relational and semantic regularization, as a standard\nframework. GT introduces a Relational Context Module (RCM) to explicitly model directional con-\nfusions between classes, maintaining an exponential moving average (EMA) estimate of cross-domain\ncontextual bias. Building upon this, a Semantic Augmentation (SA) strategy selectively augments\nminority and confusable classes through adaptive MixUp in both source-similar and source-dissimilar\ntarget regions, improving minority recall without overfitting dominant categories. To stabilize learn-\ning under biased pseudo-labels, we design a Semantic-Aware Loss (SAL) that applies diagonally\nnormalized weights, preventing gradient explosion while emphasizing minority\u2013majority corrections.\nAdditionally, a frozen Expert branch derived from large vision foundation models (LVFMs) serves\nas a supervisory reference during training, refining pseudo-label quality without adding inference\noverhead. GT\u2019s behavior-driven bias quantification makes it broadly applicable across domains with-\nout relying on dataset priors. Evaluations on standard SFOD benchmarks of Cityscapes\u2192Foggy\nand medical transfers (DDSMINBreast, RSNA) demonstrate consistent performance gains, achieving\nsignificant improvements in minority-class detection. Extensive ablations further validate the contri-\nbution of each component and the computational efficiency of GT. All relevant resources, including\npreprocessed data, trained model weights, and code, are publicly available at this link.\nKeywords: Generalization, Source-Free Domain Adaptation, Context Bias, Semi-Supervised Learning,\nLarge Vision Foundation Models, Object Detection\n1"
  },
  {
    "page": 2,
    "text": "1 Introduction\nUnsupervised Domain Adaptation (UDA) offers a\nstrategy for adapting object detectors to new\ndomains where labeled data is not available.\nMoreover many popular SFOD techniques uti-\nlize a self-supervised approach in a student-\nteacher (ST) framework. These methods boot-\nstrap by training on pseudo-labels generated by\na source-pretrained model [67, 123, 70, 21, 112].\nHowever, if the source data is biased, or the\ndomain shift between source and target domains\nis significant, there is noise in the pseudo-labels,\nwhich impacts the training of a student model\n[27]. Since the pseudo-label error is significant,\nthe, Exponential Moving Average (EMA) step,\nwhich updates the teacher model from the student\nmodel\u2019s weight, ends up corrupting the teacher\nmodel as well leading to a mode collapse [3]. This\nphenomenon of progressive degradation, especially\nin source-free setting where source data is not\navailable during adaptation, is visually evident in\nFigure 1, where traditional Mean Teacher frame-\nworks and SF-Student-Teacher frameworks suffer\nfrom declining detection quality over time, while\nour method remains robust over the epochs.\nPrior works have documented pseudo-label\nnoise and EMA instabilities in teacher\u2013student\nadaptation, including in SFOD baselines and vari-\nants inspired by Unbiased Teacher/Mean-Teacher\nfamilies. Our contribution is not simply the obser-\nvation of these issues, but a coherent, source-\nfree solution to address them. We introduce our\nGrounded Teacher (GT), specifically designed to\ntackle context imbalance and mode collapse in the\nSFOD setting, which comprises a behavioral confu-\nsion estimator (RCM) with dual use in data-space\naugmentation (SA) and loss-space reweighting\n(SAL), along with training-only LVFM Expert\nguidance to stabilize the teacher. Together, these\nmechanisms target context bias and teacher drift\nwithout any source images or inference-time cost.\nThe Relation-Contextual Module (RCM) explic-\nitly models class biases and inter-class dynamics\nwithin the training data. Leveraging prior insights,\nwe implement a semantic augmentation strategy\nthat enhances minority class representation by\nstrategically blending them with visually similar\nFig. 1: Illustrative effect of pseudo-label noise and\nEMA\ndrift\nin\nsource-free\nobject\ndetection.\nEach\nrow shows representative target-domain detections\n(Cityscapes\u2192Foggy) across training epochs for (a)\nMean Teacher (ST), (b) its Source-Free variant (SF-\nST), and (c) our Grounded Teacher (GT).\nConventional\nmean-teacher\nframeworks\nexperience\nprogressive drift as pseudo-labels introduce false pos-\nitives and obscure minority classes, causing insta-\nbility. GT mitigates this through its RCM, SA/SAL\nand Expert Supervision, which jointly manage class-\nconfusion dynamics, ensuring consistent, spatially\naccurate detections and stable performance across\nepochs under source-free conditions.\nmajority class instances, drawn from our Crop-\nbank repository[140]. This augmentation oper-\nates bidirectionally across domains, simultane-\nously addressing both class imbalance and domain\nshift. To further mitigate inter-class bias, we pro-\npose a Semantic-Aware Loss that dynamically\nprioritizes minority classes during training, partic-\nularly focusing on challenging cases where they are\nmost vulnerable to misclassification as majority\nclasses. Together, these components form a com-\nprehensive framework that holistically addresses\nrepresentation learning challenges in cross-domain\nscenarios.\nAnother key insight of this paper is that one\ncan use learnt representations from Vision Foun-\ndational Models to guide a student model in SFOD\nsettings. Due to the excellent zero shot capa-\nbilities of the Vision Foundation Models (VFMs),\nthey can effectively guide student models to learn\nthe right representation, and distances between\nthe samples, even when the target domain has a\nlarge shift from the source domain. This helps the\nmodel overcome mode collapse caused by biased\npseudo-labels during unsupervised training.\n2"
  },
  {
    "page": 3,
    "text": "Through the integration of these approaches,\nwe observe enhanced pseudo-label quality, leading\nto measurable gains on medical imaging bench-\nmarks achieving a new state-of-the-art (SOTA) of\n50.8 mAP in natural images for Cityscapes \u00d1\nFoggy Cityscapes, beating the previous standard\nof 45.0 mAP by +5.8 mAP (\u201e13% performance gain).\nWe summarize the contributions of this paper as\nfollows:\n\u2022 We highlight the problem of training insta-\nbility of a student model in SFOD setting due\nto incorrect pseudo labels from a source pre-\ntrained model in the presence of source bias,\nor large domain shift between source and\ntarget domains.\n\u2022 We propose our GT model, supported by our\nRCM, which can map the model\u2019s existing class\nbiases. To this end, we formulate a Seman-\ntic Aware Loss (SAL) to further enhance the\nperformance of biased classes.\n\u2022 We propose to leverage the zero-shot perfor-\nmance of VFMs to guide the student model in\na sample-specific manner and help learn the\ncorrect representation in a large domain shift\nscenario.\n2 Related Works\nSource-free object detection (SFOD) adapts a\ndetector to a target domain without accessing\nsource images. In practice, most methods build\nupon the mean-teacher family which involves\na student learning from pseudo-labels while a\nteacher is updated by EMA, where accuracy\nrises or drifts depending on pseudo-label qual-\nity and class imbalance. Our work follows this\nline but adds two design principles: (i) mea-\nsure the directed minority\u2192majority confusions\nas they actually occur during adaptation, and\n(ii) act on those measurements once in the data\nspace (semantic augmentation) and once in the\nloss space (bounded reweighting), while allowing\ntraining-only guidance from large vision founda-\ntion models (LVFMs).\n2.1 Relation-aware teachers and\nclass-bias remedies (DAOD vs.\nSFOD)\nA close line of work in domain-adaptive detection\n(DAOD) argues that inter-class relations can be\nexploited to counter class bias when both source\nand target images are available. CAT (Class-\nAware Teacher) argues that inter-class relations\ncan be exploited to counter class bias when both\nsource and target images are available. CAT learns\nan Inter-Class Relation module, uses those rela-\ntions to drive class-relation augmentation, and\nintroduces a class-relation loss. The approach\nis trained with source data and reported on\nCityscapes\u2192Foggy and related shifts, where the\nresults support the premise that explicitly model-\ning relations helps close minority-class gaps, but\nits mechanisms assume access to source supervi-\nsion during adaptation.\nBy contrast, IRG treats source-free detection\nand builds an Instance Relation Graph on tar-\nget proposals to guide a contrastive objective\ninside a mean-teacher loop. IRG\u2019s graph therefore\nis the supervisory signal that shapes representa-\ntions, without performing confusion-conditioned\naugmentation or class-wise bounded reweighting.\nOur method remains in the source-free regime\nlike IRG, but it differs in what is modeled (a\nbehavioral, directed confusion matrix rather than\na latent graph), how it is used (to steer MixUp\npairing and loss weights), and where cost is paid\n(LVFM guidance only at training time, no infer-\nence overhead).\n2.2 Contemporary SFOD under\nmean-teacher dynamics\nRecent\nSFOD\nwork\nsimplifies\nor\naugments\nthe idea of the mean-teacher framework. SF-\nUT (Source-Free Unbiased Teacher) streamlines\nteacher\u2013student training with strong/weak aug-\nmentation\nand\ncareful\npseudo-label\nselection,\nreporting robust baselines across urban bench-\nmarks. This highlights that much of SFOD\u2019s\ngain still depends on which pseudo-labels are\nto be trusted. LPLD (Low-confidence Pseudo-\nLabel Distillation) goes further by distilling low-\nconfidence pseudo-labels mined from RPN pro-\nposals, explicitly rescuing small or hard instances\nthat high-confidence thresholds miss. While doing\n3"
  },
  {
    "page": 4,
    "text": "so, it also uses class-relation cues when filter-\ning low-confidence candidates to help reduce\nnoise and improve performance. Orthogonal to\nboth, Dynamic Retraining-Updating (DRU) revis-\nits teacher\u2013student coordination by adaptively\nretraining and updating the network to mitigate\ndrift. It monitors student uncertainty through\ndecoder variance as it selectively reinitializes stag-\nnating decoder layers and dynamically tunes the\nteacher\u2019s EMA update rate, while using his-\ntorical loss regularization to stabilize learning.\nThese works point to three distinct aspects of\nfocus: pseudo-label selection, coverage of hard\ninstances, and teacher update policy. Grounded\nTeacher complements them with a fourth aspect:\nconfusion-conditioned data and loss interventions\nalong with training-only expert guidance, a com-\nbination aimed specifically at minority\u2192majority\ndrift while keeping deployment cost unchanged.\n2.3 Adjacent work in segmentation\nOutside detection, DRSL (Distribution Regular-\nized Self-Supervised Learning) regularizes pixel-\nlevel multi-modal class distributions for domain-\nadaptive segmentation and aligns source+target\nmodes to reduce pseudo-label noise. DRSL is\ninformative because it too addresses distribu-\ntional noise, but operates with a different objec-\ntive to minimze intra-class distribution mismatch\nbetween source and target domains at the pixel\nlevel by aligning multi-modal latent features\nfor each class, while preserving class consis-\ntency across domains. In contrast, GT\u2019s objective\noperates instance-wise and source-free, target-\ning inter-class confusion reduction rather than\ncross-domain distribution alignment. Our detector\nestimates class-wise confusion from target pseudo-\nlabels, route augmentation through variance-split\nCropbanks, and apply a diagonal-normalized,\nbounded loss that emphasizes minority\u2192majority\nconfusions without gradient blow-up. The two\nperspectives are thus complementary rather than\ninterchangeable across tasks.\n2.4 Rationale for GT\nGT draws on lessons from all the prior works\naddressed above but differs in execution. We mea-\nsure bias as a directed confusion process, use it\ntwice (augmentation and bounded reweighting),\nand utilize expertise from LVFMs only while train-\ning, so the deployed detector remains unchanged.\nThis places GT as a source-free, instance-level,\nconfusion-guided alternative that complements\npseudo-label selection and teacher-update sched-\nules, and empirically strengthens minority-class\nconsistency without inference-time cost. (Sections\n\u00a73\u2013\u00a74 develop these choices and quantify their\neffects.)\nOur setting inherits the Mean Teacher idea\nthat weight-averaged teachers provide stable tar-\ngets under consistency training, and the Unbi-\nased Teacher insight that pseudo-label bias must\nbe controlled in detection. GT stays within this\nparadigm but focuses primarily on measuring and\nregulating confusions during source-free adapta-\ntion, with LVFMs as expert supervisors rather\nthan as deployed detectors.\n3 Preliminaries\nProblem statement. Let X denotes the input\nspace and Ds \u201c tpxi\ns, yi\nsquNs\ni\u201c1, the source domain,\nwhere yi\ns \u201c pbi\ns, ci\nsq is a tuple comprising of ground\ntruth bounding box (bi\ns P R4) and the corre-\nsponding class label (ci\ns P R) of the object respec-\ntively. Here, xs P X denotes the image from the\nsource domain, and Ns denotes the total number\nof source images. The target domain Dt \u201c txi\ntuNt\ni\u201c1\nis unlabeled, where Nt denotes the total number of\ntarget images, and the target samples xi\nt P X. Our\nobjective is to transfer the source-trained model\nto the target domain without accessing samples\nfrom the source domain.\nMean\nTeacher\nFramework.\nThe\nself-\nsupervised adaptation strategy involves updating\nthe student model using unlabeled target data,\nleveraging\npseudo-labels\ngenerated\nfrom\nthe\nteacher model. Both the student and the teacher\nmodels\nare\ninitialized\nby\nthe\nsource-trained\nmodel. The pseudo labels undergo a confidence\nthreshold-based filtering process, and only the\nreliable ones are utilized to supervise the student\ntraining [62]. The pseudo-label supervision loss for\nthe object detection model can be expressed as:\n  \\m a th\ncal {L\n}_ \\te\nxt {s\ntu} = \\\nma thc\nal {L\n}_\\tex\nt {bo\nx}^S (x_t^i, yt_t^i) + \\mathcal {L}_\\text {giou}^S (x_t^i, yt_t^i) + \\mathcal {L}_\\text {cls}^S (x_t^i, yt_t^i). \\label {eq1} \n(1)\nHere yti\nt represents the pseudo-label for image i\nobtained after filtering low-confidence predictions\n4"
  },
  {
    "page": 5,
    "text": "from the teacher. The teacher is updated progres-\nsively through an Exponential Moving Average\n(EMA) of student weights. We implement a hard\nthreshold, \u03c4, on the classification scores generated\nby the teacher to ensure that only pseudo-labels\nwith high confidence are utilized by the stu-\ndent network, thereby encouraging more reliable\nlearning outcomes.\nThus, the overall self-training process for\nthe traditional student-teacher (ST) based object\ndetection framework is expressed as follows:\n  \\ Th e t a _s \n&\\l\ne\nfta\nrr o w \\ T he t a _s + \\gamma \\frac {\\partial L_{\\text {stu}}}{\\partial \\Theta _s}, \\tag {2} \\\\ \\Theta _t &\\leftarrow \\alpha \\Theta _t + (1 - \\alpha ) \\Theta _s. \\tag {3}\n(3)\nHere Lstu represents the student loss computed\nusing pseudo-labels from the teacher network,\nand \u0398s and \u0398t denote student and teacher net-\nwork, respectively. The parameters \u03b3 and \u03b1 denote\nthe student learning rate and teacher EMA rate,\nrespectively. Although the ST framework enables\nknowledge distillation using noisy pseudo-labels,\nit alone is insufficient for learning high-quality tar-\nget features in a source-free setting and leads to\nmodel degradation. Therefore, to enhance the fea-\ntures in the target domain, we introduce Visual\nFoundation Models (VFMs) as Expert and incor-\nporate supervised loss for knowledge transfer from\nexpert VFMs. Following [72], a discriminator is\nadded to encourage domain invariant feature rep-\nresentations with an associated loss, Ldis.\n4 Methodology\nOur proposed method, Grounded Teacher (GT),\nillustrated in Figure 3, extends the standard\nmean-teacher framework by introducing a novel\nRelation Contextual Module (RCM). At the heart of\nGT, the RCM is designed to systematically capture\nand quantify class-specific biases in the model.\nUnlike conventional methods that address class\nimbalance in a general way, RCM explicitly models\nthe semantic relationships between classes, with\na particular focus on minority classes that are\nfrequently misclassified as dominant ones.\nThis is achieved through the construction of\na batch-wise confusion matrix based on pseudo-\nlabels, which is normalized and continuously\naggregated into a global matrix. This evolving\nmatrix provides a robust and dynamic represen-\ntation of the model\u2019s bias landscape, forming the\nfoundation for two key components of our method:\nClass-Relation Augmentation and Semantic Loss.\nClass-Relation Augmentation targets image-level\nclass imbalance. Using the class relationships\nencoded by RCM, we identify minority classes\nin an image that share strong semantic overlap\nwith dominant classes. To increase their repre-\nsentation and improve discrimination, we apply\nMixUp [142]\u2014a technique known to be effec-\ntive in imbalanced scenarios [36, 20]\u2014to blend\ninstances of these semantically related classes.\nThis not only amplifies the presence of minor-\nity class samples but also encourages the model\nto learn finer distinctions between closely related\ncategories (Figure 3).\nIn parallel, the RCM guides the formulation of\nour Semantic-Aware Loss, a weighted loss func-\ntion that assigns greater importance to minority\nclasses\u2014especially those prone to misclassifica-\ntion. By amplifying the penalty for errors on these\nclasses, our approach effectively mitigates the\nmodel\u2019s inherent biases and promotes balanced,\nequitable learning across all categories.\n4.1 Relation Contextual module\nPrevious efforts in addressing class imbalance\nwithin domain adaptation settings [50, 56, 104]\nhave contributed significantly to improving per-\nformance on underrepresented classes. However,\na common limitation across these methods is\nthe neglect of inter-class relationships particularly\nhow semantic similarity between classes influences\nmisclassification patterns.\nOur empirical analysis reveals that misclassi-\nfications of minority classes often occur in favor\nof semantically similar majority classes. To cap-\nture and leverage these subtle relationships, we\nintroduce the Relation Contextual Module (RCM).\nUnlike general class imbalance metrics that can\nbe inferred from data distributions, inter-class\ndynamics must be learned from the model\u2019s\nbehavior during training. RCM facilitates this by\ngenerating a confusion matrix at each train-\ning batch, comparing the model\u2019s predictions\nwith ground-truth labels. This matrix is normal-\nized with respect to the ground truth, enabling\na dynamic estimation of misclassification biases\nbetween specific class pairs. To ensure stability\n5"
  },
  {
    "page": 6,
    "text": "Semantic\nAugmentation\nRelation Contextual\nModule (RCM)\nPseudo-Labels\nGRL\nTeacher\nDetector\nStudent\nDetector\nSource Dissimilar (Hard)\nDiscriminator\nEMA\nSource Similar (Easy)\nText Prompts\nTarget\u00a0\nDataset\nStrong\nAug.\nWeak\nAug.\nSource similar\nSource dissimilar\nInstance Data\nRCM\nLVFM Embedder\nWeights\nExpert Branch\nFig. 2: Detail architecture of (GT): The Relation-Contextual Module (RCM) computes a confusion matrix\nupdated via EMA, feeding both the augmentation policy (SA) and the reweighting module (SAL). Training employs\npseudo-labels from the teacher and optional Expert guidance. During inference, the Expert and RCM branches are\nremoved, yielding identical computational cost to the baseline detector.\nand robustness over time, we apply an Exponen-\ntial Moving Average (EMA) to iteratively update\na global confusion matrix. This evolving matrix\nserves as a reliable representation of the model\u2019s\nlong-term class bias patterns. Beyond providing\nsmoother updates, the use of EMA eliminates the\nrequirement for all classes to be present in every\nbatch\u2014thus streamlining the learning process.\nThe procedure for updating this matrix is detailed\nin Algorithm 1.\nWhen applied to target domain samples, RCM\nrelies on high-confidence pseudo-labels to estimate\nclass biases in the absence of ground-truth annota-\ntions, offering a reliable approximation that mir-\nrors the supervised setup on the source domain.\nAligning source and target domains is common\nin traditional domain adaptation tasks [58, 152,\n116]. While alignment can be performed in either\ndata space [12] or feature space [99], the absence of\nsource data presents a unique challenge for domain\nalignment.\nEven though the source data is inaccessible,\nthe source-pretrained model retains crucial knowl-\nedge about the source domain. Following [21],\nwe partition the target dataset into two groups,\nleveraging the pre-trained model to establish\nan explicit source-target domain distinction. To\nachieve this, a detection variance-based crite-\nrion is used, where variances are computed from\nthe pre-trained model\u2019s predictions on the target\nsamples. A higher variance suggests a stronger\nresemblance to the source domain [21]. In partic-\nular, the model exhibits greater uncertainty (hard\nsamples) when predicting source-similar images,\nleading to elevated variance values compared to\nsource-dissimilar images. The detection variance\nis determined using the following formulation:\n  v _i = \\math b b {E}[(F_{\\theta _s}(X_i) - \\mathbb {E}[F_{\\theta _s}(X_i)])^2], \n(2)\nwhere F\u03b8spXiq are the predictions of image Xi via\nthe source-pretrained model. Since this calculation\nis computationally intractable, we approximate it\nwith Monte-Carlo sampling using dropout, follow-\ning the method proposed by Gal and Ghahra-\nmani [35]. This approximation is achieved by con-\nducting M stochastic forward passes while keeping\nthe detection model unchanged [9].\nSince the outputs  F_{\\th e ta _ s}(X_i) = (\\mathbf {b_i}, \\mathbf {c_i}) consist\nof bounding box coordinates and class scores,\n6"
  },
  {
    "page": 7,
    "text": "the detection variance is defined as the prod-\nuct of the bounding box variance  v_{bi} and the\nclass score variance  v_{ci} . For a given predic-\ntion with  N_i bounding boxes and  K classes,\nwhere  \\ma t hbf\n {\\ {b\n_{i j}\n} =  (\nx_{ij}\n^1, y_{ij}^1, x_{ij}^2, y_{ij}^2)\\}_{j=1}^{N_i} and  \\ma t\nhbf\n {\\ {c\n_{i j } }  = \n(c _{ij\n}^1, c_{ij}^2, \\ldots , c_{ij}^K)\\}_{j=1}^{N_i} , we can express  v_{bi} and  v_{ci} as\nfollows:\n  v _\n{\nbi}\n =\n \n\\fr\na\nc\n {1\n}{M\n N _ i} \\sum _{j=1}^{N_i} \\sum _{m=1}^{M} \\| \\mathbf {b_{ij}^m} - \\mathbf {\\bar {b}_{ij}} \\|^2, \n(3)\n  v _\n{\nci}\n =\n \n\\fr\na\nc\n {1\n}{M\n N _ i} \\sum _{j=1}^{N_i} \\sum _{m=1}^{M} \\| \\mathbf {c_{ij}^m} - \\mathbf {\\bar {c}_{ij}} \\|^2, \n(4)\nwhere  \\\nmathbf {b_{ij}^m} and  \\\nmathbf {c_{ij}^m} represent the localization coor-\ndinates and classification scores of the  m -th for-\nward pass for the  j -th bounding box in  X_i , respec-\ntively, and  \\mathbf {\\bar {b}_{ij}} ,  \\mathbf {\\bar {c}_{ij}} denote their corresponding\naverage values over all  M forward passes.\nThe detection variance for an image  X_i is com-\nputed as  v _ i = v_{bi} v_{ci} . We then rank the images\nbased on their variances, where  r_i represents the\nrank of  X_i . The variance level  vl_i for the  i -th\nimage is given by  vl _\ni \n= \\frac {r_i}{N} . If  vl _ i \\geq \\sigma , we\ncategorize  X_i as source-similar; otherwise, it is\ndeemed source-dissimilar, with  \\ sig ma \\in (0, 1) being a\npredefined threshold. This process effectively par-\ntitions the target domain into source-similar and\nsource-dissimilar subsets for query-based adver-\nsarial alignment between domains.\nOversampling is a widely used strategy in\nclassification tasks to mitigate class imbalance\nby\nincreasing\nthe\nrepresentation\nof\nminority\nclasses. However, this method presents challenges\nin object detection, where individual images\noften contain multiple object classes. Our anal-\nysis of breast cancer detection datasets such as\nDDSM [66] reveals that most images do not con-\nsistently feature even a single instance of majority\nclasses, making conventional image-level resam-\npling approaches largely ineffective. This calls\nfor more targeted, instance-aware augmentation\ntechniques tailored to object detection.\nFollowing\nthe\ninstance-level\naugmentation\nstrategy proposed by Zhang et al. [140], we extract\nobject instances using bounding box annotations\nand strategically insert them into other images.\nTo guide this process, we leverage our Relation\nContextual Module (RCM) to identify majority\nand minority classes based on their likelihood of\ncorrect classification.\nWe compute the average diagonal value of\nthe RCM \u2014representing the mean self-classification\nprobability\u2014using the formula:\n  {\\rm  \\\nr\nc\nm\n }_\n{\\rm a vg} = \\frac {1}{C}\\sum ^C_{c=0}{\\rcm }(c,c), \n(5)\nwhere C is the number of classes. Classes with\nRCMpc, cq values above this mean are labeled as\nmajority classes, while those below are considered\nminority.\nUnlike prior methods that randomly overlay\ninstances, we employ a relation-guided MixUp\nstrategy. For each base instance, we select a\npaired instance through weighted random sam-\npling, where the weights are determined by inter-\nclass misclassification probabilities captured in the\nRCM. For majority base instances, we use the col-\numn vector RCMp:, cq, setting RCMpc, cq \u201c 0 to\nexclude self-augmentation and focus on classes\noften confused with the majority. For minor-\nity base instances, the row vector RCMpc, :q is\nused without masking, allowing for beneficial self-\naugmentation.\nPaired instances are resized to match the base\ninstance for bounding box consistency. MixUp is\nthen applied as:\n  \\ b e gin { a li g ne d } \\ha\nt { I }  &= \\ b et a  \\ c dot I_{\\rm base} + (1-\\beta ) \\cdot I_{\\rm mix},\\\\ \\hat {c} &= \\beta \\cdot c_{\\rm base} + (1-\\beta ) \\cdot c_{\\rm mix}, \\end {aligned} \n(6)\nwhere Ibase and Imix are the cropped images of the\nbase and mixed instances, and \u02c6I is the resulting\naugmented image. Correspondingly, cbase and cmix\ndenote the respective class vectors, with \u02c6c being\nthe interpolated class label.\nTo support domain adaptation, we apply this\naugmentation to both source similar and dissim-\nilar domains. For source-like images, we sample\ninstances from both domains to promote knowl-\nedge transfer. For source dissimilar images, we\nprioritize dissimilar domain instances, falling back\nto source similar instances only when necessary.\nImportantly, we refrain from augmenting minor-\nity base instances in the target domain to preserve\ntheir semantic integrity, helping the model main-\ntain focus on real target distributions and avoid\ndomain drift.\nTo enable inter-semantic Augmentation, we\nmaintain a dynamic repository of class-specific\n7"
  },
  {
    "page": 8,
    "text": "Algorithm\n1\nRelation\nContextual\nModule\n(RCM) Update Procedure\nRequire: Global\nclass-relation\nmatrix\nR\nP\nRC\u02c6C initialized to zero; EMA decay factor\n\u03b2 P r0, 1s.\n1: while training do\n2:\nInitialize batch-specific relation matrix Rb P\nRC\u02c6C to zero.\n3:\nfor each training example with ground-\ntruth label ci and predicted label xi do\n4:\nRbrci, xis \u00d0 Rbrci, xis ` 1\n5:\nend for\n6:\nfor each class c P t1, . . . , Cu do\n7:\nNormalize Rbrc, :s to sum to 1.\n8:\nUpdate Rrc, :s \u00d0 \u03b2\u00a8Rrc, :s`p1\u00b4\u03b2q\u00a8Rbrc, :\ns\n9:\nend for\n10: end while\ninstance crops, referred to as the Cropbank [140].\nThese instance crops are extracted from bound-\ning box annotations in labeled source images\nas well as pseudo-labeled target images. To\nfacilitate domain-specific augmentation, separate\nCropbanks are maintained for source similar\nand source dissimilar subsets. Each Cropbank is\nupdated in a first-in-first-out (FIFO) manner to\nensure sample diversity and freshness. This strat-\negy is particularly advantageous for the source dis-\nsimilar Cropbank, where early-stage pseudo-labels\nmay be less reliable; replacing them over time\nhelps improve augmentation quality as training\nprogresses.\nWe use the source-/target-similarity split only\nto route instance crops to the corresponding Crop-\nbank; the RCM iteslf remains a single global esti-\nmator shared across subsets. The same confusion\nstatistics condition the relation-guided augmen-\ntation (SA) and the Semantic-Aware Loss (SAL)\nacross both subsets.\n4.2 Semantic-Aware Loss\nTo further mitigate class bias, we introduce a\nweighted parameter into the classification loss,\nguided by the Relation Contextual Module (RCM)\nfor foreground classes. This weighting scheme\nemphasizes classes that are frequently misclassi-\nfied as dominant majority classes, thereby direct-\ning the model\u2019s attention toward improving per-\nformance on underrepresented or confused cate-\ngories.\nTo accentuate this focus, we apply a non-linear\ntransformation to the RCM values as follows:\n  w\n_i\n=  \\begin {cas\nes } \\ sq\nr\nt {1 - \\te\nxt {\\rcm } (\nc_i, x_i)}, & \\text {if } c_i = x_i \\\\ \\sqrt {\\frac {\\text {\\rcm }(c_i, x_i)}{\\text {\\rcm }(c_i, c_i)}}, & \\text {otherwise} \\end {cases} \\label {eq:icl_weight} \n(7)\nwhere wi denotes the weight for the ith instance,\nci is its pseudo-label, and xi is the predicted\nclass. When ci \u2030 xi, we normalize using the\ndiagonal entry RCMpci, ciq to scale the misclassi-\nfication likelihood relative to the class\u2019s overall\nperformance.\nBackground class weights are uniformly set to\n1 to prevent model bias against them. To reconcile\nthe disparity between foreground and background\nweights, we normalize foreground instance weights\nso that their mean equals that of the background\nclass:\n  W\n_f\n = \\frac {W_f}{\\text {mean}(W_f)}, \n(8)\nwhere Wf represents the set of weights for fore-\nground instances.\nTo avoid excessive influence from extreme\nweights, we apply a regularization term \u03bbl to all\nclass-relation weights:\n  W  = \n\\ f ra\nc {W + \\lambda _l}{1 + \\lambda _l}, \\label {eq:regularization} \n(9)\nensuring smoother gradients and preventing the\nloss from being overly sensitive to outlier weights.\nThe weighted classification loss is then defined\nas:\n  \\m a t\nh\nc\na\nl {\nL} _ {\\text  {cls}} = \\frac {1}{N} \\sum _{i=1}^{N} w_i \\cdot \\text {\\ce }(c_i, x_i), \\label {eq:semanticloss} \n(10)\nwhere N is the total number of instances, and\nCEp\u00a8q denotes the standard cross-entropy loss.\nBecause we regularize instance weights as per\nEq. (9), the post-regularization weights satisfy\n  \nW\n \\\ni n  \\\nl e\nf\nt [\\, \\f r a c { \\lambda _{\\ell }}{1 + \\lambda _{\\ell }} \\,,\\, 1 \\,\\right ] \\quad \\text {whenever } W \\in [0,1]. \nConsequently, the SAL Lcls is pointwise bounded,\npreventing gradient spikes from rare but extreme\n8"
  },
  {
    "page": 9,
    "text": "confusions. In ablations, removing this regularizer\nreduces performance casuing a notable drop(e.g.,\n50.8 \u00d1 47.8 mAP on Cityscapes\u00d1Foggy), indicat-\ning that boundedness is essential to avoid EMA-\namplified teacher drift (see \u00a74.2 and Appendix. A).\n4.3 Grounded Supervision\nWe introduce a framework that leverages Large\nVision Foundation Models (LVFMs) through expert\ndistillation, as depicted in 2. We introduce an\nExpert branch aimed at assimilating the strengths\nof\nLVFMs\ninto\na\nunified\nlearning\napproach.\nAs an initial assumption, we posit that the\nexpert model can effectively represent a wide\nvariety of images sourced from datasets like\nImageNet1K or ImageNet21k [26], LAION-400M\n[103],\nor\nDataComp1B\n[34].\nWe\nhave\nconsid-\nered many prominent families of teacher mod-\nels: CLIP [90], DINOV2 [87], ViT [32]and SAM\n[64], GroundingDINO [78], and BioMedParse [146].\nThese models have exhibited outstanding perfor-\nmance across a wide array of tasks, with CLIP\ndemonstrating broad proficiency, DINOV2 excelling\nin downstream dense tasks such as semantic seg-\nmentation under linear probe, and SAM partic-\nularly shining in open-vocabulary segmentation\ntasks. The newly introduced Expert branch incor-\nporates the pretrained LVFMs. In the context of\nknowledge distillation, utilizing a single expert\u2019s\nembedding directly for distilling knowledge to the\nstudent model may not suffice. We found that rely-\ning on an expert\u2019s embedding directly for distilling\nknowledge to the student model is insufficient for\neffective pseudo-label correction during adapta-\ntion. Hence, we propose using expert predictions\nand introducing a pseudo-supervised loss to guide\nthe student model out from the local minima.\nOur objective is to transfer the expertise of the\nexpert model to the student model. The expert\nmodel Ep\u00a8q is frozen, while the student model Sp\u00a8q\nis updated via gradient descent during adaptation.\nTo supervise the student using pseudo labels from\nthe expert, we employ a combination of a consis-\ntency loss (mean teacher loss) and a bounding box\nregression loss.\nLet xi be an input image. The expert produces\npseudo-labels in the form of bounding box predic-\ntions Ei\nbbox \u201c Epxiq, which are used to supervise\nthe student\u2019s prediction Si\nbbox \u201c Spxiq. We define\nthe loss as:\n  \\math c al { L }_{\\tex\nt {ex pe\nrt}} \n= \\ & \\ lambda \n_{\\te xt\n {cls}} \\cdot \\mathcal {L}_{\\text {cls}}(S_{\\text {bbox}}^i, E_{\\text {bbox}}^i) \\notag \\\\ & + \\lambda _{\\text {reg}} \\cdot \\mathcal {L}_{\\text {reg}}(S_{\\text {bbox}}^i, E_{\\text {bbox}}^i) \\label {eq_expert}\n(11)\nwhere Lcls is the classification loss (semantic aware\nloss), and Lreg is the bounding box regression loss\n(e.g., smooth L1 or GIoU loss). The weights \u03bbcls\nand \u03bbreg control the contribution of each term.\nZero-shot\nvs.\ntraining-time\nguidance.\nWe\nbenchmark the expert alone (frozen LVFM zero-\nshot detector), student\u2013teacher without Expert\n(pseudo-label self-training), and student\u2013teacher\nwith Expert (our default). Across both natural\nand medical domains, the Expert-only zero-shot\ndetector\nunderperforms\nsupervised/self-trained\ndetectors, while training-time Expert guidance\nconsistently\nimproves\nover\npseudo-label\nself-\ntraining by correcting class confusions and stabi-\nlizing teacher updates (Section \u00a74.2, Table A1).\nFor natural scenes, GroundingDINO + SAM\nyields the strongest guidance due to robust spatial\ngrounding, while for medical transfers, CLIP-ViT\nwith domain-tuned text prompts performs best.\nIn both cases we observe CLIP + DINOv2 ensem-\nbling offers small but consistent gains. All variants\nuse the same expert-supervision loss (Eq. (11))\nwithout extra hyperparameters or architectural\nchanges ((Table A2).\nThis unified loss ensures that the student\nmodel learns both semantic consistency and local-\nization accuracy from the expert model via\npseudo-supervision.\n4.4 Overall Objective and Training\nStrategy.\nThe final objective combines supervised, unsuper-\nvised, and discriminator losses:\n  \\mathc a l {L} =  \\lambda _u \\mathcal {L}_{\\text {stu}} + \\lambda _d \\mathcal {L}_{\\text {dis}} + \\mathcal {L}_{\\text {Expert}}, \n(12)\nwhere \u03bbu and \u03bbd control the contributions of\nthe unsupervised and discriminator losses, respec-\ntively, and Lstu, and LExpert have been formulated\nin Eq. (1), and (11) respectively. As the teaching\nprocess continues, an adequate amount of pseudo-\nboxes will be produced, and the influence of \u03bbu\nshould be lowered to avoid the encoder over-fitting\nto pseudo-labels. As a result, We decay \u03bbd as\nadaptation continues.\n9"
  },
  {
    "page": 10,
    "text": "Exp\nMethod\nVenue\nSF\nR@0.05\nR@0.3\nR@0.5\nR@1.0\nAUC\nF1-score\nUMT [137]\nCVPR\u201921\n\u2717\n0.01\n0.09\n0.15\n0.21\n0.204\n0.319\nSFA [114]\nMM\u201921\n\u2717\n0.03\n0.13\n0.23\n0.35\n0.241\n0.338\nH2FA [126]\nCVPR\u201922\n\u2717\n0.13\n0.32\n0.45\n0.52\n0.575\n0.312\nAQT [53]\nIJCAI\u201922\n\u2717\n0.11\n0.37\n0.44\n0.57\n0.680\n0.349\nAT [71]\nCVPR\u201922\n\u2717\n0.19\n0.38\n0.65\n0.75\n0.721\n0.512\nDDSM\nD-Adapt [55]\nICLR\u201922\n\u2717\n0.11\n0.29\n0.45\n0.59\n0.731\n0.414\nto\nHT [29]\nCVPR\u201923\n\u2717\n0.17\n0.49\n0.61\n0.69\n0.704\n0.362\nINBreast\nCLIPGAP [109]\nCVPR\u201923\n\u2717\n0.15\n0.55\n0.61\n0.75\n0.712\n0.445\nConfMIX [84]\nWACV\u201923\n\u2717\n0.19\n0.47\n0.58\n0.73\n0.737\n0.409\nMRT [148]\nICCV\u201923\n\u2717\n0.16\n0.54\n0.64\n0.72\n0.789\n0.489\nD-MASTER [6]\nMICCAI\u201924\n\u2717\n0.25\n0.61\n0.70\n0.82\n0.808\n0.524\nMexforer [114]\nMM\u201921\n\u2713\n0.02\n0.03\n0.03\n0.03\n0.060\n0.090\nIRG [27]\nCVPR\u201923\n\u2713\n0.05\n0.05\n0.07\n0.09\n0.110\n0.120\nGT (Ours)\n\u2713\n0.06\n0.45\n0.65\n0.92\n0.589\n0.758\nTable 1: Results of adaptation from DDSM to INBreast.\nThe Expert branch operates only during train-\ning to provide supervision and is removed during\ninference. So, the deployed detector is identical to\nthe student model, ensuring that inference com-\nplexity and computational cost remain the same as\nthe baseline configuration. Additional implemen-\ntation and runtime details are provided in \u00a74.3.\n5 Experiments\nWe conduct extensive experiments to assess the\neffectiveness of our approach across various chal-\nlenging medical imaging datasets, as well as stan-\ndard UDA and SFOD benchmarks. In the UDA set-\nting, both source and target domain data are\naccessible during training, facilitating adaptation.\nConversely, in the SFOD scenario, adaptation is\nperformed using only a source-trained model with-\nout access to source domain data. Additionally,\nwe perform ablation studies employing diverse\nexchange strategies to validate the efficacy of\nour proposed methods. We further analyze the\npromising results obtained by our method through\ncomprehensive visualizations and component-wise\nevaluations.\n5.1 Comparison with Current SOTA\nMethods\nWe evaluate the performance of our proposed\nGrounded Teacher (GT) method against other\napproaches on all three medical benchmarks and\nthe natural benchmark mentioned earlier for gen-\neralizability. Since UDA and SFDOAD share similar\ntask settings, we conducted comparisons with\nboth. Table 1, Table 2 and Table 3 present the\ncomparison results on medical image datasets.\nTable\n5\npresents\ncomparison\nof\nthe\nnatural\ndataset. Our proposed GT consistently outper-\nforms existing all SOTA methods, demonstrat-\ning generalizability and significant improvements\nacross both natural and medical settings.\nDDSM to INBreast. Adaptation from large to\nsmall-scale medical datasets with different modal-\nities. Here, we consider DDSM [66] dataset as the\nsource domain and INBreast [85] as the target\ndomain. Results are presented in Table 1. Our pro-\nposed method demonstrates superior performance\nacross various False positives per Image (FPI)\nvalues compared to existing methods as displayed.\nRSNA to INBreast. Adaptation across med-\nical datasets with different machine-acquisitions.\nThis is vital for enhancing healthcare outcomes,\nimproving diagnostic accuracy, and facilitating\nbetter clinical decisions. To evaluate our method\u2019s\nperformance,\nwe\nadapt\na\nmodel\ntrained\non\nDDSM [66] to RSNA [11]. Results for all FPI val-\nues are presented in Table 2, demonstrating that\nour method achieves state-of-the-art performance\non this benchmark.\nDDSM to RSNA. This experiment evaluates\ndomain adaptation across datasets collected from\ndistinct geographical regions. Specifically, we con-\nsider the DDSM [66] dataset as the source domain\nand RSNA [11] as the target domain. As shown in\n10"
  },
  {
    "page": 11,
    "text": "Exp\nMethod\nVenue\nSF\nR@0.05\nR@0.3\nR@0.5\nR@1.0\nAUC\nF1-score\nD-Adapt [55]\nICLR\u201921\n\u2717\n0.04\n0.12\n0.18\n0.29\n0.439\n0.263\nAT [73]\nCVPR\u201922\n\u2717\n0.16\n0.28\n0.35\n0.42\n0.486\n0.338\nDDSM\nH2FA [127]\nCVPR\u201922\n\u2717\n0.03\n0.13\n0.18\n0.36\n0.634\n0.236\nto\nMRT [147]\nICCV\u201923\n\u2717\n0.32\n0.52\n0.69\n0.72\n0.741\n0.352\nRSNA\nMexforer [114]\nMM\u201921\n\u2713\n0.24\n0.31\n0.39\n0.39\n0.336\n0.287\nIRG [27]\nCVPR\u201923\n\u2713\n0.16\n0.25\n0.37\n0.39\n0.308\n0.235\nGT (Ours)\n\u2713\n0.10\n0.43\n0.58\n0.91\n0.781\n0.530\nTable 2: Results of adaptation from DDSM to RSNA.\nExp\nMethod\nVenue\nSF\nR@0.05\nR@0.3\nR@0.5\nR@1.0\nAUC\nF1-score\nD-Adapt [55]\nICLR\u201921\n\u2717\n0.00\n0.06\n0.09\n0.10\n0.381\n0.362\nAT [73]\nCVPR\u201922\n\u2717\n0.01\n0.08\n0.10\n0.15\n0.385\n0.311\nRSNA\nH2FA [127]\nCVPR\u201922\n\u2717\n0.02\n0.08\n0.10\n0.12\n0.483\n0.315\nto\nMRT [147]\nICCV\u201923\n\u2717\n0.03\n0.09\n0.12\n0.17\n0.739\n0.587\nINBreast\nMexforer [114]\nMM\u201921\n\u2713\n0.02\n0.03\n0.03\n0.03\n0.060\n0.090\nIRG [27]\nCVPR\u201923\n\u2713\n0.05\n0.05\n0.07\n0.09\n0.110\n0.120\nGT (Ours)\n\u2713\n0.01\n0.28\n0.49\n0.90\n0.638\n0.605\nTable 3: Results of adaptation from RSNA to INBreast.\nTable 2, our proposed method consistently outper-\nforms existing approaches across all FPI thresh-\nolds, demonstrating its robustness and effective-\nness in cross-domain generalization.\nDDSM to DeepLesion (CT) We follow a\nsource-free protocol on DeepLesion by splitting\nscanner/protocols to induce shift and training\nuses the same hyperparameters as other medi-\ncal runs. We report AP50 and sensitivity at fixed\nFPPI. GT outperforms strong SFOD baselines\nby a meaningful margin while maintaining box-\nlevel consistency. Full protocol and curves are in\nAppx M (This complements mammography and\nCXR, demonstrating generalization across imag-\ning physics and anatomy).\nCityscapes\nto\nFoggy\nCityscapes.\nObject\ndetectors often experience a significant drop in\nperformance when deployed under adverse real-\nworld conditions such as fog, due to the domain\nshift caused by the lack of such conditions in\nthe training data. Domain adaptation aims to\nbridge this gap between normal and adverse\nweather scenarios. To investigate this, we con-\nduct experiments on the widely-used Cityscapes\n\u00d1 FoggyCityscapes benchmark. As presented in\nTable 5, student-teacher based frameworks consis-\ntently outperform non-student-teacher approaches\nby a notable margin.\nUnder identical VGG-16 backbones and stan-\ndard Cityscapes\u2192Foggy settings, our method\nGT reaches 50.8 mAP, outperforming contempo-\nrary SF-UT and LPLD baselines. Furthermore,\nit demonstrates notable gains in minority object\nclasses as rider improves by +6.3 AP50 and bicy-\ncle by +8.9 AP50 over the best prior baselines,\ntracking SAL\u2019s emphasis on minority\u2192majority\nconfusion and SA\u2019s context diversification. This\nhighlights\nits\nrobustness\nand\nadaptability\nin\nchallenging\nweather\nconditions,\nand\nsupports\nthat confusion-guided SA/SAL with training-only\nExpert guidance address minority-class instabil-\nity beyond what strong/weak augmentation and\nlow-confidence distillation alone provide.\n5.2 Compute Overhead\nAll experiments here were conducted on a single\nNVIDIA A100 (40 GB) GPU using identical data-\nloading and optimization settings as the baseline\nmean-teacher detector to ensure fair measure-\nment. Integrating RCM + SAL introduces only\n2\u20133 % additional training time and negligible\n11"
  },
  {
    "page": 12,
    "text": "Exp\nMethod\nVenue\nSF\nR@0.05\nR@0.3\nR@0.5\nR@1.0\nAUC\nF1-score\nD-Adapt [55]\nICLR\u201921\n\u2717\n0.04\n0.12\n0.18\n0.29\n0.439\n0.263\nAT [73]\nCVPR\u201922\n\u2717\n0.16\n0.28\n0.35\n0.42\n0.486\n0.338\nDDSM\nH2FA [127]\nCVPR\u201922\n\u2717\n0.03\n0.13\n0.18\n0.36\n0.634\n0.236\nto\nMRT [147]\nICCV\u201923\n\u2717\n0.32\n0.52\n0.69\n0.72\n0.741\n0.352\nDeepLesion\nMexforer [114]\nMM\u201921\n\u2713\n0.24\n0.31\n0.39\n0.39\n0.336\n0.287\nIRG [27]\nCVPR\u201923\n\u2713\n0.16\n0.25\n0.37\n0.39\n0.308\n0.235\nGT (Ours)\n\u2713\n0.10\n0.43\n0.58\n0.91\n0.781\n0.530\nTable 4: Results of adaptation from DDSM to DeepLesion.\nMethod\nVenue\nSF\nPerson\nRider\nCar\nTruck\nBus\nTrain\nMcycle\nBicycle\nmAP\nDA-Faster [18]\nCVPR\u201918\n\u2717\n29.2\n40.4\n43.4\n19.7\n38.3\n28.5\n23.7\n32.7\n32.0\nEPM [49]\nECCV\u201920\n\u2717\n44.2\n46.6\n58.5\n24.8\n45.2\n29.1\n28.6\n34.6\n39.0\nSSAL [86]\nNIPS\u201921\n\u2717\n45.1\n47.4\n59.4\n24.5\n50.0\n25.7\n26.0\n38.7\n39.6\nSFA [114]\nMM\u201921\n\u2717\n46.5\n48.6\n62.6\n25.1\n46.2\n29.4\n28.3\n44.0\n41.3\nUMT [28]\nCVPR\u201921\n\u2717\n33.0\n46.7\n48.6\n34.1\n56.5\n46.8\n30.4\n37.3\n41.7\nD-adapt [55]\nICLR\u201921\n\u2717\n40.8\n47.1\n57.5\n33.5\n46.9\n41.4\n33.6\n43.0\n43.0\nTIA [144]\nCVPR\u201922\n\u2717\n34.8\n46.3\n49.7\n31.1\n52.1\n48.6\n37.7\n38.1\n42.3\nPT [44]\nICML\u201922\n\u2717\n40.2\n48.8\n63.4\n30.7\n51.8\n30.6\n35.4\n44.5\n42.7\nMTTrans [136]\nECCV\u201922\n\u2717\n47.7\n49.9\n65.2\n25.8\n45.9\n33.8\n32.6\n46.5\n43.4\nSIGMA [68]\nCVPR\u201922\n\u2717\n44.0\n43.9\n60.3\n31.6\n50.4\n51.5\n31.7\n40.6\n44.2\nO2net [41]\nMM\u201922\n\u2717\n48.7\n51.5\n63.6\n31.1\n47.6\n47.8\n38.0\n45.9\n46.8\nAQT [53]\nIJCAI\u201922\n\u2717\n49.3\n52.3\n64.4\n27.7\n53.7\n46.5\n36.0\n46.4\n47.1\nAT [71]\nCVPR\u201922\n\u2717\n43.7\n54.1\n62.3\n31.9\n54.4\n49.3\n35.2\n47.9\n47.4\nTDD [45]\nCVPR\u201922\n\u2717\n50.7\n53.7\n68.2\n35.1\n53.0\n45.1\n38.9\n49.1\n49.2\nCIGAR [79]\nCVPR\u201923\n\u2717\n45.3\n45.3\n61.6\n32.1\n50.0\n51.0\n31.9\n40.4\n44.7\nCSDA [38]\nICCV\u201923\n\u2717\n46.6\n46.3\n63.1\n28.1\n56.3\n53.7\n33.1\n39.1\n45.8\nHT [30]\nCVPR\u201923\n\u2717\n52.1\n55.8\n67.5\n32.7\n55.9\n49.1\n40.1\n50.3\n50.4\nMRT [147]\nICCV\u201923\n\u2717\n52.8\n51.7\n68.7\n35.9\n58.1\n54.5\n41.0\n47.1\n51.2\nSFOD [69]\nAAAI\u201921\n\u2713\n21.7\n44.0\n40.4\n32.6\n11.8\n25.3\n34.5\n34.3\n30.6\nSFOD-Mosaic [69]\nAAAI\u201921\n\u2713\n25.5\n44.5\n40.7\n33.2\n22.2\n28.4\n34.1\n39.0\n33.5\nHCL [52]\nNIPS\u201921\n\u2713\n38.7\n46.0\n47.9\n33.0\n45.7\n38.9\n32.8\n34.9\n39.7\nSOAP [124]\nIJIS\u201921\n\u2713\n35.9\n45.0\n48.4\n23.9\n37.2\n24.3\n31.8\n37.9\n35.5\nLODS [67]\nCVPR\u201922\n\u2713\n34.0\n45.7\n48.8\n27.3\n39.7\n19.6\n33.2\n37.8\n35.8\nAASFOD [21]\nAAAI\u201923\n\u2713\n32.3\n44.1\n44.6\n28.1\n34.3\n29.0\n31.8\n38.9\n35.4\nIRG [110]\nCVPR\u201923\n\u2713\n37.4\n45.2\n51.9\n24.4\n39.6\n25.2\n31.5\n41.6\n37.1\nPETS [77]\nICCV\u201923\n\u2713\n46.1\n52.8\n63.4\n21.8\n46.7\n5.5\n37.4\n48.4\n40.3\nDACA [145]\nIJCV\u201924\n\u2713\n44.7\n31.2\n60.1\n53.1\n53.9\n0.0\n27.8\n45.9\n39.9\nLPLD [133]\nECCV\u201924\n\u2713\n39.7\n49.1\n56.6\n29.6\n46.3\n26.4\n36.1\n43.6\n40.9\nSF-UT [43]\nECCV\u201924\n\u2713\n40.9\n48.0\n58.9\n29.6\n51.9\n50.2\n36.2\n44.1\n45.0\nGT (Ours)\n\u2713\n42.7\n55.4\n61.7\n40.7\n62.0\n54.6\n39.1\n53.0\n50.8\nOracle\n\u2717\n66.3\n61.1\n80.8\n45.6\n68.8\n52.0\n49.1\n54.9\n59.8\nTable 5: Results of adaptation from Normal Cityscapes to Foggy weather Cityscapes (CF). \u201cSF\u201d refers to source-\nfree setting. \u201cOracle\u201d refers to the models trained by using labels during training.\nVRAM increase (\u00a1 0.3 GB). Enabling the training-\nonly Expert branch for extracting frozen LVFM\nfeatures adds 10\u201312 % training time and about 1\nGB of VRAM usage. Importantly, these modules\nare disabled during inference and the Inference\ncost remains unchanged as the deployed student\ndetector has the same architecture, latency, and\nFLOPs as the baselines. Additional details about\nprofiling logs, GPU-memory traces, and wall-clock\nanalyses are provided in Appendix C.\n12"
  },
  {
    "page": 13,
    "text": "Method\nRCM\nSA\nSAL\nmAP\nBase (AT [72])\n30.4\n+ SA\n\u2713\n\u2713\n41.8\n+ SAL\n\u2713\n\u2713\n44.2\nGT (Full)\n\u2713\n\u2713\n\u2713\n50.8\nTable 6: Ablation studies on GT components. RCM\nis included in all studies as it forms the basis of CRA\nand SAL. We report the mean average precision at 0.50\nIoU (mAP). Our contributions are not included in the\nbase framework (AT [71]).\n5.3 Ablation Studies\nTo verify the significance of our contributions,\nwe conducted an ablation study. All experi-\nments within this study were performed on the\nCityscapes \u00d1 Foggy Cityscapes benchmark using\nthe VGG16 backbone.\n5.3.1 Quantitative Ablation\nTable 6 presents a quantitative analysis of the\nimpact of each component within our framework.\nThe base framework prior to integrating our mod-\nules aligns with the AT model [71], achieving\na 30.4 mAP. Given that the Relation-Contextual\nModule (RCM) is integral to both class-relation\naugmentation and semantic-aware loss, it remains\na constant across all experimental variations. Inte-\ngrating Semantic Augmentation (SA) along with\nRCM increases the performance to 41.8 mAP, a\ngain of +11.4 over the base. Adding the Seman-\ntic Aware Loss (SAL) with RCM separately results\nin a higher mAP of 44.2, demonstrating a +13.8\nimprovement compared to the baseline, indicat-\ning that SAL provides a greater performance boost\nthan SA in this ablation setting. Notably, Class-\nRelation Augmentation (CRA) significantly reduces\nthe performance disparity between minority and\nmajority classes, as evidenced. Finally, our full\nGT model, which combines RCM, SA, and SAL,\nachieves the highest score of 50.8 mAP, which not\nonly boosts overall performance but also under-\nscores our method\u2019s efficacy in managing class\nimbalance.\n5.3.2 Impact of Augmentation\nIn our study, we assess the influence of augmen-\ntation strategies, focusing on both the proportion\nSelection Method\nmAP\nRandom (0.5)\n46.4\nSemantic Augmentation (1.0)\n47.5\nSemantic Augmentation (0.5)\n50.8\nTable 7: Comparison of selecting class\ninstances randomly and via CRA. Values\nin brackets refer to the likelihood of an\ninstance being augmented.\nof augmented data and the criteria for selecting\ninstances to augment. Image augmentation plays\na pivotal role in our methodology, enhancing the\ndataset by incorporating additional representa-\ntions, particularly of minority classes. However,\nexcessive augmentation can lead to the model\nlearning less accurate class representations. To\nmitigate this, we judiciously augment a random\nsubset of images.\nThe pairing of class instances during augmen-\ntation is crucial for improving its effectiveness.\nUnlike random pairing, our Semantic Augmenta-\ntion (SA) method prioritizes pairing instances from\nclosely related minority and majority classes. This\ntargeted approach ensures that the Mixup out-\nputs are more meaningful, thereby enhancing the\nperformance of minority classes.\nTable 7 presents the experimental results of\nthis approach. We compare the effects of randomly\nselecting class instances for Mixup against our SA\nmethod across different augmentation probabili-\nties, which represent the likelihood of an instance\nin the base image being augmented. Applying\nMixup randomly yields a modest improvement\nof +1.1 mAP in overall performance. In contrast,\nemploying SA leads to a more substantial increase\nof +4.4 mAP. These findings indicate that pair-\ning highly related classes during Mixup enhances\nminority class performance with minimal impact\non majority classes. Furthermore, our experiments\nreveal that excessive augmentation can nega-\ntively affect model performance, underscoring the\nimportance of a balanced augmentation strategy.\n5.3.3 Weighing Strategy for Semantic\nLoss\nTo enhance the performance of minority classes,\nwe introduce a Semantic Aware Loss (SAL) func-\ntion, as defined in Eq 10. Class-level loss functions\n13"
  },
  {
    "page": 14,
    "text": "Fig. 3: FROC analysis of model generalization on unseen target domains. Performance comparison\nbetween our method (GT) and the MT baseline for detection tasks under domain shift: (a) DDSM\u2192INBreast (410\nimages), (b) DDSM\u2192RSNA (993 images), and (c) RSNA\u2192INBreast (410 images), where our method demon-\nstrates superior performance across different domain shifts. Higher curves indicate better trade-offs between\nsensitivity (TPR) and false positives per image (FPI).\nFig. 4: Qualitative results of our proposed Grounded Teacher on Medical settings (DDSM \u00d1 INBreast). We\npresent visual comparisons between AT (top row) and GT (bottom row). Our method, GT, effectively identifies\nMalignancies better and mitigates false negatives. Bounding box colors indicate: Blue \u2014 Ground truth, Green \u2014\ntrue positive, and Red \u2014 false negative.\nare a prevalent strategy for addressing dataset\nclass imbalances. We compare our SAL with a\nvariant of existing class-level losses that utilize\nonly the diagonal elements of the Inter-Class\nRelation module, which correspond to the ground-\ntruth class likelihood for accurate classification.\nIn contrast, SAL leverages the likelihood of the\nground-truth being classified as the predicted class\nto influence its loss function.\nAs shown in Table 8, incorporating inter-\nclass relationships through SAL achieves an mAP\nof 50.8, a performance improvement of +3.7\nmAP compared to the base. While SAL effec-\ntively utilizes inter-class information, there are\ninstances where it might excessively penalize well-\nperforming classes without proper constraints. To\nmitigate this, we apply a regularization term, as\nspecified in Eq 9. Omitting this regularization\n(SAL w/o Reg.) results in an mAP of 47.8, demon-\nstrating a significant performance drop of -3.0 mAP\ncompared to the full SAL. This clearly highlights\nthe importance of the regularization term, likely\npreventing certain class weights from becoming\ndisproportionately small during training.\n5.3.4 Qualitative Results\nFigure 5 presents the qualitative outcomes of\nour method, comparing predictions from baseline\n14"
  },
  {
    "page": 15,
    "text": "Fig. 5: Qualitative results of our proposed Grounded Teacher on Natural settings (Cityscapes \u00d1 Foggy\nCityscapes). We present visual comparisons between AT (top row) and GT (bottom row). Our method, GT, effec-\ntively mitigates misclassifications, reduces false negatives, and eliminates false positives. Bounding box colors\nindicate: Green \u2014 true positive, Blue \u2014 misclassified, Red \u2014 false negative, and Pink \u2014 false positive.\nClass Weight Strategy\nmAP\nClass-Level\n47.1\nSAL w/o Reg.\n47.8\nSAL\n50.8\nTable 8: Class loss weighting\nstrategies. Class-Level uses only\nthe diagonal values in our RCM\nwith regularization. SAL refers\nto our full Semantic Aware Loss.\nAT [71] in the top row with those from the Ground\nTruth (GT) in the bottom row. The GT effectively\ncorrects misclassifications, as highlighted by the\nblue boxes. For instance, in columns 1, 2, and 4,\nthe GT accurately labels \u2019person\u2019, \u2019car\u2019, and \u2019truck\u2019,\nrespectively, rectifying the AT\u2019s errors. Moreover,\nthe GT demonstrates a notable reduction in both\nfalse positives and false negatives, indicated by the\npink and red boxes, respectively. This enhance-\nment signifies improved detection accuracy across\nvarious object scales and classes. This improve-\nment is particularly evident in column 3, where\nthe GT successfully identifies small-scale objects\nand provides more accurate detections for larger-\nscale objects. Also in Fig 5, we show how quali-\ntatively the predictions are improved with various\nproposed modules.\nReferences\n[1] Zeynettin Akkus, Alsu Galimzianova, Assaf\nHoogi, Daniel L. Rubin, and Bradley J.\nErickson.\nDeep learning for brain mri\nsegmentation: State of the art and future\ndirections.\nJournal of Digital Imaging,\n30(4):449\u2013459, 2017.\n[2] Yazeed Alashban.\nBreast cancer detec-\ntion and classification with digital breast\ntomosynthesis: a two-stage deep learning\napproach.\nDiagnostic and Interventional\nRadiology, 2024.\n[3] Munsif Ali, Leonardo Rossi, and Massimo\nBertozzi.\nCfts-gan: Continual few-shot\nteacher student for generative adversarial\nnetworks, 2024.\n[4] Vaswani Ashish. Attention is all you need.\nAdvances in neural information processing\nsystems, 30:I, 2017.\n[5] Tajamul Ashraf and Tisha Madame. Fed-\nerated learning framework for x-ray imag-\ning. In Artificial Intelligence and Imaging\nfor Diagnostic and Treatment Challenges in\nBreast Care: First Deep Breast Workshop,\nDeep-Breath 2024, Held in Conjunction with\nMICCAI 2024, Marrakesh, Morocco, Octo-\nber 10, 2024, Proceedings, page 13. Springer\nNature, 2024.\n[6] Tajamul\nAshraf,\nKrithika\nRangarajan,\nMohit Gambhir, Richa Gauba, and Chetan\nArora.\nD-master: Mask annealed trans-\nformer for unsupervised domain adaptation\nin breast cancer detection from mammo-\ngrams.\nIn International Conference on\nMedical Image Computing and Computer-\nAssisted\nIntervention,\npages\n189\u2013199.\nSpringer, 2024.\n[7] Hakan Bilen, Marco Pedersoli, and Tinne\nTuytelaars. Weakly supervised object detec-\ntion with convex clustering. In Proceedings\n15"
  },
  {
    "page": 16,
    "text": "Table 9: Ablation study isolating the contribution of Expert guidance. We compare a frozen LVFM\ndetector alone (Zero-shot Expert), self-training without Expert supervision (No-Expert), and the full GT with\nboth pseudo-labels and Expert guidance (PL+Expert). Results are AP50 for medical transfers and mAP for\nCityscapes\u00d1Foggy.\nMethod Variant\nDDSM\u00d1INBreast\nDDSM\u00d1RSNA\nRSNA\u00d1INBreast\nCityscapes\u00d1Foggy\nZero-shot Expert (frozen LVFM only)\n68.4\n70.2\n65.7\n42.1\nNo-Expert (PL only)\n74.6\n75.3\n71.9\n47.8\nGT (PL + Expert)\n78.9\n79.5\n76.1\n50.8\nTable 10: Ablation of different Large-Vision-Foundation-Model (LVFM) experts. Each expert super-\nvises the student during training using the same loss in Eq. (11). Best results for each task are highlighted in bold.\nExpert Type\nDDSM\u00d1INBreast\nDDSM\u00d1RSNA\nRSNA\u00d1INBreast\nCityscapes\u00d1Foggy\nCLIP-ViT (prompt-tuned)\n78.9\n79.5\n76.1\n49.2\nDINOv2 (self-sup.)\n77.3\n77.9\n74.6\n48.6\nGroundingDINO + SAM\n76.2\n76.7\n75.3\n50.8\nCLIP + DINOv2 (ensemble)\n78.1\n78.8\n75.9\n50.2\nTable 11: Evaluation on the DeepLesion dataset\nunder source-free setting. Source and target splits\nare formed by scanner/protocol differences. Metrics:\nAP50 and sensitivity at 4 false positives per image\n(FPPI).\nMethod\nAP50\nSensitivity@4FPPI\nSF-UT (ECCV\u201924)\n64.8\n75.2\nLPLD (ECCV\u201924)\n65.7\n76.5\nGT (ours)\n69.9\n80.3\nof the IEEE conference on computer vision\nand pattern recognition, pages 1081\u20131089,\n2015.\n[8] Hakan Bilen and Andrea Vedaldi. Weakly\nsupervised deep detection networks.\nIn\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n2846\u20132854, 2016.\n[9] Charles Blundell, Julien Cornebise, Koray\nKavukcuoglu, and Daan Wierstra. Weight\nuncertainty in neural networks. In ICML,\npage 1613\u20131622, 2015.\n[10] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kir-\nillov, and Sergey Zagoruyko.\nEnd-to-end\nobject detection with transformers. In Euro-\npean conference on computer vision, pages\n213\u2013229. Springer, 2020.\n[11] C. Carr, F. Kitamura, J. K-Cramer, J. Mon-\ngan, K. Andriole, M. V, M. Riopel, R. Ball,\nand S. Dane. Rsna screening mammography\nbreast cancer detection, 2022.\n[12] Chaoqi Chen, Zebiao Zheng, Xinghao Ding,\nYue Huang, and Qi Dou.\nHarmoniz-\ning transferability and discriminability for\nadapting object detectors. In CVPR, pages\n8866\u20138875, 2020.\n[13] Chaoqi Chen, Zebiao Zheng, Yue Huang,\nXinghao Ding, and Yizhou Yu.\nI3net:\nImplicit\ninstance-invariant\nnetwork\nfor\nadapting one-stage object detectors.\nIn\nProceedings of the IEEE/CVF conference\non computer vision and pattern recognition,\npages 12576\u201312585, 2021.\n[14] Qingchao Chen and Yang Liu.\nStructure-\naware\nfeature\nfusion\nfor\nunsupervised\ndomain adaptation.\nIn Proceedings of the\nAAAI Conference on Artificial Intelligence,\nvolume 34, pages 10567\u201310574, 2020.\n[15] Qingchao Chen, Yang Liu, Zhaowen Wang,\nIan Wassell, and Kevin Chetty. Re-weighted\nadversarial adaptation network for unsuper-\nvised domain adaptation. In Proceedings of\nthe IEEE conference on computer vision and\npattern recognition, pages 7976\u20137985, 2018.\n[16] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen,\nBo-Cheng Tsai, Yu-Chiang Frank Wang,\nand Min Sun.\nNo more discrimination:\n16"
  },
  {
    "page": 17,
    "text": "Cross city adaptation of road scene seg-\nmenters. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision,\npages 1992\u20132001, 2017.\n[17] Yuhua Chen, Wen Li, Christos Sakaridis,\nDengxin Dai, and Luc Van Gool. Domain\nadaptive faster r-cnn for object detection\nin the wild.\nIn Proceedings of the IEEE\nconference on computer vision and pattern\nrecognition, pages 3339\u20133348, 2018.\n[18] Yuhua Chen, Wen Li, Christos Sakaridis,\nDengxin Dai, and Luc Van Gool. Domain\nadaptive faster r-cnn for object detection in\nthe wild. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\n[19] Gong Cheng, Xiang Yuan, Xiwen Yao,\nKebing Yan, Qinghua Zeng, Xingxing Xie,\nand Junwei Han. Towards large-scale small\nobject detection: Survey and benchmarks.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\n[20] H. Chou, S. Chang, J. Pan, W. Wei, and\nD. Juan. Remix: rebalanced mixup. Com-\nputer Vision \u2013 ECCV 2020 Workshops,\npages 95\u2013110, 2020.\n[21] Qiaosong Chu, Shuyan Li, Guangyi Chen,\nKai Li, and Xiu Li. Adversarial alignment\nfor source free object detection.\nIn Pro-\nceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 37, pages 452\u2013460,\n2023.\n[22] Ramazan Gokberk Cinbis, Jakob Verbeek,\nand Cordelia Schmid.\nWeakly supervised\nobject localization with multi-fold multiple\ninstance learning.\nIEEE transactions on\npattern analysis and machine intelligence,\n39(1):189\u2013203, 2016.\n[23] Marius Cordts, Mohamed Omran, Sebastian\nRamos, Timo Rehfeld, Markus Enzweiler,\nRodrigo\nBenenson,\nUwe\nFranke,\nStefan\nRoth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene under-\nstanding.\nIn Proceedings of the IEEE\nconference on computer vision and pattern\nrecognition, pages 3213\u20133223, 2016.\n[24] Marius Cordts, Mohamed Omran, Sebastian\nRamos, Timo Rehfeld, Markus Enzweiler,\nRodrigo\nBenenson,\nUwe\nFranke,\nStefan\nRoth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene under-\nstanding.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition,\npages 3213\u20133223, 2016.\n[25] Zhigang Dai, Bolun Cai, Yugeng Lin, and\nJunying Chen. Up-detr: Unsupervised pre-\ntraining for object detection with trans-\nformers. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern\nrecognition, pages 1601\u20131610, 2021.\n[26] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In\n2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee,\n2009.\n[27] Jinhong Deng, Wen Li, Yuhua Chen, and\nLixin Duan.\nUnbiased mean teacher for\ncross-domain object detection. In Proceed-\nings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages\n4091\u20134101, 2021.\n[28] Jinhong Deng, Wen Li, Yuhua Chen, and\nLixin Duan.\nUnbiased mean teacher for\ncross-domain object detection.\nIn Pro-\nceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition\n(CVPR), pages 4091\u20134101, June 2021.\n[29] Jinhong Deng, Dongli Xu, Wen Li, and\nLixin Duan. Harmonious teacher for cross-\ndomain object detection.\nIn Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR),\npages 23829\u201323838, June 2023.\n[30] Jinhong Deng, Dongli Xu, Wen Li, and\nLixin Duan. Harmonious teacher for cross-\ndomain object detection.\nIn Proceedings\nof the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages\n23829\u201323838, 2023.\n[31] Jiahua Dong, Zhen Fang, Anjin Liu, Gan\nSun, and Tongliang Liu. Confident anchor-\ninduced multi-source free domain adapta-\ntion. Advances in Neural Information Pro-\ncessing Systems, 34:2848\u20132860, 2021.\n[32] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov,\nDirk\nWeissenborn,\nXiao-\nhua Zhai, Thomas Unterthiner, Mostafa\nDehghani,\nMatthias\nMinderer,\nGeorg\nHeigold, Sylvain Gelly, et al.\nAn image\nis worth 16x16 words: Transformers for\n17"
  },
  {
    "page": 18,
    "text": "image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[33] Mark Everingham, Luc Van Gool, Christo-\npher KI Williams, John Winn, and Andrew\nZisserman. The pascal visual object classes\n(voc) challenge.\nInternational journal of\ncomputer vision, 88:303\u2013338, 2010.\n[34] Samir Yitzhak Gadre, Gabriel Ilharco, Alex\nFang, Jonathan Hayase, Georgios Smyr-\nnis, Thao Nguyen, Ryan Marten, Mitchell\nWortsman, Dhruba Ghosh, Jieyu Zhang,\net al. Datacomp: In search of the next gen-\neration of multimodal datasets. Advances in\nNeural Information Processing Systems, 36,\n2024.\n[35] Yarin\nGal\nand\nZoubin\nGhahramani.\nDropout\nas\na\nbayesian\napproximation:\nRepresenting model uncertainty in deep\nlearning. In Maria-Florina Balcan and Kil-\nian Q. Weinberger, editors, ICML, pages\n1050\u20131059, 2016.\n[36] A. Galdr\u00b4an, G. Carneiro, and M. \u00b4A. G.\nBallester. Balanced-mixup for highly imbal-\nanced medical image classification. Medical\nImage Computing and Computer Assisted\nIntervention \u2013 MICCAI 2021, pages 323\u2013\n333, 2021.\n[37] Yaroslav Ganin, Evgeniya Ustinova, Hana\nAjakan, Pascal Germain, Hugo Larochelle,\nFran\u00b8cois Laviolette, Mario March, and Vic-\ntor Lempitsky. Domain-adversarial training\nof neural networks.\nJournal of machine\nlearning research, 17(59):1\u201335, 2016.\n[38] Changlong Gao, Chengxu Liu, Yujie Dun,\nand\nXueming\nQian.\nCsda:\nLearning\ncategory-scale\njoint\nfeature\nfor\ndomain\nadaptive object detection. In Proceedings of\nthe IEEE/CVF International Conference on\nComputer Vision, pages 11421\u201311430, 2023.\n[39] Andreas Geiger, Philip Lenz, Christoph\nStiller,\nand\nRaquel\nUrtasun.\nVision\nmeets robotics: The kitti dataset.\nThe\nInternational Journal of Robotics Research,\n32(11):1231\u20131237, 2013.\n[40] Ross Girshick.\nFast r-cnn.\nIn Proceed-\nings of the IEEE international conference\non computer vision, pages 1440\u20131448, 2015.\n[41] Kaixiong Gong, Shuang Li, Shugang Li, Rui\nZhang, Chi Harold Liu, and Qiang Chen.\nImproving transferability for domain adap-\ntive detection transformers. In Proceedings\nof the 30th ACM International Conference\non Multimedia, pages 1543\u20131551, 2022.\n[42] Cagri\nGungor\nand\nAdriana\nKovashka.\nBoosting weakly supervised object detection\nusing fusion and priors from hallucinated\ndepth.\nIn Proceedings of the IEEE/CVF\nWinter Conference on Applications of Com-\nputer Vision, pages 739\u2013748, 2024.\n[43] Yan Hao, Florent Forest, and Olga Fink.\nSimplifying source-free domain adaptation\nfor object detection: Effective self-training\nstrategies and performance insights.\nIn\nEuropean Conference on Computer Vision,\npages 196\u2013213. Springer, 2024.\n[44] Mengzhe He, Yali Wang, Jiaxi Wu, Yiru\nWang, Hanqing Li, Bo Li, Weihao Gan,\nWei Wu, and Yu Qiao.\nCross domain\nobject detection by target-perceived dual\nbranch\ndistillation.\nIn\nProceedings\nof\nthe IEEE/CVF Conference on Computer\nVision\nand\nPattern\nRecognition,\npages\n9570\u20139580, 2022.\n[45] Mengzhe He, Yali Wang, Jiaxi Wu, Yiru\nWang, Hanqing Li, Bo Li, Weihao Gan,\nWei Wu, and Yu Qiao.\nCross domain\nobject detection by target-perceived dual\nbranch\ndistillation.\nIn\nProceedings\nof\nthe IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR),\npages 9570\u20139580, June 2022.\n[46] Zhenwei\nHe\nand\nLei\nZhang.\nMulti-\nadversarial\nfaster-rcnn\nfor\nunrestricted\nobject detection.\nIn Proceedings of the\nIEEE/CVF\ninternational\nconference\non\ncomputer vision, pages 6668\u20136677, 2019.\n[47] Judy Hoffman, Eric Tzeng, Taesung Park,\nJun-Yan Zhu, Phillip Isola, Kate Saenko,\nAlexei Efros, and Trevor Darrell. Cycada:\nCycle-consistent adversarial domain adap-\ntation.\nIn International conference on\nmachine learning, pages 1989\u20131998. Pmlr,\n2018.\n[48] Judy Hoffman, Dequan Wang, Fisher Yu,\nand Trevor Darrell. Fcns in the wild: Pixel-\nlevel adversarial and constraint-based adap-\ntation.\narXiv preprint arXiv:1612.02649,\n2016.\n[49] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu\nLin, and Ming-Hsuan Yang.\nEvery pixel\nmatters: Center-aware feature alignment for\n18"
  },
  {
    "page": 19,
    "text": "domain adaptive object detector. In Com-\nputer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part IX 16, pages 733\u2013\n748. Springer, 2020.\n[50] Tzu Ming Harry Hsu, Wei Yu Chen, Cheng-\nAn Hou, Yao-Hung Hubert Tsai, Yi-Ren\nYeh, and Yu-Chiang Frank Wang. Unsuper-\nvised Domain Adaptation with Imbalanced\nCross-Domain Data. In IEEE/CVF Inter-\nnational Conference on Computer Vision,\npages 4121\u20134129, December 2015.\n[51] Jiaxing Huang, Dayan Guan, Aoran Xiao,\nand Shijian Lu.\nModel adaptation: His-\ntorical contrastive learning for unsupervised\ndomain adaptation without source data.\nAdvances in Neural Information Processing\nSystems, 34:3635\u20133649, 2021.\n[52] Jiaxing Huang, Dayan Guan, Aoran Xiao,\nand Shijian Lu.\nModel adaptation: His-\ntorical contrastive learning for unsupervised\ndomain adaptation without source data,\n2022.\n[53] Wei-Jie Huang, Yu-Lin Lu, Shih-Yao Lin,\nYusheng Xie, and Yen-Yu Lin. Aqt: Adver-\nsarial query transformers for domain adap-\ntive object detection.\nIn IJCAI, pages\n972\u2013979, 2022.\n[54] Naoto Inoue, Ryosuke Furuta, Toshihiko\nYamasaki, and Kiyoharu Aizawa.\nCross-\ndomain weakly-supervised object detection\nthrough progressive domain adaptation. In\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n5001\u20135009, 2018.\n[55] Junguang\nJiang,\nBaixu\nChen,\nJianmin\nWang, and Mingsheng Long.\nDecoupled\nadaptation for cross-domain object detec-\ntion, 2022.\n[56] Xiang Jiang, Qicheng Lao, Stan Matwin,\nand Mohammad Havaei.\nImplicit Class-\nConditioned Domain Alignment for Unsu-\npervised Domain Adaptation.\nIn Inter-\nnational Conference on Machine Learning,\npages 4816\u20134827, November 2020.\n[57] Mengmeng Jing, Xiantong Zhen, Jingjing\nLi, and Cees Snoek.\nVariational model\nperturbation for source-free domain adap-\ntation.\nAdvances in Neural Information\nProcessing Systems, 35:17173\u201317187, 2022.\n[58] Guoliang\nKang,\nLu\nJiang,\nYi\nYang,\nand\nAlexander\nG.\nHauptmann.\nCon-\ntrastive adaptation network for unsuper-\nvised domain adaptation. In CVPR, pages\n4893\u20134902, 2019.\n[59] S.R.\nKebede,\nF.G.\nWaldamichael,\nT.G.\nDebelee, and et al. Dual view deep learning\nfor enhanced breast cancer screening using\nmammography. Sci Rep, 14:3839, 2024.\n[60] Mikhail\nKennerley,\nJian-Gang\nWang,\nBharadwaj Veeravalli, and Robby T. Tan.\nCAT: Exploiting Inter-Class Dynamics for\nDomain Adaptive Object Detection, 2024.\n[61] A. Khalid, A. Mehmood, A. Alabrah, B. F.\nAlkhamees, F. Amin, H. AlSalman, and\nG. S. Choi. Breast cancer detection and pre-\nvention using machine learning. Diagnostics\n(Basel), 13(19):3113, Oct 2023.\n[62] Mehran Khodabandeh, Arash Vahdat, Mani\nRanjbar, and William G Macready.\nA\nrobust learning approach to domain adap-\ntive object detection.\nIn Proceedings of\nthe IEEE/CVF International Conference on\nComputer Vision, pages 480\u2013490, 2019.\n[63] Taekyung Kim, Minki Jeong, Seunghyeon\nKim, Seokeon Choi, and Changick Kim.\nDiversify and match: A domain adaptive\nrepresentation learning paradigm for object\ndetection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition, pages 12456\u201312465, 2019.\n[64] Alexander Kirillov, Eric Mintun, Nikhila\nRavi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead,\nAlexander C Berg, Wan-Yen Lo, et al.\nSegment anything.\nIn Proceedings of the\nIEEE/CVF International Conference on\nComputer Vision, pages 4015\u20134026, 2023.\n[65] Hannah\nKniesel,\nLeon\nSick,\nTristan\nPayer,\nTim\nBergner,\nKavitha\nShaga\nDevan,\nClarissa\nRead,\nPaul\nWalther,\nTimo\nRopinski,\nand\nPedro\nHermosilla.\nWeakly supervised virus capsid detection\nwith\nimage-level\nannotations\nin\nelec-\ntron microscopy images.\nIn The Twelfth\nInternational\nConference\non\nLearning\nRepresentations, 2023.\n[66] Rebecca Sawyer Lee, Francisco Gimenez,\nAssaf Hoogi, Kanae Kawai Miyake, Mia\nGorovoy, and Daniel L Rubin.\nA curated\n19"
  },
  {
    "page": 20,
    "text": "mammography data set for use in computer-\naided detection and diagnosis research. Sci-\nentific data, 4(1):1\u20139, 2017.\n[67] Shuaifeng Li, Mao Ye, Xiatian Zhu, Lihua\nZhou, and Lin Xiong.\nSource-free object\ndetection by learning to overlook domain\nstyle. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern\nRecognition, pages 8014\u20138023, 2022.\n[68] Wuyang Li, Xinyu Liu, and Yixuan Yuan.\nSigma: Semantic-complete graph matching\nfor domain adaptive object detection.\nIn\nProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recogni-\ntion (CVPR), pages 5291\u20135300, June 2022.\n[69] Xianfeng Li, Weijie Chen, Di Xie, Shicai\nYang, Peng Yuan, Shiliang Pu, and Yuet-\ning Zhuang. A free lunch for unsupervised\ndomain adaptive object detection without\nsource data, 2020.\n[70] Xianfeng Li, Weijie Chen, Di Xie, Shicai\nYang, Peng Yuan, Shiliang Pu, and Yuet-\ning Zhuang. A free lunch for unsupervised\ndomain adaptive object detection without\nsource data.\nIn Proceedings of the AAAI\nConference on Artificial Intelligence, vol-\nume 35, pages 8474\u20138481, 2021.\n[71] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma,\nYen-Cheng Liu, Kan Chen, Bichen Wu,\nZijian He, Kris Kitani, and Peter Vajda.\nCross-domain adaptive teacher for object\ndetection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition, pages 7581\u20137590, 2022.\n[72] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma,\nYen-Cheng Liu, Kan Chen, Bichen Wu,\nZijian He, Kris Kitani, and Peter Vajda.\nCross-domain adaptive teacher for object\ndetection.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition,\npages 7571\u20137580, 2022.\n[73] Yu-Jhe Li, Xiaoliang Dai, Chih-Yao Ma,\nYen-Cheng Liu, Kan Chen, Bichen Wu,\nZijian He, Kris Kitani, and Peter Vajda.\nCross-domain adaptive teacher for object\ndetection, 2022.\n[74] Jian Liang, Dapeng Hu, and Jiashi Feng. Do\nwe really need to access the source data?\nsource hypothesis transfer for unsupervised\ndomain adaptation. In International confer-\nence on machine learning, pages 6028\u20136039.\nPMLR, 2020.\n[75] Tsung-Yi Lin, Priya Goyal, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Focal loss for\ndense object detection. In Proceedings of the\nIEEE international conference on computer\nvision, pages 2980\u20132988, 2017.\n[76] Tsung-Yi\nLin,\nMichael\nMaire,\nSerge\nBelongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in\ncontext. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014.\n[77] Qipeng Liu, Luojun Lin, Zhifeng Shen,\nand Zhifeng Yang.\nPeriodically exchange\nteacher-student for source-free object detec-\ntion.\nIn Proceedings of the IEEE/CVF\nInternational\nConference\non\nComputer\nVision, pages 6414\u20136424, 2023.\n[78] Shilong Liu, Zhaoyang Zeng, Tianhe Ren,\nFeng Li, Hao Zhang, Jie Yang, Qing Jiang,\nChunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, and Lei Zhang. Grounding dino: Mar-\nrying dino with grounded pre-training for\nopen-set object detection, 2024.\n[79] Yabo Liu, Jinghua Wang, Chao Huang,\nYaowei\nWang,\nand\nYong\nXu.\nCigar:\nCross-modality graph reasoning for domain\nadaptive object detection.\nIn Proceedings\nof the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages\n23776\u201323786, 2023.\n[80] Yuang Liu, Wei Zhang, and Jun Wang.\nSource-free domain adaptation for seman-\ntic\nsegmentation.\nIn\nProceedings\nof\nthe IEEE/CVF Conference on Computer\nVision\nand\nPattern\nRecognition,\npages\n1215\u20131224, 2021.\n[81] Shao-Yuan Lo, Wei Wang, Jim Thomas,\nJingjing Zheng, Vishal M Patel, and Cheng-\nHao Kuo. Learning feature decomposition\nfor domain adaptive monocular depth esti-\nmation.\nIn 2022 IEEE/RSJ International\nConference on Intelligent Robots and Sys-\ntems (IROS), pages 8376\u20138382. IEEE, 2022.\n[82] Xin Luo, Wei Chen, Zhengfa Liang, Longqi\nYang, Siwei Wang, and Chen Li.\nCrots:\nCross-domain teacher\u2013student learning for\n20"
  },
  {
    "page": 21,
    "text": "source-free domain adaptive semantic seg-\nmentation. International Journal of Com-\nputer Vision, 132(1):20\u201339, 2024.\n[83] Chuofan Ma, Yi Jiang, Xin Wen, Zehuan\nYuan,\nand\nXiaojuan\nQi.\nCodet:\nCo-\noccurrence guided region-word alignment\nfor\nopen-vocabulary\nobject\ndetection.\nAdvances in Neural Information Processing\nSystems, 36, 2024.\n[84] Giulio Mattolin, Luca Zanella, Elisa Ricci,\nand Yiming Wang. Confmix: Unsupervised\ndomain adaptation for object detection via\nconfidence-based mixing.\nIn Proceedings\nof the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages\n423\u2013433, 2023.\n[85] In\u02c6es\nC\nMoreira,\nIgor\nAmaral,\nIn\u02c6es\nDomingues, Ant\u00b4onio Cardoso, Maria Joao\nCardoso, and Jaime S Cardoso.\nInbreast:\ntoward\na\nfull-field\ndigital\nmammo-\ngraphic database.\nAcademic radiology,\n19(2):236\u2013248, 2012.\n[86] Muhammad\nAkhtar\nMunir,\nMuham-\nmad Haris Khan, M Sarfraz, and Mohsen\nAli. Ssal: Synergizing between self-training\nand\nadversarial\nlearning\nfor\ndomain\nadaptive object detection.\nAdvances in\nNeural\nInformation\nProcessing\nSystems,\n34:22770\u201322782, 2021.\n[87] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo\nMoutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haz-\niza, Francisco Massa, Alaaeldin El-Nouby,\net al. Dinov2: Learning robust visual fea-\ntures without supervision.\narXiv preprint\narXiv:2304.07193, 2023.\n[88] Poojan\nOza,\nVishwanath\nA\nSindagi,\nVibashan\nVishnukumar\nSharmini,\nand\nVishal M Patel.\nUnsupervised domain\nadaptation of object detectors: A survey.\nIEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023.\n[89] Oscar Pina and Ver\u00b4onica Vilaplana. Unsu-\npervised domain adaptation for multi-stain\ncell detection in breast cancer with trans-\nformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition (CVPR) Workshops, pages\n5066\u20135074, 2024.\n[90] Alec Radford, Jong Wook Kim, Chris Hal-\nlacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural lan-\nguage supervision. In International confer-\nence on machine learning, pages 8748\u20138763.\nPMLR, 2021.\n[91] Krithika\nRangarajan,\nPranjal\nAggarwal,\nDhruv Kumar Gupta, Rohan Dhanakshirur,\nAkhil Baby, Chandan Pal, Arun Kumar\nGupta, Smriti Hari, Subhashis Banerjee,\nand Chetan Arora.\nDeep learning for\ndetection of iso-dense, obscure masses in\nmammographically dense breasts. European\nradiology, 2023.\n[92] Joseph Redmon, Santosh Divvala, Ross Gir-\nshick, and Ali Farhadi.\nYou only look\nonce: Unified, real-time object detection. In\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages\n779\u2013788, 2016.\n[93] Joseph Redmon and Ali Farhadi. Yolo9000:\nbetter, faster, stronger.\nIn Proceedings of\nthe IEEE conference on computer vision and\npattern recognition, pages 7263\u20137271, 2017.\n[94] Shaoqing Ren, Kaiming He, Ross Girshick,\nand Jian Sun. Faster r-cnn: Towards real-\ntime object detection with region proposal\nnetworks. Advances in neural information\nprocessing systems, 28, 2015.\n[95] Shaoqing Ren, Kaiming He, Ross Girshick,\nand Jian Sun. Faster r-cnn: Towards real-\ntime object detection with region proposal\nnetworks. In Advances in Neural Informa-\ntion Processing Systems, page 91\u201399, 2015.\n[96] Farzaneh Rezaeianaran, Rakshith Shetty,\nRahaf Aljundi, Daniel Olmeda Reino, Shan-\nshan Zhang, and Bernt Schiele.\nSeek-\ning similarities over differences: Similarity-\nbased\ndomain\nalignment\nfor\nadaptive\nobject detection.\nIn Proceedings of the\nIEEE/CVF International Conference on\nComputer Vision, pages 9204\u20139213, 2021.\n[97] F. Ryan, K. L. Rom\u00b4an, B. Z. Gerbol\u00b4es,\nK. M. Rebescher, M. S. Txurio, R. C.\nUgarte, M. J. G. Gonz\u00b4alez, and I. M. Oliver.\nUnsupervised domain adaptation for the\nsegmentation of breast tissue in mammogra-\nphy images, 2021. Computer Methods and\nPrograms in Biomedicine, 211, 106368.\n21"
  },
  {
    "page": 22,
    "text": "[98] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya\nHarada, and Kate Saenko.\nStrong-weak\ndistribution alignment for adaptive object\ndetection. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern\nrecognition, pages 6956\u20136965, 2019.\n[99] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya\nHarada, and Kate Saenko.\nStrong-weak\ndistribution alignment for adaptive object\ndetection. In CVPR, pages 6956\u20136965, 2019.\n[100] Kuniaki Saito, Kohei Watanabe, Yoshi-\ntaka Ushiku, and Tatsuya Harada.\nMaxi-\nmum classifier discrepancy for unsupervised\ndomain adaptation.\nIn Proceedings of the\nIEEE conference on computer vision and\npattern recognition, pages 3723\u20133732, 2018.\n[101] Christos Sakaridis, Dengxin Dai, and Luc\nVan Gool.\nSemantic foggy scene under-\nstanding with synthetic data. International\nJournal of Computer Vision, 126(9):973\u2013\n992, Sep 2018.\n[102] Christos Sakaridis, Dengxin Dai, and Luc\nVan Gool.\nSemantic foggy scene under-\nstanding with synthetic data. International\nJournal of Computer Vision, 126:973\u2013992,\n2018.\n[103] Christoph\nSchuhmann,\nRichard\nVencu,\nRomain\nBeaumont,\nRobert\nKaczmar-\nczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komat-\nsuzaki.\nLaion-400m:\nOpen\ndataset\nof\nclip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021.\n[104] Korawat Tanwisuth, Xinjie Fan, Huangjie\nZheng,\nShujian\nZhang,\nHao\nZhang,\nBo\nChen,\nand\nMingyuan\nZhou.\nA\nPrototype-Oriented Framework for Unsu-\npervised Domain Adaptation. In Advances\nin Neural Information Processing Systems,\nvolume 34, pages 17194\u201317208, 2021.\n[105] Antti Tarvainen and Harri Valpola. Mean\nteachers are better role models: Weight-\naveraged consistency targets improve semi-\nsupervised\ndeep\nlearning\nresults.\nIn\nAdvances in Neural Information Processing\nSystems, page 1195\u20131204, 2017.\n[106] Zhi Tian, Xiangxiang Chu, Xiaoming Wang,\nXiaolin Wei, and Chunhua Shen. Fully con-\nvolutional one-stage 3d object detection on\nlidar range images.\nAdvances in Neural\nInformation Processing Systems, 35:34899\u2013\n34911, 2022.\n[107] Eric Tzeng, Judy Hoffman, Kate Saenko,\nand Trevor Darrell. Adversarial discrimina-\ntive domain adaptation. In Proceedings of\nthe IEEE conference on computer vision and\npattern recognition, pages 7167\u20137176, 2017.\n[108] Arina Varlamova, Valery Belotsky, Grig-\nory Novikov, Anton Konushin, and Evgeny\nSidorov. Features fusion for dual-view mam-\nmography mass detection. arXiv preprint,\n2024.\n[109] Vidit Vidit, Martin Engilberge, and Math-\nieu Salzmann. Clip the gap: A single domain\ngeneralization approach for object detec-\ntion.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern\nrecognition, pages 3219\u20133229, 2023.\n[110] Vibashan VS, Poojan Oza, and Vishal M\nPatel.\nInstance relation graph guided\nsource-free domain adaptive object detec-\ntion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern\nRecognition, pages 3520\u20133530, 2023.\n[111] Vibashan VS, Poojan Oza, and Vishal M\nPatel.\nTowards online domain adaptive\nobject detection.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 478\u2013488,\n2023.\n[112] Vibashan Vs, Poojan Oza, Vishwanath A\nSindagi, and Vishal M Patel.\nMixture of\nteacher experts for source-free domain adap-\ntive object detection. In 2022 IEEE Inter-\nnational Conference on Image Processing\n(ICIP), pages 3606\u20133610. IEEE, 2022.\n[113] Jian Wang, Liang Qiao, Shichong Zhou,\nJin Zhou, Jun Wang, Juncheng Li, Shihui\nYing, Cai Chang, and Jun Shi.\nWeakly\nsupervised lesion detection and diagnosis\nfor breast cancers with partially annotated\nultrasound images. IEEE Transactions on\nMedical Imaging, 2024.\n[114] Wen Wang, Yang Cao, Jing Zhang, Fengxi-\nang He, Zheng-Jun Zha, Yonggang Wen, and\nDacheng Tao.\nExploring sequence feature\nalignment for domain adaptive detection\ntransformers.\nIn Proceedings of the 29th\nACM International Conference on Multime-\ndia, pages 1730\u20131738, 2021.\n22"
  },
  {
    "page": 23,
    "text": "[115] Xinggang\nWang,\nKaibing\nChen,\nZilong\nHuang, Cong Yao, and Wenyu Liu. Point\nlinking network for object detection. arXiv\npreprint arXiv:1706.03646, 2017.\n[116] Xudong Wang, Zhaowei Cai, Dashan Gao,\nand Nuno Vasconcelos. Towards universal\nobject detection by domain attention.\nIn\nCVPR, pages 7289\u20137298, 2019.\n[117] Y. Wang and Y. Yao.\nBreast lesion\ndetection\nusing\nan\nanchor-free\nnetwork\nfrom ultrasound images with segmentation-\nbased enhancement.\nScientific Reports,\n12(1):14720, 2022.\n[118] Yu Wang and Yudong Yao.\nBreast lesion\ndetection using an anchor-free network from\nultrasound images with segmentation-based\nenhancement, December 2022.\nPublisher\nCopyright: \u00a9 2022, The Author(s).\n[119] Yuting\nWang,\nVelibor\nIlic,\nJiatong\nLi,\nBranislav Kisa\u02c7canin, and Vladimir Pavlovic.\nAlwod:\nActive\nlearning\nfor\nweakly-\nsupervised object detection. In Proceedings\nof the IEEE/CVF International Conference\non\nComputer\nVision,\npages\n6459\u20136469,\n2023.\n[120] Zhenbin\nWang,\nMao\nYe,\nXiatian\nZhu,\nLiuhan Peng, Liang Tian, and Yingying\nZhu.\nMetateacher: Coordinating multi-\nmodel domain adaptation for medical image\nclassification. Advances in Neural Informa-\ntion Processing Systems, 35:20823\u201320837,\n2022.\n[121] Yuxin Wu, Alexander Kirillov, Francisco\nMassa, Wan-Yen Lo, and Ross Girshick.\nDetectron2, 2019.\n[122] Haifeng Xia, Handong Zhao, and Zhengming\nDing.\nAdaptive adversarial network for\nsource-free domain adaptation. In Proceed-\nings of the IEEE/CVF international confer-\nence on computer vision, pages 9010\u20139019,\n2021.\n[123] Lin Xiong, Mao Ye, Dan Zhang, Yan Gan,\nXue Li, and Yingying Zhu.\nSource data-\nfree domain adaptation of object detec-\ntor through domain-specific perturbation.\nInternational Journal of Intelligent Sys-\ntems, 36(8):3746\u20133766, 2021.\n[124] Lin Xiong, Mao Ye, Dan Zhang, Yan Gan,\nXue Li, and Yingying Zhu.\nSource data-\nfree domain adaptation of object detec-\ntor through domain-specific perturbation.\nInternational Journal of Intelligent Sys-\ntems, 36, 05 2021.\n[125] Minghao Xu, Hang Wang, Bingbing Ni,\nQi Tian, and Wenjun Zhang.\nCross-\ndomain detection via graph-induced pro-\ntotype\nalignment.\nIn\nProceedings\nof\nthe IEEE/CVF Conference on Computer\nVision\nand\nPattern\nRecognition,\npages\n12355\u201312364, 2020.\n[126] Yunqiu Xu, Yifan Sun, Zongxin Yang, Jiaxu\nMiao, and Yi Yang. H2FA R-CNN: Holistic\nand hierarchical feature alignment for cross-\ndomain weakly supervised object detection.\nIn Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition\n(CVPR), pages 14329\u201314339, 2022.\n[127] Yunqiu Xu, Yifan Sun, Zongxin Yang, Jiaxu\nMiao, and Yi Yang.\nH2fa r-cnn: Holistic\nand hierarchical feature alignment for cross-\ndomain weakly supervised object detec-\ntion.\nIn 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition\n(CVPR), pages 14309\u201314319, 2022.\n[128] Ke Yan, Youbao Tang, Yifan Peng, Veit\nSandfort, Mohammadhadi Bagheri, Zhiyong\nLu, and Ronald M. Summers. Mulan: Mul-\ntitask universal lesion analysis network for\njoint lesion detection, tagging, and segmen-\ntation, 2019.\n[129] Jianwei Yang, Chunyuan Li, Xiyang Dai,\nand Jianfeng Gao.\nFocal modulation net-\nworks.\nAdvances in Neural Information\nProcessing Systems, 35:4203\u20134217, 2022.\n[130] Shiqi Yang, Shangling Jui, Joost van de\nWeijer, et al.\nAttracting and dispersing:\nA simple approach for source-free domain\nadaptation.\nAdvances in Neural Infor-\nmation Processing Systems, 35:5802\u20135815,\n2022.\n[131] Shiqi Yang, Joost van de Weijer, Luis Her-\nranz, Shangling Jui, et al.\nExploiting the\nintrinsic neighborhood structure for source-\nfree domain adaptation. Advances in neural\ninformation processing systems, 34:29393\u2013\n29405, 2021.\n[132] Yuxiang Yang, Xinyi Zeng, Pinxian Zeng,\nBinyu Yan, Xi Wu, Jiliu Zhou, and Yan\nWang.\nBtmuda: A bi-level multi-source\nunsupervised domain adaptation framework\nfor breast cancer diagnosis, 2024.\n23"
  },
  {
    "page": 24,
    "text": "[133] Ilhoon Yoon, Hyeongjun Kwon, Jin Kim,\nJunyoung\nPark,\nHyunsung\nJang,\nand\nKwanghoon Sohn.\nEnhancing source-free\ndomain adaptive object detection with low-\nconfidence pseudo label distillation.\nIn\nEuropean Conference on Computer Vision,\npages 337\u2013353. Springer, 2024.\n[134] Fisher\nYu,\nHaofeng\nChen,\nXin\nWang,\nWenqi Xian, Yingying Chen, Fangchen Liu,\nVashisht Madhavan, and Trevor Darrell.\nBdd100k: A diverse driving dataset for het-\nerogeneous multitask learning. In Proceed-\nings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages\n2636\u20132645, 2020.\n[135] Fisher\nYu,\nHaofeng\nChen,\nXin\nWang,\nWenqi\nXian,\nYingying\nChen,\nFangchen\nLiu, Vashisht Madhavan, and Trevor Dar-\nrell.\nBdd100k: A diverse driving dataset\nfor heterogeneous multitask learning.\nIn\nIEEE/CVF\nConference\non\nComputer\nVision\nand\nPattern\nRecognition,\npages\n2633\u20132642, 2020.\n[136] Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi\nZhou,\nYohei\nNakata,\nDenis\nGudovskiy,\nTomoyuki Okuno, Jianxin Li, Kurt Keutzer,\nand Shanghang Zhang.\nMttrans: Cross-\ndomain object detection with mean teacher\ntransformer.\nIn European Conference on\nComputer Vision, pages 629\u2013645. Springer,\n2022.\n[137] Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi\nZhou,\nYohei\nNakata,\nDenis\nGudovskiy,\nTomoyuki Okuno, Jianxin Li, Kurt Keutzer,\nand Shanghang Zhang.\nMttrans: Cross-\ndomain object detection with&nbsp;mean\nteacher transformer.\nIn Computer Vision\n\u2013 ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Pro-\nceedings, Part IX, page 629\u2013645, Berlin,\nHeidelberg, 2022. Springer-Verlag.\n[138] Dingwen Zhang, Junwei Han, Gong Cheng,\nand Ming-Hsuan Yang. Weakly supervised\nobject localization and detection: A sur-\nvey. IEEE transactions on pattern analysis\nand machine intelligence, 44(9):5866\u20135885,\n2021.\n[139] Dingwen Zhang, Wenyuan Zeng, Jieru Yao,\nand Junwei Han. Weakly supervised object\ndetection using proposal-and semantic-level\nrelationships.\nIEEE Transactions on Pat-\ntern Analysis and Machine Intelligence,\n44(6):3349\u20133363, 2020.\n[140] Fangyuan Zhang, Tianxiang Pan, and Bin\nWang.\nSemi-supervised\nobject\ndetec-\ntion with adaptive class-rebalancing self-\ntraining. AAAI, 36(3):3252\u20133261, 2022.\n[141] Hao Zhang, Feng Li, Shilong Liu, Lei\nZhang, Hang Su, Jun Zhu, Lionel Ni, and\nHeung-Yeung Shum.\nDino: Detr with\nimproved denoising anchor boxes for end-\nto-end object detection.\nIn The Eleventh\nInternational Conference on Learning Rep-\nresentations, 2022.\n[142] Hongyi Zhang, Moustapha Cisse, Yann N.\nDauphin, and David Lopez-Paz.\nmixup:\nBeyond empirical risk minimization. Inter-\nnational Conference on Learning Represen-\ntations, 2018.\n[143] Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen\nLi, Siyuan Li, Liang Lin, and Guanbin Li.\nDivide and contrast: Source-free domain\nadaptation via adaptive contrastive learn-\ning. Advances in Neural Information Pro-\ncessing Systems, 35:5137\u20135149, 2022.\n[144] Liang Zhao and Limin Wang. Task-specific\ninconsistency alignment for domain adap-\ntive object detection.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR),\npages 14217\u201314226, June 2022.\n[145] Sicheng Zhao, Huizai Yao, Chuang Lin, Yue\nGao, and Guiguang Ding.\nMulti-source-\nfree domain adaptive object detection. Int.\nJ. Comput. Vision, 132(12):5950\u20135982, July\n2024.\n[146] Theodore Zhao, Yu Gu, Jianwei Yang,\nNaoto Usuyama, Ho Hin Lee, Sid Kiblawi,\nTristan Naumann, Jianfeng Gao, Angela\nCrabtree, Jacob Abel, Christine Moung-\nWen, Brian Piening, Carlo Bifulco, Mu Wei,\nHoifung Poon, and Sheng Wang.\nA foun-\ndation model for joint segmentation, detec-\ntion and recognition of biomedical objects\nacross nine modalities.\nNature Methods,\n22(1):166\u2013176, November 2024.\n[147] Zijing Zhao, Sitong Wei, Qingchao Chen,\nDehui Li, Yifan Yang, Yuxin Peng, and\nYang Liu.\nMasked retraining teacher-\nstudent framework for domain adaptive\nobject detection.\nIn Proceedings of the\n24"
  },
  {
    "page": 25,
    "text": "IEEE/CVF International Conference on\nComputer Vision, pages 19039\u201319049, 2023.\n[148] Zijing Zhao, Sitong Wei, Qingchao Chen,\nDehui Li, Yifan Yang, Yuxin Peng, and\nYang Liu.\nMasked retraining teacher-\nstudent framework for domain adaptive\nobject detection. In 2023 IEEE/CVF Inter-\nnational Conference on Computer Vision\n(ICCV), pages 18993\u201319003, 2023.\n[149] Minghang Zheng, Peng Gao, Renrui Zhang,\nKunchang Li, Xiaogang Wang, Hongsheng\nLi, and Hao Dong. End-to-end object detec-\ntion with adaptive clustering transformer.\narXiv preprint arXiv:2011.09315, 2020.\n[150] Xingyi\nZhou,\nRohit\nGirdhar,\nArmand\nJoulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl,\nand\nIshan\nMisra.\nDetecting twenty-thousand classes\nusing image-level supervision.\nIn Euro-\npean Conference on Computer Vision, pages\n350\u2013368. Springer, 2022.\n[151] Xingyi Zhou, Dequan Wang, and Philipp\nKr\u00a8ahenb\u00a8uhl.\nObjects as points.\narXiv\npreprint arXiv:1904.07850, 2019.\n[152] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang,\nJianping Shi, and Dahua Lin.\nAdapting\nobject detectors via selective cross-domain\nalignment. In CVPR, pages 687\u2013696, 2019.\n[153] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li,\nXiaogang Wang, and Jifeng Dai. Deformable\ndetr:\nDeformable\ntransformers\nfor\nend-\nto-end object detection.\narXiv preprint\narXiv:2010.04159, 2020.\n[154] Zhengxia Zou, Keyan Chen, Zhenwei Shi,\nYuhong Guo, and Jieping Ye. Object detec-\ntion in 20 years: A survey. Proceedings of\nthe IEEE, 111(3):257\u2013276, 2023.\n25"
  }
]